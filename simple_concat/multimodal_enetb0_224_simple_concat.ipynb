{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLOOi0GQM5B"
      },
      "source": [
        "## Garbage Classification Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
        "from torchvision.models.efficientnet import EfficientNet_B0_Weights\n",
        "import torch.nn.functional as F # To normalize extracted feature vectors before fusing\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import wandb\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import time\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "NOTES = '''\n",
        "'''\n",
        "\n",
        "# ========================================= GLOBAL CONFIGURATION ================================================\n",
        "# Data Directories\n",
        "DATA_DIR = r\"C:\\NN Data\\garbage_data\\kfold_garbage_data\"\n",
        "CLASSES = [\"Black\", \"Blue\", \"Green\", \"TTR\"]\n",
        "\n",
        "# ========================================= Experiment Settings =========================================\n",
        "WANDB_RUN_NAME = \"experiment_multimodal_enetb0_224_simple_concat\"\n",
        "MODEL_NAME = \"experiment_multimodal_enetb0_224_simple_concat\"\n",
        "\n",
        "# ========================================= Data Settings =========================================\n",
        "IMAGE_SIZE = (224, 224)  # Input image size for EfficientNetV2-S\n",
        "NUM_CLASSES = 4  # Number of output classes for classification\n",
        "MAX_LEN = 40  # Maximum token length for DistilBERT tokenizer\n",
        "TEST_SIZE = 0.2  # Test dataset size split\n",
        "K_FOLDS = 5  # Number of folds for stratified k-fold cross-validation\n",
        "\n",
        "# ========================================= Training Hyperparameters =========================================\n",
        "BATCH_SIZE = 64  # Number of samples per batch\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "EPOCHS = 50  # Maximum number of training epochs\n",
        "DROPOUT_IMAGE = 0.2 # Reduce from 0.3\n",
        "DROPOUT_TEXT = 0.1 # Reduce from 0.2\n",
        "DROPOUT_FUSION = 0.2 \n",
        "DROPOUT_CLASSIFIER = 0.1\n",
        "PATIENCE = 10  # Number of epochs to wait before early stopping\n",
        "CONVERGENCE_THRESHOLD = 0.001  # Minimum improvement in validation loss to continue training\n",
        "\n",
        "# ========================================= Optimization Settings =========================================\n",
        "OPTIMIZER = \"AdamW\"\n",
        "LR_SCHEDULING_FACTOR = 0.3\n",
        "LEARNING_RATE_UNFREEZE_IMAGE = 1e-5\n",
        "LEARNING_RATE_UNFREEZE_TEXT = 1e-5\n",
        "LEARNING_RATE_FUSION = 1e-3\n",
        "LEARNING_RATE_CLASSIFIER = 5e-3\n",
        "LEARNING_RATE_IMAGE = 0.001 # # EfficientNetB0\n",
        "LEARNING_RATE_TEXT = 0.00002 # DistilBERT Uncased\n",
        "WEIGHT_DECAY_TEXT = 1e-3  # Reduce from 1e-2\n",
        "WEIGHT_DECAY_IMAGE = 1e-4  # Reduce from 1e-3\n",
        "WEIGHT_DECAY_FUSION = 4e-4 \n",
        "WEIGHT_DECAY_CLASSIFIER = 1e-3  # Reduce from 1e-4\n",
        "LABEL_SMOOTHING_PREDICTION = 0.05 # Reduce from 0.1\n",
        "\n",
        "# ========================================= System Settings =========================================\n",
        "NUM_WORKERS = 4  # Dataloader parallelization\n",
        "\n",
        "# Wandb Configuration\n",
        "WANDB_CONFIG = {\n",
        "    \"entity\": \"shcau-university-of-calgary-in-alberta\",\n",
        "    \"project\": \"transfer_learning_garbage\",\n",
        "    \"name\": WANDB_RUN_NAME,\n",
        "    \"tags\": [\"distilBERT\", \"efficientnet\", \"CVPR_2024_dataset\"],\n",
        "    \"notes\": NOTES,\n",
        "    \"config\": {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"dataset\": \"CVPR_2024_dataset\",\n",
        "        \"image_size\": IMAGE_SIZE,\n",
        "        \"num_workers\": NUM_WORKERS,\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"max_len\": MAX_LEN,\n",
        "        \"learning_rate_image\": LEARNING_RATE_IMAGE,\n",
        "        \"learning_rate_text\": LEARNING_RATE_TEXT,\n",
        "        \"learning_rate_fusion\": LEARNING_RATE_FUSION,\n",
        "        \"learning_rate_classifier\": LEARNING_RATE_CLASSIFIER,\n",
        "        \"learning_rate_unfreeze_image\": LEARNING_RATE_UNFREEZE_IMAGE, # learning rate for unfrozen EfficientNet layers\n",
        "        \"learning_rate_unfreeze_text\": LEARNING_RATE_UNFREEZE_TEXT, # learning rate for unfrozen DistilBERT layers\n",
        "        \"dropout_image\": DROPOUT_IMAGE,\n",
        "        \"dropout_text\": DROPOUT_TEXT,\n",
        "        \"dropout_classifier\": DROPOUT_CLASSIFIER,\n",
        "        \"convergence_threshold\": CONVERGENCE_THRESHOLD,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"weight_decay_text\": WEIGHT_DECAY_TEXT,\n",
        "        \"weight_decay_image\": WEIGHT_DECAY_IMAGE,\n",
        "        \"WEIGHT_DECAY_CLASSIFIER\": WEIGHT_DECAY_CLASSIFIER,\n",
        "        \"label_smoothing_prediction\": LABEL_SMOOTHING_PREDICTION,\n",
        "        \"optimizer\": OPTIMIZER \n",
        "    },\n",
        "    \"job_type\": \"train\",\n",
        "    \"resume\": \"allow\",\n",
        "}\n",
        "\n",
        "# Normalization Stats\n",
        "NORMALIZATION_STATS = {\n",
        "    \"mean\": [0.485, 0.456, 0.406],\n",
        "    \"std\": [0.229, 0.224, 0.225],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOG_FILE = \"experiment_multimodal_enetb0_224_simple_concat.txt\"  # Log file name\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Log everything (INFO and above)\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(LOG_FILE, mode='w'),  # Overwrite log file on each run\n",
        "        logging.StreamHandler(sys.stdout)  # Print log messages to console too\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:16,461 - INFO - [CONFIG] ============================== Experiment Configuration ==============================\n",
            "2025-03-24 10:35:16,462 - INFO - [CONFIG] Experiment Name: experiment_multimodal_enetb0_224_simple_concat\n",
            "2025-03-24 10:35:16,462 - INFO - [CONFIG] Entity: shcau-university-of-calgary-in-alberta\n",
            "2025-03-24 10:35:16,464 - INFO - [CONFIG] Project: transfer_learning_garbage\n",
            "2025-03-24 10:35:16,464 - INFO - [CONFIG] Tags: distilBERT, efficientnet, CVPR_2024_dataset\n",
            "2025-03-24 10:35:16,465 - INFO - [CONFIG] Notes: \n",
            "\n",
            "2025-03-24 10:35:16,466 - INFO - [CONFIG] Job Type: train\n",
            "2025-03-24 10:35:16,466 - INFO - [CONFIG] Resume: allow\n",
            "2025-03-24 10:35:16,467 - INFO - [CONFIG] ------------------------------ Hyperparameters ------------------------------\n",
            "2025-03-24 10:35:16,467 - INFO - [CONFIG] epochs: 50\n",
            "2025-03-24 10:35:16,468 - INFO - [CONFIG] batch_size: 64\n",
            "2025-03-24 10:35:16,469 - INFO - [CONFIG] dataset: CVPR_2024_dataset\n",
            "2025-03-24 10:35:16,469 - INFO - [CONFIG] image_size: (224, 224)\n",
            "2025-03-24 10:35:16,470 - INFO - [CONFIG] num_workers: 4\n",
            "2025-03-24 10:35:16,470 - INFO - [CONFIG] num_classes: 4\n",
            "2025-03-24 10:35:16,471 - INFO - [CONFIG] max_len: 40\n",
            "2025-03-24 10:35:16,472 - INFO - [CONFIG] learning_rate_image: 0.001\n",
            "2025-03-24 10:35:16,472 - INFO - [CONFIG] learning_rate_text: 2e-05\n",
            "2025-03-24 10:35:16,473 - INFO - [CONFIG] learning_rate_fusion: 0.001\n",
            "2025-03-24 10:35:16,474 - INFO - [CONFIG] learning_rate_classifier: 0.005\n",
            "2025-03-24 10:35:16,474 - INFO - [CONFIG] learning_rate_unfreeze_image: 1e-05\n",
            "2025-03-24 10:35:16,475 - INFO - [CONFIG] learning_rate_unfreeze_text: 1e-05\n",
            "2025-03-24 10:35:16,475 - INFO - [CONFIG] dropout_image: 0.2\n",
            "2025-03-24 10:35:16,476 - INFO - [CONFIG] dropout_text: 0.1\n",
            "2025-03-24 10:35:16,476 - INFO - [CONFIG] dropout_classifier: 0.1\n",
            "2025-03-24 10:35:16,477 - INFO - [CONFIG] convergence_threshold: 0.001\n",
            "2025-03-24 10:35:16,478 - INFO - [CONFIG] patience: 10\n",
            "2025-03-24 10:35:16,478 - INFO - [CONFIG] weight_decay_text: 0.001\n",
            "2025-03-24 10:35:16,479 - INFO - [CONFIG] weight_decay_image: 0.0001\n",
            "2025-03-24 10:35:16,479 - INFO - [CONFIG] WEIGHT_DECAY_CLASSIFIER: 0.001\n",
            "2025-03-24 10:35:16,480 - INFO - [CONFIG] label_smoothing_prediction: 0.05\n",
            "2025-03-24 10:35:16,480 - INFO - [CONFIG] optimizer: AdamW\n",
            "2025-03-24 10:35:16,481 - INFO - [CONFIG] =============================================================================\n"
          ]
        }
      ],
      "source": [
        "# Log the configuration\n",
        "logging.info(\"[CONFIG] ============================== Experiment Configuration ==============================\")\n",
        "\n",
        "# Log top-level keys\n",
        "logging.info(f\"[CONFIG] Experiment Name: {WANDB_CONFIG['name']}\")\n",
        "logging.info(f\"[CONFIG] Entity: {WANDB_CONFIG['entity']}\")\n",
        "logging.info(f\"[CONFIG] Project: {WANDB_CONFIG['project']}\")\n",
        "logging.info(f\"[CONFIG] Tags: {', '.join(WANDB_CONFIG['tags'])}\")\n",
        "logging.info(f\"[CONFIG] Notes: {WANDB_CONFIG['notes']}\")\n",
        "logging.info(f\"[CONFIG] Job Type: {WANDB_CONFIG['job_type']}\")\n",
        "logging.info(f\"[CONFIG] Resume: {WANDB_CONFIG['resume']}\")\n",
        "\n",
        "# Log nested configuration (under 'config')\n",
        "logging.info(\"[CONFIG] ------------------------------ Hyperparameters ------------------------------\")\n",
        "for key, value in WANDB_CONFIG[\"config\"].items():\n",
        "    logging.info(f\"[CONFIG] {key}: {value}\")\n",
        "\n",
        "logging.info(\"[CONFIG] =============================================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_wandb(fold):\n",
        "    \"\"\"Initialize wandb for each fold with a unique run name.\"\"\"\n",
        "    wandb.init(\n",
        "        entity=WANDB_CONFIG[\"entity\"],\n",
        "        project=WANDB_CONFIG[\"project\"],\n",
        "        name=f\"{WANDB_RUN_NAME}_fold_{fold + 1}\",\n",
        "        tags=WANDB_CONFIG[\"tags\"],\n",
        "        notes=WANDB_CONFIG[\"notes\"],\n",
        "        config=WANDB_CONFIG[\"config\"],\n",
        "        job_type=WANDB_CONFIG[\"job_type\"],\n",
        "        resume=WANDB_CONFIG[\"resume\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SpaCy for lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load NLTK stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Standardize text, remove stopwords, and apply lemmatization.\"\"\"\n",
        "    # 1. Standardize text (lowercasing & trimming spaces)\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # 2. Remove stopwords\n",
        "    text_tokens = text.split()\n",
        "    text = \" \".join([word for word in text_tokens if word not in stop_words])\n",
        "\n",
        "    # 3. Lemmatization\n",
        "    doc = nlp(text)\n",
        "    text = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "    return text\n",
        "\n",
        "def read_text_files_with_labels_and_image_paths(path):\n",
        "    \"\"\"Extract text from file names, apply preprocessing, and return labels with image paths.\"\"\"\n",
        "    texts, labels, image_paths = [], [], []\n",
        "    class_folders = sorted(os.listdir(path))\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = sorted(os.listdir(class_path))  # Sort to ensure order consistency\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    # Extract filename without extension\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "\n",
        "                    # Replace underscores with spaces\n",
        "                    text = file_name_no_ext.replace(\"_\", \" \")\n",
        "\n",
        "                    # Remove numbers\n",
        "                    text_without_digits = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "                    # Apply preprocessing\n",
        "                    preprocessed_text = preprocess_text(text_without_digits)\n",
        "\n",
        "                    texts.append(preprocessed_text)\n",
        "                    labels.append(label_map[class_name])\n",
        "                    image_paths.append(file_path)\n",
        "\n",
        "    return np.array(texts), np.array(labels), np.array(image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# logging.info(\"[INFO] Extracting text, labels, and image paths...\")\n",
        "# texts, labels, image_paths= read_text_files_with_labels_and_image_paths(DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    \"\"\"Dataset class for text data.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "    \n",
        "# Custom dataset class for images\n",
        "class ImageDataset(Dataset):\n",
        "    \"\"\"Dataset class for image data.\"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    \"\"\"Dataset class for multimodal data (image + text).\"\"\"\n",
        "    def __init__(self, image_dataset, text_dataset):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_dataset), len(self.text_dataset))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.image_dataset[idx]\n",
        "        text_data = self.text_dataset[idx]\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": text_data[\"input_ids\"],\n",
        "            \"attention_mask\": text_data[\"attention_mask\"],\n",
        "            \"label\": label\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # EfficientNet-B0 for image feature extraction\n",
        "        self.image_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze all layers of EfficientNet-B0 initially\n",
        "        for param in self.image_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last 1 layers of EfficientNet-B0\n",
        "        for param in self.image_model.features[-4:].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Remove the classifier layer of EfficientNet-B0 to use a custom prediction layer\n",
        "        num_ftrs = self.image_model.classifier[1].in_features\n",
        "        self.image_model.classifier = nn.Identity()\n",
        "\n",
        "        # Project image features to 256 dimensions\n",
        "        self.image_fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 256),\n",
        "            nn.BatchNorm1d(256),  # Add batch normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_IMAGE)\n",
        "            )  # Dropout for image features\n",
        "\n",
        "        # DistilBERT for text feature extraction\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # Freeze all layers of DistilBERT initially\n",
        "        for param in self.text_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last 1 layers of DistilBERT\n",
        "        for param in self.text_model.transformer.layer[-3:].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Project text features to 256 dimensions\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(self.text_model.config.hidden_size, 256),\n",
        "            nn.BatchNorm1d(256),  # Add batch normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_TEXT)  # Dropout for text features\n",
        "        )\n",
        "\n",
        "        # Normalization layers\n",
        "        self.text_norms = nn.LayerNorm(256)\n",
        "        self.image_norm = nn.LayerNorm(256)\n",
        "\n",
        "        # Feature fusion layer (concatenation of text and image features)\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512, 512),  # Concatenated features have 512 dimensions\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),  # Add ReLU activation\n",
        "\n",
        "            nn.Linear(512, 256),  # Intermediate layer\n",
        "            nn.BatchNorm1d(256),  # Batch normalization\n",
        "            nn.ReLU(),            # ReLU activation\n",
        "        )\n",
        "\n",
        "        # Dropout layer for the final classification layer\n",
        "        self.dropout = nn.Dropout(DROPOUT_CLASSIFIER)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, image_inputs):\n",
        "        # Extract text features\n",
        "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(text_output.last_hidden_state[:, 0, :])  # Use the [CLS] token\n",
        "        text_features = self.text_norms(text_features)\n",
        "        image_features = self.image_fc(self.image_model(image_inputs))\n",
        "        image_features = self.image_norm(image_features)\n",
        "        text_features = F.normalize(text_features, p=2, dim=1)\n",
        "        image_features = F.normalize(image_features, p=2, dim=1)\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "        combined_features = self.fusion_fc(combined_features)\n",
        "        output = self.classifier(self.dropout(combined_features))\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:45,908 - INFO - First 4 samples of dataset:\n",
            "\n",
            "2025-03-24 10:35:45,908 - INFO - Texts: ['aero bar wrapper' 'break glass' 'break rubber' 'butter paper']\n",
            "2025-03-24 10:35:45,909 - INFO - Labels: [0 0 0 0]\n",
            "2025-03-24 10:35:45,910 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Aero_bar_wrapper_1.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_Glass_5291.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_rubber_7263.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Butter_Paper_9976.png']\n",
            "2025-03-24 10:35:45,911 - INFO - \n",
            "Last 4 samples of dataset:\n",
            "\n",
            "2025-03-24 10:35:45,912 - INFO - Texts: ['wristwatch' 'xbox controller' 'xbox one controller' 'zipper file bag']\n",
            "2025-03-24 10:35:45,912 - INFO - Labels: [3 3 3 3]\n",
            "2025-03-24 10:35:45,913 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\wristwatch_3782.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_controller_2047.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_one_controller_2048.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\zipper_file_bag_2049.png']\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "texts, labels, image_paths = read_text_files_with_labels_and_image_paths(DATA_DIR)\n",
        "\n",
        "# Log first and last 4 samples\n",
        "logging.info(\"First 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[:4]}\")\n",
        "logging.info(f\"Labels: {labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[-4:]}\")\n",
        "logging.info(f\"Labels: {labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split into test set and development set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:45,932 - INFO - First 4 samples of test set:\n",
            "\n",
            "2025-03-24 10:35:45,933 - INFO - Texts: ['ballast light' 'old phone' 'milk jug lid tab' 'dirty dish sponge']\n",
            "2025-03-24 10:35:45,933 - INFO - Labels: [3 3 0 0]\n",
            "2025-03-24 10:35:45,934 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\ballast_light_286.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\Old_Phones_7828.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\milk_jug_lid_tab_1137.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\dirty_dish_sponge_437.png']\n",
            "2025-03-24 10:35:45,935 - INFO - \n",
            "Last 4 samples of test set:\n",
            "\n",
            "2025-03-24 10:35:45,936 - INFO - Texts: ['empty glass jar' 'non - stretchy plastic' 'backpack' 'piece break glass']\n",
            "2025-03-24 10:35:45,937 - INFO - Labels: [1 0 3 0]\n",
            "2025-03-24 10:35:45,937 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Blue\\\\empty_glass_jar_1609.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\non-stretchy plastic.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\backpack_216.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\piece_of_broken_glass_1315.png']\n"
          ]
        }
      ],
      "source": [
        "# Split into a test set and development set\n",
        "train_texts, test_texts, train_labels, test_labels, train_image_paths, test_image_paths = train_test_split(\n",
        "    texts, labels, image_paths, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Log first 4 samples of test set\n",
        "logging.info(\"First 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[:4]}\")\n",
        "logging.info(f\"Labels: {test_labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[-4:]}\")\n",
        "logging.info(f\"Labels: {test_labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE), \n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS[\"mean\"], std=NORMALIZATION_STATS[\"std\"])  # Apply correct normalization\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS[\"mean\"], std=NORMALIZATION_STATS[\"std\"])  # Only resize + normalize\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS[\"mean\"], std=NORMALIZATION_STATS[\"std\"])  # Only resize + normalize\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Tokenizer for DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoader for test set\n",
        "\n",
        "Create the dataloader for the test set and set aside for model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test dataset\n",
        "test_image_dataset = ImageDataset(test_image_paths, test_labels, transform[\"test\"])\n",
        "test_text_dataset = CustomTextDataset(test_texts, test_labels, tokenizer, max_len=MAX_LEN)  # Ensure tokenizer is defined\n",
        "test_multimodal_dataset = MultimodalDataset(test_image_dataset, test_text_dataset)\n",
        "\n",
        "# DataLoader for test set\n",
        "test_loader = DataLoader(test_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a peek at a batch in the test set to verify that data has been correctly organized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:47,763 - INFO - [INFO] One Batch Sample Inspection:\n",
            "2025-03-24 10:35:47,763 - INFO -    Images Shape: torch.Size([64, 3, 224, 224])\n",
            "2025-03-24 10:35:47,764 - INFO -    Input IDs Shape: torch.Size([64, 40])\n",
            "2025-03-24 10:35:47,764 - INFO -    Attention Mask Shape: torch.Size([64, 40])\n",
            "2025-03-24 10:35:47,765 - INFO -    Labels Shape: torch.Size([64])\n",
            "2025-03-24 10:35:47,766 - INFO - \n",
            "[INFO] First Sample:\n",
            "2025-03-24 10:35:47,770 - INFO -    Image Tensor: tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         ...,\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
            "\n",
            "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         ...,\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
            "\n",
            "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         ...,\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]])\n",
            "2025-03-24 10:35:47,772 - INFO -    Input IDs: tensor([  101, 28030,  2422,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "2025-03-24 10:35:47,773 - INFO -    Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "2025-03-24 10:35:47,773 - INFO -    Label: 3\n"
          ]
        }
      ],
      "source": [
        "# Get one batch\n",
        "for batch in test_loader:\n",
        "    images = batch[\"image\"]  # Image tensor\n",
        "    input_ids = batch[\"input_ids\"]  # Tokenized text tensor\n",
        "    attention_mask = batch[\"attention_mask\"]  # Attention mask\n",
        "    labels = batch[\"label\"]  # Labels tensor\n",
        "\n",
        "    # Log shapes of tensors\n",
        "    logging.info(\"[INFO] One Batch Sample Inspection:\")\n",
        "    logging.info(f\"   Images Shape: {images.shape}\")\n",
        "    logging.info(f\"   Input IDs Shape: {input_ids.shape}\")\n",
        "    logging.info(f\"   Attention Mask Shape: {attention_mask.shape}\")\n",
        "    logging.info(f\"   Labels Shape: {labels.shape}\")\n",
        "\n",
        "    # Log first sample details\n",
        "    logging.info(\"\\n[INFO] First Sample:\")\n",
        "    logging.info(f\"   Image Tensor: {images[0]}\")\n",
        "    logging.info(f\"   Input IDs: {input_ids[0]}\")\n",
        "    logging.info(f\"   Attention Mask: {attention_mask[0]}\")\n",
        "    logging.info(f\"   Label: {labels[0]}\")\n",
        "\n",
        "    break  # Stop after inspecting one batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Stratified K-Fold on the development set to split into train/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:47,789 - INFO - [INFO] Fold 1/5\n",
            "2025-03-24 10:35:47,790 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 10:35:47,792 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 10:35:47,793 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 10:35:47,793 - INFO - [INFO] Fold 2/5\n",
            "2025-03-24 10:35:47,794 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 10:35:47,796 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 10:35:47,797 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 10:35:47,797 - INFO - [INFO] Fold 3/5\n",
            "2025-03-24 10:35:47,798 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 10:35:47,800 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 10:35:47,800 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 10:35:47,801 - INFO - [INFO] Fold 4/5\n",
            "2025-03-24 10:35:47,802 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 10:35:47,803 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-24 10:35:47,804 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n",
            "2025-03-24 10:35:47,805 - INFO - [INFO] Fold 5/5\n",
            "2025-03-24 10:35:47,806 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 10:35:47,807 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-24 10:35:47,808 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    logging.info(f\"[INFO] Fold {fold + 1}/{K_FOLDS}\")\n",
        "\n",
        "    # Extract labels for current fold\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "\n",
        "    # Log class distributions\n",
        "    logging.info(\"[INFO] Class Distributions:\")\n",
        "    logging.info(f\"   Train Class Distribution: {Counter(train_labels_fold)}\")\n",
        "    logging.info(f\"   Validation Class Distribution: {Counter(val_labels_fold)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify k-fold was applied correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:47,819 - INFO - [INFO] No data leakage detected in Fold 1\n",
            "2025-03-24 10:35:47,821 - INFO - [INFO] No data leakage detected in Fold 2\n",
            "2025-03-24 10:35:47,822 - INFO - [INFO] No data leakage detected in Fold 3\n",
            "2025-03-24 10:35:47,824 - INFO - [INFO] No data leakage detected in Fold 4\n",
            "2025-03-24 10:35:47,825 - INFO - [INFO] No data leakage detected in Fold 5\n"
          ]
        }
      ],
      "source": [
        "# Ensure no data leakage in folds\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    train_set = set(train_idx)\n",
        "    val_set = set(val_idx)\n",
        "\n",
        "    # Check for intersection (should be empty)\n",
        "    intersection = train_set.intersection(val_set)\n",
        "    assert len(intersection) == 0, f\"Data leakage detected in Fold {fold + 1}\"\n",
        "\n",
        "    logging.info(f\"[INFO] No data leakage detected in Fold {fold + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Move data to the appropriate device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)  # Compute batch loss\n",
        "\n",
        "            # Aggregate loss for averaging\n",
        "            total_loss += loss.item() * labels.size(0)  # Multiply by batch size for proper averaging\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total  # Normalize loss over total samples\n",
        "    accuracy = correct / total  # Compute accuracy\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_warmup_lr(epoch, warmup_epochs, base_lr):\n",
        "    \"\"\"\n",
        "    Linear warmup schedule for the learning rate.\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return base_lr * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        return base_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True):\n",
        "    initialize_wandb(fold)\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    best_val_loss = float(\"inf\")  # Track best validation loss\n",
        "    epochs_without_improvement = 0  # Track epochs without improvement until equals patience\n",
        "\n",
        "    # ================ ReduceLROnPlateau Scheduler ================\n",
        "    plateau_scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=LR_SCHEDULING_FACTOR, patience=3, verbose=True\n",
        "    )\n",
        "    \n",
        "    # AMP GradScaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    epoch_start_time = time.time()  # Start total training timer\n",
        "    logging.info(\"[TRAIN INFO] Starting Training...\")\n",
        "\n",
        "    # Warmup settings\n",
        "    WARMUP_EPOCHS = 8  # Number of epochs for warmup\n",
        "    base_lr_image = LEARNING_RATE_IMAGE  # Base learning rate for EfficientNet\n",
        "    base_lr_text = LEARNING_RATE_TEXT  # Base learning rate for DistilBERT\n",
        "    base_lr_fusion = LEARNING_RATE_FUSION  # Base learning rate for fusion layer\n",
        "    base_lr_classifier = LEARNING_RATE_CLASSIFIER  # Base learning rate for classifier\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        logging.info(f\"[TRAIN INFO] ============================== Epoch {epoch + 1}/{EPOCHS} ==============================\")\n",
        "        \n",
        "        # Apply learning rate warmup\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            warmup_lr_image = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_image)\n",
        "            warmup_lr_text = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_text)\n",
        "            warmup_lr_fusion = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_fusion)\n",
        "            warmup_lr_classifier = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_classifier)\n",
        "\n",
        "            # Update learning rates for each parameter group\n",
        "            optimizer.param_groups[0][\"lr\"] = warmup_lr_image  # Unfrozen EfficientNet layer\n",
        "            optimizer.param_groups[1][\"lr\"] = warmup_lr_text  # Unfrozen DistilBERT layer\n",
        "            optimizer.param_groups[2][\"lr\"] = warmup_lr_image  # Image FC layer\n",
        "            optimizer.param_groups[3][\"lr\"] = warmup_lr_text  # Text FC layer\n",
        "            optimizer.param_groups[4][\"lr\"] = warmup_lr_fusion  # Fusion layer\n",
        "            optimizer.param_groups[5][\"lr\"] = warmup_lr_classifier  # Classifier layer\n",
        "\n",
        "        model.train()  # Set model to training modes\n",
        "        total_train_loss = 0  # Track total training loss for the epoch\n",
        "        batch_train_loss = 0  # Track batch loss for gradient accumulation\n",
        "        step = 0  # Track the number of batches processed\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Training phase\n",
        "        for step, batch in enumerate(dataloaders[\"train_loader\"], 1):\n",
        "            # Move data to device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask, images)  # Send inputs to network and receive outputs\n",
        "                loss = criterion(outputs, labels) / GRAD_ACCUM_STEPS  # Compute loss (no normalization for gradient accumulation)\n",
        "\n",
        "            # Backward pass and optimizer step\n",
        "            scaler.scale(loss).backward()  # Scale loss and backpropagate\n",
        "\n",
        "            batch_train_loss += loss.item()\n",
        "            total_train_loss += loss.item() * GRAD_ACCUM_STEPS  # Undo normalization for total loss\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            # Perform optimizer step before learning rate scheduler step\n",
        "            if step % GRAD_ACCUM_STEPS == 0 or step == len(dataloaders[\"train_loader\"]):\n",
        "                # Gradient Clipping\n",
        "                scaler.unscale_(optimizer)  # Unscale gradients before clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to a max norm of 1.0\n",
        "\n",
        "                # Optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Log batch loss\n",
        "                logging.info(f\"[TRAIN INFO] Batch {step}/{len(dataloaders['train_loader'])}, Accumulated loss over {GRAD_ACCUM_STEPS} batches: {batch_train_loss:.4f}\")\n",
        "                batch_train_loss = 0  # Reset batch loss for the next accumulation\n",
        "\n",
        "        # Validation step to see how well model performs this epoch\n",
        "        logging.info(f\"[TRAIN INFO] Evaluating model...\")\n",
        "        val_loss, val_acc = evaluate_model(model, dataloaders[\"val_loader\"], device)\n",
        "        avg_train_loss = total_train_loss / len(dataloaders[\"train_loader\"])\n",
        "\n",
        "        # **Learning Rate Scheduler Handling**\n",
        "        plateau_scheduler.step(val_loss)  \n",
        "\n",
        "        # Log weight decay and learning rate updates\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_val_loss_diff\": avg_train_loss - val_loss,  # Track overfitting tendency\n",
        "            \"early_stopping_epochs\": epochs_without_improvement,  # Track early stopping\n",
        "            \"learning_rate_image\": optimizer.param_groups[0][\"lr\"],  # Log learning rates\n",
        "            \"learning_rate_text\": optimizer.param_groups[1][\"lr\"],\n",
        "            \"learning_rate_fusion\": optimizer.param_groups[4][\"lr\"],\n",
        "            \"learning_rate_classifier\": optimizer.param_groups[5][\"lr\"],\n",
        "        })\n",
        "\n",
        "        logging.info(f\"[TRAIN INFO] Epoch {epoch + 1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss - CONVERGENCE_THRESHOLD:  # If loss improves, save the model\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0  # Reset epochs without improvement counter for patience\n",
        "            torch.save(model.state_dict(), f\"{MODEL_NAME}_fold_{fold+1}.pth\")\n",
        "            logging.info(f\"[TRAIN INFO] Best Model Saved for Fold {fold + 1}\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1  # Increment until patience reached\n",
        "\n",
        "        # Early stopping if no improvement for epochs\n",
        "        if epochs_without_improvement >= PATIENCE:\n",
        "            total_training_time = time.time() - epoch_start_time\n",
        "            logging.info(f\"[TRAIN INFO] Early stopping at epoch {epoch + 1} as validation loss did not improve for {PATIENCE} epochs.\")\n",
        "            logging.info(f\"[TRAIN INFO] Total Time: {total_training_time:.2f}s\")\n",
        "            wandb.finish()\n",
        "            break\n",
        "\n",
        "    total_training_time = time.time() - epoch_start_time\n",
        "    logging.info(f\"[TRAIN INFO] Fold {fold + 1} Training Complete at epoch {epoch + 1}. Total Time: {total_training_time:.2f}s\")\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:47,879 - INFO - [K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\n",
            "2025-03-24 10:35:47,882 - INFO - [K-FOLD INFO] ============================== Fold 1/5 ==============================\n",
            "2025-03-24 10:35:47,884 - INFO - [K-FOLD INFO] Fold 1:\n",
            "2025-03-24 10:35:47,885 - INFO -    Train Samples: 8594\n",
            "2025-03-24 10:35:47,886 - INFO -    Validation Samples: 2149\n",
            "2025-03-24 10:35:47,886 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 1\n",
            "2025-03-24 10:35:47,887 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 1:\n",
            "2025-03-24 10:35:47,888 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 10:35:48,422 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 1\n",
            "2025-03-24 10:35:48,423 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 1:\n",
            "2025-03-24 10:35:48,424 - INFO - [K-FOLD INFO] Loss function initialized for Fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: shcau (shcau-university-of-calgary-in-alberta) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_enetb0_224_simple_concat\\wandb\\run-20250324_103548-v81jf9e8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/v81jf9e8' target=\"_blank\">experiment_multimodal_enetb0_224_simple_concat_fold_1</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/v81jf9e8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/v81jf9e8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arkzs\\miniforge3\\envs\\enel645_torch_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_21264\\836902376.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:49,938 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 10:35:49,938 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_21264\\836902376.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 10:35:56,598 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0901\n",
            "2025-03-24 10:36:04,465 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4260\n",
            "2025-03-24 10:36:12,311 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3956\n",
            "2025-03-24 10:36:20,058 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3240\n",
            "2025-03-24 10:36:28,135 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3102\n",
            "2025-03-24 10:36:35,635 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2396\n",
            "2025-03-24 10:36:44,296 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2669\n",
            "2025-03-24 10:36:52,857 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2111\n",
            "2025-03-24 10:37:01,179 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2309\n",
            "2025-03-24 10:37:09,913 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.1919\n",
            "2025-03-24 10:37:18,405 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1657\n",
            "2025-03-24 10:37:26,876 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1682\n",
            "2025-03-24 10:37:35,420 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1794\n",
            "2025-03-24 10:37:43,982 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1514\n",
            "2025-03-24 10:37:52,395 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.0793\n",
            "2025-03-24 10:38:00,754 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1092\n",
            "2025-03-24 10:38:09,321 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.0700\n",
            "2025-03-24 10:38:17,786 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0495\n",
            "2025-03-24 10:38:26,380 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0458\n",
            "2025-03-24 10:38:34,975 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0665\n",
            "2025-03-24 10:38:43,380 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.9957\n",
            "2025-03-24 10:38:52,165 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0579\n",
            "2025-03-24 10:39:00,369 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0085\n",
            "2025-03-24 10:39:08,961 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.9614\n",
            "2025-03-24 10:39:17,367 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9557\n",
            "2025-03-24 10:39:26,000 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9617\n",
            "2025-03-24 10:39:34,756 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9258\n",
            "2025-03-24 10:39:43,273 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9424\n",
            "2025-03-24 10:39:51,638 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9694\n",
            "2025-03-24 10:39:59,762 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9576\n",
            "2025-03-24 10:40:07,227 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8734\n",
            "2025-03-24 10:40:14,751 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8644\n",
            "2025-03-24 10:40:22,276 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9178\n",
            "2025-03-24 10:40:28,045 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7424\n",
            "2025-03-24 10:40:28,735 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2387\n",
            "2025-03-24 10:40:28,736 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 10:41:25,367 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1006, Val Loss: 0.6808, Val Acc: 0.7534\n",
            "2025-03-24 10:41:25,699 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 10:41:25,701 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 10:41:31,909 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6526\n",
            "2025-03-24 10:41:40,089 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9421\n",
            "2025-03-24 10:41:47,994 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8855\n",
            "2025-03-24 10:41:55,862 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8427\n",
            "2025-03-24 10:42:04,148 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.9377\n",
            "2025-03-24 10:42:12,076 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8181\n",
            "2025-03-24 10:42:19,936 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8285\n",
            "2025-03-24 10:42:28,090 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7988\n",
            "2025-03-24 10:42:35,890 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8047\n",
            "2025-03-24 10:42:43,743 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7945\n",
            "2025-03-24 10:42:51,398 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7658\n",
            "2025-03-24 10:42:59,559 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7189\n",
            "2025-03-24 10:43:08,115 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7393\n",
            "2025-03-24 10:43:16,014 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7729\n",
            "2025-03-24 10:43:23,847 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7227\n",
            "2025-03-24 10:43:31,697 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7192\n",
            "2025-03-24 10:43:39,437 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7324\n",
            "2025-03-24 10:43:47,487 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7531\n",
            "2025-03-24 10:43:55,194 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7446\n",
            "2025-03-24 10:44:02,874 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7203\n",
            "2025-03-24 10:44:10,454 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6878\n",
            "2025-03-24 10:44:18,214 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7767\n",
            "2025-03-24 10:44:26,244 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7982\n",
            "2025-03-24 10:44:33,906 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7075\n",
            "2025-03-24 10:44:41,352 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7364\n",
            "2025-03-24 10:44:48,831 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7699\n",
            "2025-03-24 10:44:56,365 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6592\n",
            "2025-03-24 10:45:04,138 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6306\n",
            "2025-03-24 10:45:11,708 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7285\n",
            "2025-03-24 10:45:19,445 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7680\n",
            "2025-03-24 10:45:27,199 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7605\n",
            "2025-03-24 10:45:35,069 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6710\n",
            "2025-03-24 10:45:42,773 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7467\n",
            "2025-03-24 10:45:48,448 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6116\n",
            "2025-03-24 10:45:49,041 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1587\n",
            "2025-03-24 10:45:49,041 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 10:46:45,838 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.7676, Val Loss: 0.4791, Val Acc: 0.8241\n",
            "2025-03-24 10:46:46,149 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 10:46:46,150 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 10:46:52,463 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4990\n",
            "2025-03-24 10:47:00,573 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6491\n",
            "2025-03-24 10:47:08,616 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6505\n",
            "2025-03-24 10:47:16,826 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6754\n",
            "2025-03-24 10:47:24,759 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6694\n",
            "2025-03-24 10:47:32,750 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6925\n",
            "2025-03-24 10:47:40,849 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6799\n",
            "2025-03-24 10:47:49,020 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6414\n",
            "2025-03-24 10:47:56,964 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6447\n",
            "2025-03-24 10:48:05,167 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6858\n",
            "2025-03-24 10:48:13,305 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6757\n",
            "2025-03-24 10:48:21,800 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6319\n",
            "2025-03-24 10:48:30,402 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6260\n",
            "2025-03-24 10:48:38,488 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6381\n",
            "2025-03-24 10:48:46,396 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6080\n",
            "2025-03-24 10:48:54,407 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7461\n",
            "2025-03-24 10:49:01,892 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6482\n",
            "2025-03-24 10:49:10,180 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6818\n",
            "2025-03-24 10:49:18,186 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6960\n",
            "2025-03-24 10:49:26,557 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6483\n",
            "2025-03-24 10:49:35,009 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6519\n",
            "2025-03-24 10:49:43,586 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5976\n",
            "2025-03-24 10:49:52,189 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7036\n",
            "2025-03-24 10:50:00,578 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5467\n",
            "2025-03-24 10:50:09,139 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6468\n",
            "2025-03-24 10:50:17,584 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6268\n",
            "2025-03-24 10:50:26,166 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6100\n",
            "2025-03-24 10:50:34,629 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6347\n",
            "2025-03-24 10:50:42,809 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6098\n",
            "2025-03-24 10:50:51,357 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6234\n",
            "2025-03-24 10:50:59,752 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7021\n",
            "2025-03-24 10:51:08,297 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7081\n",
            "2025-03-24 10:51:16,860 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5660\n",
            "2025-03-24 10:51:23,259 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5181\n",
            "2025-03-24 10:51:23,862 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.3228\n",
            "2025-03-24 10:51:23,863 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 10:52:23,085 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6565, Val Loss: 0.4194, Val Acc: 0.8353\n",
            "2025-03-24 10:52:23,390 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 10:52:23,390 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 10:52:29,926 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4412\n",
            "2025-03-24 10:52:38,362 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5798\n",
            "2025-03-24 10:52:46,910 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6049\n",
            "2025-03-24 10:52:55,494 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5361\n",
            "2025-03-24 10:53:03,936 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5379\n",
            "2025-03-24 10:53:12,597 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6123\n",
            "2025-03-24 10:53:21,020 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5603\n",
            "2025-03-24 10:53:29,534 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5892\n",
            "2025-03-24 10:53:38,674 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6581\n",
            "2025-03-24 10:53:47,712 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5954\n",
            "2025-03-24 10:53:57,097 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6859\n",
            "2025-03-24 10:54:06,101 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5902\n",
            "2025-03-24 10:54:14,846 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5436\n",
            "2025-03-24 10:54:23,256 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6242\n",
            "2025-03-24 10:54:31,835 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6092\n",
            "2025-03-24 10:54:40,415 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5648\n",
            "2025-03-24 10:54:48,639 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5102\n",
            "2025-03-24 10:54:56,440 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6619\n",
            "2025-03-24 10:55:04,691 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5870\n",
            "2025-03-24 10:55:12,883 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6681\n",
            "2025-03-24 10:55:20,679 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6149\n",
            "2025-03-24 10:55:28,476 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5277\n",
            "2025-03-24 10:55:36,096 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5511\n",
            "2025-03-24 10:55:43,953 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5807\n",
            "2025-03-24 10:55:52,229 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5614\n",
            "2025-03-24 10:56:00,595 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6101\n",
            "2025-03-24 10:56:08,865 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5536\n",
            "2025-03-24 10:56:17,145 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5829\n",
            "2025-03-24 10:56:25,467 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5479\n",
            "2025-03-24 10:56:34,007 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6071\n",
            "2025-03-24 10:56:42,068 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5785\n",
            "2025-03-24 10:56:50,408 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6261\n",
            "2025-03-24 10:56:58,649 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5141\n",
            "2025-03-24 10:57:04,438 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4414\n",
            "2025-03-24 10:57:05,055 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0976\n",
            "2025-03-24 10:57:05,055 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 10:58:04,280 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5853, Val Loss: 0.3866, Val Acc: 0.8553\n",
            "2025-03-24 10:58:04,596 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 10:58:04,596 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 10:58:10,586 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4199\n",
            "2025-03-24 10:58:18,410 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5527\n",
            "2025-03-24 10:58:26,491 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4991\n",
            "2025-03-24 10:58:34,781 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5436\n",
            "2025-03-24 10:58:43,018 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5216\n",
            "2025-03-24 10:58:51,391 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5581\n",
            "2025-03-24 10:58:59,977 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5724\n",
            "2025-03-24 10:59:08,205 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5143\n",
            "2025-03-24 10:59:16,282 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5763\n",
            "2025-03-24 10:59:24,151 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5252\n",
            "2025-03-24 10:59:32,110 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6100\n",
            "2025-03-24 10:59:39,929 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5962\n",
            "2025-03-24 10:59:47,950 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4857\n",
            "2025-03-24 10:59:55,945 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5329\n",
            "2025-03-24 11:00:04,493 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5275\n",
            "2025-03-24 11:00:12,811 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5649\n",
            "2025-03-24 11:00:21,013 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5183\n",
            "2025-03-24 11:00:29,522 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5553\n",
            "2025-03-24 11:00:37,710 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4620\n",
            "2025-03-24 11:00:45,797 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5720\n",
            "2025-03-24 11:00:53,772 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5477\n",
            "2025-03-24 11:01:02,125 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5116\n",
            "2025-03-24 11:01:10,122 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4803\n",
            "2025-03-24 11:01:18,120 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5231\n",
            "2025-03-24 11:01:26,227 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5413\n",
            "2025-03-24 11:01:33,954 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5179\n",
            "2025-03-24 11:01:41,561 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5866\n",
            "2025-03-24 11:01:49,563 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5183\n",
            "2025-03-24 11:01:57,587 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4907\n",
            "2025-03-24 11:02:05,973 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5109\n",
            "2025-03-24 11:02:14,298 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5778\n",
            "2025-03-24 11:02:22,370 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5530\n",
            "2025-03-24 11:02:30,534 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5417\n",
            "2025-03-24 11:02:36,347 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3826\n",
            "2025-03-24 11:02:36,930 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1697\n",
            "2025-03-24 11:02:36,931 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:03:41,142 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5381, Val Loss: 0.3631, Val Acc: 0.8599\n",
            "2025-03-24 11:03:41,544 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 11:03:41,545 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 11:03:48,288 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3350\n",
            "2025-03-24 11:03:57,561 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5370\n",
            "2025-03-24 11:04:06,695 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4505\n",
            "2025-03-24 11:04:15,712 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5302\n",
            "2025-03-24 11:04:24,302 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5567\n",
            "2025-03-24 11:04:32,909 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4886\n",
            "2025-03-24 11:04:41,302 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5192\n",
            "2025-03-24 11:04:50,109 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4984\n",
            "2025-03-24 11:04:58,241 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5677\n",
            "2025-03-24 11:05:05,973 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4657\n",
            "2025-03-24 11:05:14,007 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4365\n",
            "2025-03-24 11:05:21,674 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5215\n",
            "2025-03-24 11:05:29,841 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4972\n",
            "2025-03-24 11:05:37,455 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5173\n",
            "2025-03-24 11:05:45,078 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5228\n",
            "2025-03-24 11:05:52,871 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4667\n",
            "2025-03-24 11:06:00,867 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5193\n",
            "2025-03-24 11:06:08,465 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4673\n",
            "2025-03-24 11:06:16,107 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4412\n",
            "2025-03-24 11:06:23,988 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5831\n",
            "2025-03-24 11:06:32,280 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4940\n",
            "2025-03-24 11:06:40,062 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4770\n",
            "2025-03-24 11:06:47,642 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4345\n",
            "2025-03-24 11:06:55,590 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4662\n",
            "2025-03-24 11:07:03,516 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4893\n",
            "2025-03-24 11:07:11,073 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5236\n",
            "2025-03-24 11:07:18,842 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4820\n",
            "2025-03-24 11:07:26,343 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5741\n",
            "2025-03-24 11:07:34,051 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5271\n",
            "2025-03-24 11:07:41,339 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5110\n",
            "2025-03-24 11:07:48,606 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5903\n",
            "2025-03-24 11:07:56,239 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4666\n",
            "2025-03-24 11:08:03,620 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4907\n",
            "2025-03-24 11:08:09,256 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4032\n",
            "2025-03-24 11:08:09,819 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1541\n",
            "2025-03-24 11:08:09,820 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:09:05,487 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.5039, Val Loss: 0.3697, Val Acc: 0.8678\n",
            "2025-03-24 11:09:05,488 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 11:09:11,016 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3440\n",
            "2025-03-24 11:09:18,379 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5068\n",
            "2025-03-24 11:09:25,950 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4950\n",
            "2025-03-24 11:09:33,343 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4272\n",
            "2025-03-24 11:09:40,721 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5268\n",
            "2025-03-24 11:09:48,185 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4685\n",
            "2025-03-24 11:09:55,805 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4944\n",
            "2025-03-24 11:10:03,203 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4192\n",
            "2025-03-24 11:10:10,553 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4661\n",
            "2025-03-24 11:10:17,883 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4455\n",
            "2025-03-24 11:10:25,397 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4889\n",
            "2025-03-24 11:10:32,713 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4812\n",
            "2025-03-24 11:10:40,109 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4799\n",
            "2025-03-24 11:10:47,286 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5046\n",
            "2025-03-24 11:10:54,787 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4374\n",
            "2025-03-24 11:11:02,062 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4271\n",
            "2025-03-24 11:11:09,510 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4379\n",
            "2025-03-24 11:11:16,896 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4297\n",
            "2025-03-24 11:11:24,242 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4836\n",
            "2025-03-24 11:11:31,891 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4665\n",
            "2025-03-24 11:11:39,051 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5056\n",
            "2025-03-24 11:11:46,289 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4892\n",
            "2025-03-24 11:11:53,758 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4843\n",
            "2025-03-24 11:12:01,482 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4410\n",
            "2025-03-24 11:12:08,789 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4565\n",
            "2025-03-24 11:12:16,159 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4910\n",
            "2025-03-24 11:12:23,582 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4812\n",
            "2025-03-24 11:12:31,293 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5252\n",
            "2025-03-24 11:12:38,480 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4655\n",
            "2025-03-24 11:12:45,952 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4706\n",
            "2025-03-24 11:12:53,423 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4280\n",
            "2025-03-24 11:13:00,752 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4905\n",
            "2025-03-24 11:13:08,075 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5053\n",
            "2025-03-24 11:13:13,525 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3267\n",
            "2025-03-24 11:13:14,070 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1032\n",
            "2025-03-24 11:13:14,071 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:14:09,414 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4709, Val Loss: 0.3631, Val Acc: 0.8697\n",
            "2025-03-24 11:14:09,415 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 11:14:15,484 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3107\n",
            "2025-03-24 11:14:23,207 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4823\n",
            "2025-03-24 11:14:31,719 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4455\n",
            "2025-03-24 11:14:39,918 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4455\n",
            "2025-03-24 11:14:48,313 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4013\n",
            "2025-03-24 11:14:56,454 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3919\n",
            "2025-03-24 11:15:04,711 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4354\n",
            "2025-03-24 11:15:12,736 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4350\n",
            "2025-03-24 11:15:20,569 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4223\n",
            "2025-03-24 11:15:28,814 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4420\n",
            "2025-03-24 11:15:36,856 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4737\n",
            "2025-03-24 11:15:45,111 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4213\n",
            "2025-03-24 11:15:53,310 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4479\n",
            "2025-03-24 11:16:01,449 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5424\n",
            "2025-03-24 11:16:09,245 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4168\n",
            "2025-03-24 11:16:17,136 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4544\n",
            "2025-03-24 11:16:25,093 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4595\n",
            "2025-03-24 11:16:33,093 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4670\n",
            "2025-03-24 11:16:40,685 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4266\n",
            "2025-03-24 11:16:48,393 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4311\n",
            "2025-03-24 11:16:56,243 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4282\n",
            "2025-03-24 11:17:03,892 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4410\n",
            "2025-03-24 11:17:11,473 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4310\n",
            "2025-03-24 11:17:19,378 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3990\n",
            "2025-03-24 11:17:27,036 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4068\n",
            "2025-03-24 11:17:34,648 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4167\n",
            "2025-03-24 11:17:42,483 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5101\n",
            "2025-03-24 11:17:49,937 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4447\n",
            "2025-03-24 11:17:57,633 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4110\n",
            "2025-03-24 11:18:05,042 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4425\n",
            "2025-03-24 11:18:12,408 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4140\n",
            "2025-03-24 11:18:20,243 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4716\n",
            "2025-03-24 11:18:27,783 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4789\n",
            "2025-03-24 11:18:33,649 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-24 11:18:34,200 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0833\n",
            "2025-03-24 11:18:34,200 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:19:29,847 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4405, Val Loss: 0.3920, Val Acc: 0.8613\n",
            "2025-03-24 11:19:29,847 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 11:19:35,444 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2744\n",
            "2025-03-24 11:19:43,175 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3529\n",
            "2025-03-24 11:19:50,524 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3537\n",
            "2025-03-24 11:19:58,159 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3896\n",
            "2025-03-24 11:20:05,819 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4530\n",
            "2025-03-24 11:20:13,222 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4438\n",
            "2025-03-24 11:20:20,620 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4405\n",
            "2025-03-24 11:20:28,107 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3774\n",
            "2025-03-24 11:20:35,799 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4155\n",
            "2025-03-24 11:20:43,396 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3763\n",
            "2025-03-24 11:20:50,919 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4593\n",
            "2025-03-24 11:20:58,392 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3835\n",
            "2025-03-24 11:21:05,911 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3562\n",
            "2025-03-24 11:21:13,389 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4681\n",
            "2025-03-24 11:21:20,729 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4201\n",
            "2025-03-24 11:21:28,172 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3887\n",
            "2025-03-24 11:21:35,710 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4506\n",
            "2025-03-24 11:21:43,304 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4319\n",
            "2025-03-24 11:21:50,820 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4069\n",
            "2025-03-24 11:21:58,493 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3847\n",
            "2025-03-24 11:22:05,984 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5249\n",
            "2025-03-24 11:22:13,355 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4013\n",
            "2025-03-24 11:22:20,654 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4012\n",
            "2025-03-24 11:22:28,310 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4287\n",
            "2025-03-24 11:22:35,833 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4040\n",
            "2025-03-24 11:22:43,269 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4173\n",
            "2025-03-24 11:22:50,456 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4066\n",
            "2025-03-24 11:22:57,935 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4810\n",
            "2025-03-24 11:23:05,485 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3568\n",
            "2025-03-24 11:23:12,943 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4205\n",
            "2025-03-24 11:23:20,477 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4772\n",
            "2025-03-24 11:23:27,746 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4270\n",
            "2025-03-24 11:23:35,323 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4224\n",
            "2025-03-24 11:23:41,144 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3241\n",
            "2025-03-24 11:23:41,670 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1412\n",
            "2025-03-24 11:23:41,670 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:24:36,803 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4166, Val Loss: 0.3669, Val Acc: 0.8683\n",
            "2025-03-24 11:24:36,804 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 11:24:42,585 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2893\n",
            "2025-03-24 11:24:50,140 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3960\n",
            "2025-03-24 11:24:57,680 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3478\n",
            "2025-03-24 11:25:05,128 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4149\n",
            "2025-03-24 11:25:12,603 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3642\n",
            "2025-03-24 11:25:20,183 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3366\n",
            "2025-03-24 11:25:27,960 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3943\n",
            "2025-03-24 11:25:35,267 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3503\n",
            "2025-03-24 11:25:42,941 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3763\n",
            "2025-03-24 11:25:50,496 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3604\n",
            "2025-03-24 11:25:57,917 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3757\n",
            "2025-03-24 11:26:05,529 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4169\n",
            "2025-03-24 11:26:13,497 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3967\n",
            "2025-03-24 11:26:21,098 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3585\n",
            "2025-03-24 11:26:28,608 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3673\n",
            "2025-03-24 11:26:36,064 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3476\n",
            "2025-03-24 11:26:43,450 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3641\n",
            "2025-03-24 11:26:51,097 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4396\n",
            "2025-03-24 11:26:58,616 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3938\n",
            "2025-03-24 11:27:06,298 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3738\n",
            "2025-03-24 11:27:14,093 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3171\n",
            "2025-03-24 11:27:21,689 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3683\n",
            "2025-03-24 11:27:29,484 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3749\n",
            "2025-03-24 11:27:38,090 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3588\n",
            "2025-03-24 11:27:46,779 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3359\n",
            "2025-03-24 11:27:55,118 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3831\n",
            "2025-03-24 11:28:03,865 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3962\n",
            "2025-03-24 11:28:12,461 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4035\n",
            "2025-03-24 11:28:20,988 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3686\n",
            "2025-03-24 11:28:29,355 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3807\n",
            "2025-03-24 11:28:37,059 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3911\n",
            "2025-03-24 11:28:44,663 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3453\n",
            "2025-03-24 11:28:52,226 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3718\n",
            "2025-03-24 11:28:58,047 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2802\n",
            "2025-03-24 11:28:58,584 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0748\n",
            "2025-03-24 11:28:58,585 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:29:57,539 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3738, Val Loss: 0.3491, Val Acc: 0.8795\n",
            "2025-03-24 11:29:57,854 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 11:29:57,855 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 11:30:04,162 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2486\n",
            "2025-03-24 11:30:11,754 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3515\n",
            "2025-03-24 11:30:19,440 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3239\n",
            "2025-03-24 11:30:27,206 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3589\n",
            "2025-03-24 11:30:35,334 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3553\n",
            "2025-03-24 11:30:43,552 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3219\n",
            "2025-03-24 11:30:52,202 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3418\n",
            "2025-03-24 11:31:00,220 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3513\n",
            "2025-03-24 11:31:08,745 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3408\n",
            "2025-03-24 11:31:17,607 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3517\n",
            "2025-03-24 11:31:26,022 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3488\n",
            "2025-03-24 11:31:34,719 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3735\n",
            "2025-03-24 11:31:43,409 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3468\n",
            "2025-03-24 11:31:51,860 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3478\n",
            "2025-03-24 11:32:00,405 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3612\n",
            "2025-03-24 11:32:08,266 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3417\n",
            "2025-03-24 11:32:16,138 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3675\n",
            "2025-03-24 11:32:23,905 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3539\n",
            "2025-03-24 11:32:31,650 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3768\n",
            "2025-03-24 11:32:39,184 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3872\n",
            "2025-03-24 11:32:46,787 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3203\n",
            "2025-03-24 11:32:54,386 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3493\n",
            "2025-03-24 11:33:02,182 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3607\n",
            "2025-03-24 11:33:09,844 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3250\n",
            "2025-03-24 11:33:17,671 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3212\n",
            "2025-03-24 11:33:25,174 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3644\n",
            "2025-03-24 11:33:32,896 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3504\n",
            "2025-03-24 11:33:40,371 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3693\n",
            "2025-03-24 11:33:47,991 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3090\n",
            "2025-03-24 11:33:55,772 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3223\n",
            "2025-03-24 11:34:03,566 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3770\n",
            "2025-03-24 11:34:11,131 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3605\n",
            "2025-03-24 11:34:18,907 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3279\n",
            "2025-03-24 11:34:24,567 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3252\n",
            "2025-03-24 11:34:25,143 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1458\n",
            "2025-03-24 11:34:25,143 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:35:21,669 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3520, Val Loss: 0.3629, Val Acc: 0.8753\n",
            "2025-03-24 11:35:21,669 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 11:35:27,544 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2653\n",
            "2025-03-24 11:35:35,217 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3556\n",
            "2025-03-24 11:35:42,838 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3790\n",
            "2025-03-24 11:35:50,556 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3456\n",
            "2025-03-24 11:35:58,523 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3507\n",
            "2025-03-24 11:36:06,328 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2959\n",
            "2025-03-24 11:36:14,022 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3721\n",
            "2025-03-24 11:36:21,183 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3131\n",
            "2025-03-24 11:36:28,868 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3364\n",
            "2025-03-24 11:36:37,534 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2987\n",
            "2025-03-24 11:36:46,116 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3458\n",
            "2025-03-24 11:36:53,656 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3310\n",
            "2025-03-24 11:37:01,779 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3804\n",
            "2025-03-24 11:37:09,455 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3286\n",
            "2025-03-24 11:37:17,227 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3502\n",
            "2025-03-24 11:37:24,879 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3350\n",
            "2025-03-24 11:37:32,913 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3580\n",
            "2025-03-24 11:37:40,581 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3093\n",
            "2025-03-24 11:37:48,068 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-24 11:37:55,604 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3138\n",
            "2025-03-24 11:38:03,462 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3387\n",
            "2025-03-24 11:38:11,128 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3334\n",
            "2025-03-24 11:38:18,692 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3318\n",
            "2025-03-24 11:38:26,329 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3375\n",
            "2025-03-24 11:38:34,217 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3918\n",
            "2025-03-24 11:38:41,799 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3251\n",
            "2025-03-24 11:38:49,524 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3057\n",
            "2025-03-24 11:38:57,219 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3606\n",
            "2025-03-24 11:39:04,830 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3677\n",
            "2025-03-24 11:39:12,642 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3487\n",
            "2025-03-24 11:39:20,402 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3444\n",
            "2025-03-24 11:39:28,224 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3554\n",
            "2025-03-24 11:39:35,824 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3451\n",
            "2025-03-24 11:39:41,602 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2572\n",
            "2025-03-24 11:39:42,205 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0967\n",
            "2025-03-24 11:39:42,205 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:40:38,858 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3414, Val Loss: 0.3657, Val Acc: 0.8725\n",
            "2025-03-24 11:40:38,858 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 11:40:44,750 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2287\n",
            "2025-03-24 11:40:52,416 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3069\n",
            "2025-03-24 11:41:00,398 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3612\n",
            "2025-03-24 11:41:08,198 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3053\n",
            "2025-03-24 11:41:16,027 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3533\n",
            "2025-03-24 11:41:24,910 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3205\n",
            "2025-03-24 11:41:33,741 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3320\n",
            "2025-03-24 11:41:42,209 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3200\n",
            "2025-03-24 11:41:51,024 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3302\n",
            "2025-03-24 11:41:59,528 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3378\n",
            "2025-03-24 11:42:07,532 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3555\n",
            "2025-03-24 11:42:15,351 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3058\n",
            "2025-03-24 11:42:22,994 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3094\n",
            "2025-03-24 11:42:30,620 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3066\n",
            "2025-03-24 11:42:38,115 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3027\n",
            "2025-03-24 11:42:45,852 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3390\n",
            "2025-03-24 11:42:53,518 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3309\n",
            "2025-03-24 11:43:01,112 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3293\n",
            "2025-03-24 11:43:08,851 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3349\n",
            "2025-03-24 11:43:16,555 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3201\n",
            "2025-03-24 11:43:24,290 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3375\n",
            "2025-03-24 11:43:32,039 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3258\n",
            "2025-03-24 11:43:40,598 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3255\n",
            "2025-03-24 11:43:49,210 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3423\n",
            "2025-03-24 11:43:57,790 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3301\n",
            "2025-03-24 11:44:06,135 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3126\n",
            "2025-03-24 11:44:14,214 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3457\n",
            "2025-03-24 11:44:22,984 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-24 11:44:31,208 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3221\n",
            "2025-03-24 11:44:39,697 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3123\n",
            "2025-03-24 11:44:48,376 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-24 11:44:56,769 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3408\n",
            "2025-03-24 11:45:04,763 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3402\n",
            "2025-03-24 11:45:10,966 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2521\n",
            "2025-03-24 11:45:11,575 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1105\n",
            "2025-03-24 11:45:11,576 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:46:10,038 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3270, Val Loss: 0.3759, Val Acc: 0.8730\n",
            "2025-03-24 11:46:10,039 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 11:46:15,892 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2105\n",
            "2025-03-24 11:46:23,449 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3202\n",
            "2025-03-24 11:46:31,342 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2832\n",
            "2025-03-24 11:46:38,940 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3303\n",
            "2025-03-24 11:46:47,308 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3202\n",
            "2025-03-24 11:46:55,800 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3375\n",
            "2025-03-24 11:47:03,802 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3004\n",
            "2025-03-24 11:47:12,119 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3087\n",
            "2025-03-24 11:47:20,538 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3155\n",
            "2025-03-24 11:47:28,965 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3027\n",
            "2025-03-24 11:47:37,346 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3203\n",
            "2025-03-24 11:47:45,273 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3191\n",
            "2025-03-24 11:47:52,784 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3265\n",
            "2025-03-24 11:48:00,302 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3425\n",
            "2025-03-24 11:48:07,879 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-24 11:48:15,710 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3375\n",
            "2025-03-24 11:48:23,402 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2997\n",
            "2025-03-24 11:48:30,901 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3441\n",
            "2025-03-24 11:48:38,420 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3179\n",
            "2025-03-24 11:48:46,078 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-24 11:48:54,096 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3408\n",
            "2025-03-24 11:49:02,505 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3244\n",
            "2025-03-24 11:49:11,172 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2951\n",
            "2025-03-24 11:49:19,535 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2999\n",
            "2025-03-24 11:49:27,868 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3425\n",
            "2025-03-24 11:49:36,438 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3060\n",
            "2025-03-24 11:49:44,893 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3172\n",
            "2025-03-24 11:49:53,340 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3319\n",
            "2025-03-24 11:50:01,801 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3176\n",
            "2025-03-24 11:50:10,668 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3253\n",
            "2025-03-24 11:50:19,215 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3242\n",
            "2025-03-24 11:50:27,873 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3198\n",
            "2025-03-24 11:50:36,633 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3444\n",
            "2025-03-24 11:50:43,010 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2360\n",
            "2025-03-24 11:50:43,636 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0938\n",
            "2025-03-24 11:50:43,637 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:51:44,844 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3181, Val Loss: 0.3735, Val Acc: 0.8748\n",
            "2025-03-24 11:51:44,844 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 11:51:51,241 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2116\n",
            "2025-03-24 11:51:58,999 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2939\n",
            "2025-03-24 11:52:07,437 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 11:52:15,636 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3003\n",
            "2025-03-24 11:52:23,662 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3217\n",
            "2025-03-24 11:52:31,641 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3083\n",
            "2025-03-24 11:52:39,372 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2985\n",
            "2025-03-24 11:52:47,241 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3053\n",
            "2025-03-24 11:52:55,341 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-24 11:53:03,233 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3146\n",
            "2025-03-24 11:53:11,160 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2990\n",
            "2025-03-24 11:53:19,183 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3017\n",
            "2025-03-24 11:53:27,621 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-24 11:53:36,222 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3127\n",
            "2025-03-24 11:53:44,890 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-24 11:53:53,536 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2982\n",
            "2025-03-24 11:54:01,966 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2986\n",
            "2025-03-24 11:54:10,436 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2949\n",
            "2025-03-24 11:54:19,221 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3141\n",
            "2025-03-24 11:54:27,407 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 11:54:35,307 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-24 11:54:43,348 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-24 11:54:51,582 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2986\n",
            "2025-03-24 11:54:59,556 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3035\n",
            "2025-03-24 11:55:07,386 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3285\n",
            "2025-03-24 11:55:15,143 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2920\n",
            "2025-03-24 11:55:23,581 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2880\n",
            "2025-03-24 11:55:31,979 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3297\n",
            "2025-03-24 11:55:40,174 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2812\n",
            "2025-03-24 11:55:48,331 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-24 11:55:56,751 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3042\n",
            "2025-03-24 11:56:05,215 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3216\n",
            "2025-03-24 11:56:13,927 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3250\n",
            "2025-03-24 11:56:20,371 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2304\n",
            "2025-03-24 11:56:20,987 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1309\n",
            "2025-03-24 11:56:20,988 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 11:57:22,555 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.3040, Val Loss: 0.3749, Val Acc: 0.8730\n",
            "2025-03-24 11:57:22,556 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 11:57:29,150 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2028\n",
            "2025-03-24 11:57:37,943 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3010\n",
            "2025-03-24 11:57:46,500 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3035\n",
            "2025-03-24 11:57:55,104 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2817\n",
            "2025-03-24 11:58:03,727 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3032\n",
            "2025-03-24 11:58:12,312 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2797\n",
            "2025-03-24 11:58:20,692 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-24 11:58:29,286 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2883\n",
            "2025-03-24 11:58:37,739 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2906\n",
            "2025-03-24 11:58:46,385 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3158\n",
            "2025-03-24 11:58:54,896 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-24 11:59:03,541 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2936\n",
            "2025-03-24 11:59:12,719 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2984\n",
            "2025-03-24 11:59:21,461 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3245\n",
            "2025-03-24 11:59:29,865 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3133\n",
            "2025-03-24 11:59:38,758 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2961\n",
            "2025-03-24 11:59:47,912 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-24 11:59:56,473 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3230\n",
            "2025-03-24 12:00:04,689 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2998\n",
            "2025-03-24 12:00:13,145 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-24 12:00:21,495 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-24 12:00:29,955 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3061\n",
            "2025-03-24 12:00:38,692 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2845\n",
            "2025-03-24 12:00:47,083 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3295\n",
            "2025-03-24 12:00:55,338 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2811\n",
            "2025-03-24 12:01:03,679 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2971\n",
            "2025-03-24 12:01:12,073 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2998\n",
            "2025-03-24 12:01:20,431 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-24 12:01:28,867 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3100\n",
            "2025-03-24 12:01:37,032 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2932\n",
            "2025-03-24 12:01:45,581 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-24 12:01:54,019 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2999\n",
            "2025-03-24 12:02:02,545 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2997\n",
            "2025-03-24 12:02:09,062 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2018\n",
            "2025-03-24 12:02:09,702 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0939\n",
            "2025-03-24 12:02:09,703 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:03:10,360 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2987, Val Loss: 0.3709, Val Acc: 0.8748\n",
            "2025-03-24 12:03:10,360 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 12:03:16,848 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2322\n",
            "2025-03-24 12:03:25,173 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-24 12:03:33,666 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2952\n",
            "2025-03-24 12:03:42,429 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2811\n",
            "2025-03-24 12:03:50,393 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3119\n",
            "2025-03-24 12:03:58,684 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2850\n",
            "2025-03-24 12:04:07,028 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2933\n",
            "2025-03-24 12:04:15,214 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3074\n",
            "2025-03-24 12:04:23,377 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2985\n",
            "2025-03-24 12:04:31,622 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3131\n",
            "2025-03-24 12:04:40,019 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3095\n",
            "2025-03-24 12:04:48,340 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2978\n",
            "2025-03-24 12:04:56,813 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2855\n",
            "2025-03-24 12:05:05,539 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2828\n",
            "2025-03-24 12:05:13,836 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-24 12:05:22,177 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2750\n",
            "2025-03-24 12:05:30,384 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-24 12:05:38,822 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3163\n",
            "2025-03-24 12:05:47,022 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3046\n",
            "2025-03-24 12:05:55,163 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-24 12:06:03,387 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-24 12:06:11,558 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2855\n",
            "2025-03-24 12:06:19,751 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3147\n",
            "2025-03-24 12:06:28,008 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2888\n",
            "2025-03-24 12:06:35,966 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2947\n",
            "2025-03-24 12:06:44,179 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2847\n",
            "2025-03-24 12:06:52,371 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2849\n",
            "2025-03-24 12:07:00,367 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3007\n",
            "2025-03-24 12:07:08,776 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-24 12:07:17,298 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2751\n",
            "2025-03-24 12:07:25,885 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3315\n",
            "2025-03-24 12:07:34,427 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3062\n",
            "2025-03-24 12:07:43,140 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3160\n",
            "2025-03-24 12:07:49,458 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2019\n",
            "2025-03-24 12:07:50,105 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1379\n",
            "2025-03-24 12:07:50,105 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:08:49,335 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2971, Val Loss: 0.3849, Val Acc: 0.8762\n",
            "2025-03-24 12:08:49,335 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-24 12:08:55,374 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2500\n",
            "2025-03-24 12:09:03,509 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-24 12:09:11,911 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2868\n",
            "2025-03-24 12:09:20,186 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2957\n",
            "2025-03-24 12:09:28,325 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2914\n",
            "2025-03-24 12:09:36,586 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2923\n",
            "2025-03-24 12:09:45,033 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3063\n",
            "2025-03-24 12:09:53,598 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3041\n",
            "2025-03-24 12:10:02,031 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-24 12:10:10,212 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2780\n",
            "2025-03-24 12:10:18,280 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2909\n",
            "2025-03-24 12:10:26,443 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2863\n",
            "2025-03-24 12:10:34,818 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2996\n",
            "2025-03-24 12:10:43,089 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2984\n",
            "2025-03-24 12:10:51,416 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2891\n",
            "2025-03-24 12:10:59,340 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2942\n",
            "2025-03-24 12:11:07,648 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-24 12:11:15,621 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-24 12:11:23,976 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3200\n",
            "2025-03-24 12:11:32,529 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-24 12:11:41,100 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2907\n",
            "2025-03-24 12:11:49,599 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3131\n",
            "2025-03-24 12:11:58,041 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-24 12:12:06,822 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2891\n",
            "2025-03-24 12:12:15,354 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 12:12:23,848 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2843\n",
            "2025-03-24 12:12:32,492 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2745\n",
            "2025-03-24 12:12:41,152 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3025\n",
            "2025-03-24 12:12:49,633 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-24 12:12:58,269 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2817\n",
            "2025-03-24 12:13:06,629 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-24 12:13:15,443 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2967\n",
            "2025-03-24 12:13:24,255 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2807\n",
            "2025-03-24 12:13:30,658 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2218\n",
            "2025-03-24 12:13:31,313 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1016\n",
            "2025-03-24 12:13:31,313 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:14:28,329 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2904, Val Loss: 0.3738, Val Acc: 0.8767\n",
            "2025-03-24 12:14:28,330 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-24 12:14:34,432 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2178\n",
            "2025-03-24 12:14:42,257 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2750\n",
            "2025-03-24 12:14:49,983 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2926\n",
            "2025-03-24 12:14:57,983 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-24 12:15:05,696 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2588\n",
            "2025-03-24 12:15:13,534 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3146\n",
            "2025-03-24 12:15:21,152 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2859\n",
            "2025-03-24 12:15:29,018 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2770\n",
            "2025-03-24 12:15:37,181 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2955\n",
            "2025-03-24 12:15:45,813 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2623\n",
            "2025-03-24 12:15:54,349 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-24 12:16:02,767 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2703\n",
            "2025-03-24 12:16:11,148 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3161\n",
            "2025-03-24 12:16:19,536 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2996\n",
            "2025-03-24 12:16:28,239 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3055\n",
            "2025-03-24 12:16:36,211 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-24 12:16:44,400 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-24 12:16:52,384 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2780\n",
            "2025-03-24 12:16:59,773 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2532\n",
            "2025-03-24 12:17:07,980 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-24 12:17:15,469 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-24 12:17:23,103 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2839\n",
            "2025-03-24 12:17:30,654 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3077\n",
            "2025-03-24 12:17:38,583 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2851\n",
            "2025-03-24 12:17:47,013 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2784\n",
            "2025-03-24 12:17:55,373 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3007\n",
            "2025-03-24 12:18:03,463 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2843\n",
            "2025-03-24 12:18:11,769 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2825\n",
            "2025-03-24 12:18:20,211 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2874\n",
            "2025-03-24 12:18:28,723 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-24 12:18:37,138 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2957\n",
            "2025-03-24 12:18:45,700 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2828\n",
            "2025-03-24 12:18:54,156 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2949\n",
            "2025-03-24 12:19:00,641 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2052\n",
            "2025-03-24 12:19:01,256 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0689\n",
            "2025-03-24 12:19:01,257 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:20:04,274 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2872, Val Loss: 0.3729, Val Acc: 0.8739\n",
            "2025-03-24 12:20:04,275 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-24 12:20:10,890 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2142\n",
            "2025-03-24 12:20:19,730 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3040\n",
            "2025-03-24 12:20:28,087 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-24 12:20:36,790 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-24 12:20:45,247 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-24 12:20:53,974 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3074\n",
            "2025-03-24 12:21:02,918 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-24 12:21:11,493 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2881\n",
            "2025-03-24 12:21:19,989 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-24 12:21:28,522 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2832\n",
            "2025-03-24 12:21:37,176 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3154\n",
            "2025-03-24 12:21:45,941 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2934\n",
            "2025-03-24 12:21:54,719 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2937\n",
            "2025-03-24 12:22:03,270 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2729\n",
            "2025-03-24 12:22:12,131 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-24 12:22:20,992 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-24 12:22:29,015 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2662\n",
            "2025-03-24 12:22:37,688 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2853\n",
            "2025-03-24 12:22:46,045 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-24 12:22:54,020 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3101\n",
            "2025-03-24 12:23:01,512 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2802\n",
            "2025-03-24 12:23:09,243 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-24 12:23:17,870 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-24 12:23:26,190 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2841\n",
            "2025-03-24 12:23:33,790 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-24 12:23:42,038 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2826\n",
            "2025-03-24 12:23:50,054 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3196\n",
            "2025-03-24 12:23:58,377 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2763\n",
            "2025-03-24 12:24:06,662 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2973\n",
            "2025-03-24 12:24:15,012 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-24 12:24:23,388 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3237\n",
            "2025-03-24 12:24:31,652 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2942\n",
            "2025-03-24 12:24:39,917 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-24 12:24:45,858 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2146\n",
            "2025-03-24 12:24:46,457 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0754\n",
            "2025-03-24 12:24:46,458 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:25:44,760 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2891, Val Loss: 0.3755, Val Acc: 0.8790\n",
            "2025-03-24 12:25:44,760 - INFO - [TRAIN INFO] Early stopping at epoch 20 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 12:25:44,761 - INFO - [TRAIN INFO] Total Time: 6594.82s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▂▃▃▄▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▂▃▄▄▅▆▇█▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▂▃▄▄▅▆▇█▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▂▃▄▄▅▆▇█▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▂▃▄▄▅▆▇█▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▅▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▂▁▁▁▁▂▂▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate_classifier</td><td>0.00014</td></tr><tr><td>learning_rate_fusion</td><td>3e-05</td></tr><tr><td>learning_rate_image</td><td>3e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.28909</td></tr><tr><td>train_val_loss_diff</td><td>-0.08638</td></tr><tr><td>val_accuracy</td><td>0.87901</td></tr><tr><td>val_loss</td><td>0.37547</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_enetb0_224_simple_concat_fold_1</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/v81jf9e8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/v81jf9e8</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_103548-v81jf9e8\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 12:25:47,057 - INFO - [TRAIN INFO] Fold 1 Training Complete at epoch 20. Total Time: 6597.12s\n",
            "2025-03-24 12:25:47,083 - INFO - [K-FOLD INFO] Fold 1 completed in 6599.20 seconds\n",
            "2025-03-24 12:25:47,085 - INFO - [K-FOLD INFO] ============================== Fold 2/5 ==============================\n",
            "2025-03-24 12:25:47,090 - INFO - [K-FOLD INFO] Fold 2:\n",
            "2025-03-24 12:25:47,092 - INFO -    Train Samples: 8594\n",
            "2025-03-24 12:25:47,093 - INFO -    Validation Samples: 2149\n",
            "2025-03-24 12:25:47,094 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 2\n",
            "2025-03-24 12:25:47,096 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 2:\n",
            "2025-03-24 12:25:47,097 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 12:25:47,730 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 2\n",
            "2025-03-24 12:25:47,732 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 2:\n",
            "2025-03-24 12:25:47,733 - INFO - [K-FOLD INFO] Loss function initialized for Fold 2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_enetb0_224_simple_concat\\wandb\\run-20250324_122547-8t6uaa5e</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/8t6uaa5e' target=\"_blank\">experiment_multimodal_enetb0_224_simple_concat_fold_2</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/8t6uaa5e' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/8t6uaa5e</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 12:25:48,763 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 12:25:48,764 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 12:25:55,186 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0967\n",
            "2025-03-24 12:26:03,813 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4355\n",
            "2025-03-24 12:26:11,981 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3797\n",
            "2025-03-24 12:26:20,022 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3205\n",
            "2025-03-24 12:26:28,417 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3017\n",
            "2025-03-24 12:26:36,383 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2577\n",
            "2025-03-24 12:26:44,867 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2646\n",
            "2025-03-24 12:26:53,211 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2077\n",
            "2025-03-24 12:27:01,205 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2167\n",
            "2025-03-24 12:27:09,344 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2086\n",
            "2025-03-24 12:27:17,034 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2452\n",
            "2025-03-24 12:27:24,745 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1794\n",
            "2025-03-24 12:27:32,483 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1561\n",
            "2025-03-24 12:27:40,250 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1067\n",
            "2025-03-24 12:27:48,241 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1245\n",
            "2025-03-24 12:27:56,106 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.0497\n",
            "2025-03-24 12:28:03,799 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.0662\n",
            "2025-03-24 12:28:11,706 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0415\n",
            "2025-03-24 12:28:19,288 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0332\n",
            "2025-03-24 12:28:27,042 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0392\n",
            "2025-03-24 12:28:34,752 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0537\n",
            "2025-03-24 12:28:42,543 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0396\n",
            "2025-03-24 12:28:50,220 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.9488\n",
            "2025-03-24 12:28:57,790 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0183\n",
            "2025-03-24 12:29:05,572 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9692\n",
            "2025-03-24 12:29:13,062 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9651\n",
            "2025-03-24 12:29:20,666 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9999\n",
            "2025-03-24 12:29:28,513 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9092\n",
            "2025-03-24 12:29:36,033 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9086\n",
            "2025-03-24 12:29:43,726 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.8498\n",
            "2025-03-24 12:29:51,355 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8971\n",
            "2025-03-24 12:29:58,949 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8576\n",
            "2025-03-24 12:30:06,550 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.8967\n",
            "2025-03-24 12:30:12,144 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6700\n",
            "2025-03-24 12:30:12,696 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2270\n",
            "2025-03-24 12:30:12,697 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:31:07,838 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.0946, Val Loss: 0.6702, Val Acc: 0.7557\n",
            "2025-03-24 12:31:08,112 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 12:31:08,113 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 12:31:14,323 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6743\n",
            "2025-03-24 12:31:22,486 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.8400\n",
            "2025-03-24 12:31:30,516 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8565\n",
            "2025-03-24 12:31:38,244 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8308\n",
            "2025-03-24 12:31:45,728 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8109\n",
            "2025-03-24 12:31:53,367 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8409\n",
            "2025-03-24 12:32:00,915 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8706\n",
            "2025-03-24 12:32:08,618 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8603\n",
            "2025-03-24 12:32:15,939 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7453\n",
            "2025-03-24 12:32:23,223 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8272\n",
            "2025-03-24 12:32:30,708 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7591\n",
            "2025-03-24 12:32:38,434 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8222\n",
            "2025-03-24 12:32:46,051 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7803\n",
            "2025-03-24 12:32:53,701 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7809\n",
            "2025-03-24 12:33:01,320 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7254\n",
            "2025-03-24 12:33:08,892 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7523\n",
            "2025-03-24 12:33:16,461 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7799\n",
            "2025-03-24 12:33:24,064 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7297\n",
            "2025-03-24 12:33:31,720 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7003\n",
            "2025-03-24 12:33:39,279 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7844\n",
            "2025-03-24 12:33:46,608 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7457\n",
            "2025-03-24 12:33:53,950 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7505\n",
            "2025-03-24 12:34:01,523 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7281\n",
            "2025-03-24 12:34:09,090 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7760\n",
            "2025-03-24 12:34:16,537 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7249\n",
            "2025-03-24 12:34:23,959 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6624\n",
            "2025-03-24 12:34:31,234 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7529\n",
            "2025-03-24 12:34:39,064 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6799\n",
            "2025-03-24 12:34:46,566 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6852\n",
            "2025-03-24 12:34:53,861 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6393\n",
            "2025-03-24 12:35:01,466 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7328\n",
            "2025-03-24 12:35:09,175 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6716\n",
            "2025-03-24 12:35:16,542 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7178\n",
            "2025-03-24 12:35:22,163 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5141\n",
            "2025-03-24 12:35:22,787 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2146\n",
            "2025-03-24 12:35:22,787 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:36:24,875 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.7635, Val Loss: 0.4955, Val Acc: 0.8041\n",
            "2025-03-24 12:36:25,243 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 12:36:25,244 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 12:36:31,836 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5067\n",
            "2025-03-24 12:36:40,914 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6475\n",
            "2025-03-24 12:36:49,413 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6470\n",
            "2025-03-24 12:36:57,676 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6485\n",
            "2025-03-24 12:37:06,276 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5830\n",
            "2025-03-24 12:37:14,831 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6490\n",
            "2025-03-24 12:37:23,305 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6368\n",
            "2025-03-24 12:37:31,820 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6327\n",
            "2025-03-24 12:37:40,447 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6422\n",
            "2025-03-24 12:37:49,103 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6722\n",
            "2025-03-24 12:37:57,609 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6807\n",
            "2025-03-24 12:38:06,113 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5797\n",
            "2025-03-24 12:38:14,901 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5982\n",
            "2025-03-24 12:38:23,437 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6216\n",
            "2025-03-24 12:38:31,813 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6344\n",
            "2025-03-24 12:38:40,389 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6495\n",
            "2025-03-24 12:38:48,845 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6212\n",
            "2025-03-24 12:38:57,276 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7583\n",
            "2025-03-24 12:39:05,837 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6495\n",
            "2025-03-24 12:39:14,238 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5872\n",
            "2025-03-24 12:39:22,985 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6223\n",
            "2025-03-24 12:39:31,397 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6883\n",
            "2025-03-24 12:39:39,982 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6429\n",
            "2025-03-24 12:39:48,466 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6281\n",
            "2025-03-24 12:39:56,978 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6241\n",
            "2025-03-24 12:40:05,467 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6577\n",
            "2025-03-24 12:40:13,865 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6790\n",
            "2025-03-24 12:40:22,241 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5691\n",
            "2025-03-24 12:40:30,388 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6026\n",
            "2025-03-24 12:40:38,962 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5842\n",
            "2025-03-24 12:40:47,429 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6961\n",
            "2025-03-24 12:40:55,725 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6812\n",
            "2025-03-24 12:41:04,155 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5888\n",
            "2025-03-24 12:41:10,264 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5272\n",
            "2025-03-24 12:41:10,878 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1296\n",
            "2025-03-24 12:41:10,878 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:42:12,076 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6390, Val Loss: 0.4361, Val Acc: 0.8311\n",
            "2025-03-24 12:42:12,393 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 12:42:12,394 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 12:42:18,707 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4752\n",
            "2025-03-24 12:42:27,287 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5694\n",
            "2025-03-24 12:42:35,957 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6719\n",
            "2025-03-24 12:42:44,553 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6559\n",
            "2025-03-24 12:42:52,971 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6281\n",
            "2025-03-24 12:43:01,412 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5855\n",
            "2025-03-24 12:43:10,008 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6244\n",
            "2025-03-24 12:43:18,420 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5548\n",
            "2025-03-24 12:43:26,964 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5637\n",
            "2025-03-24 12:43:35,362 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5458\n",
            "2025-03-24 12:43:43,594 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6345\n",
            "2025-03-24 12:43:52,243 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6242\n",
            "2025-03-24 12:44:00,760 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6203\n",
            "2025-03-24 12:44:09,290 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6102\n",
            "2025-03-24 12:44:17,854 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5398\n",
            "2025-03-24 12:44:26,296 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5554\n",
            "2025-03-24 12:44:34,877 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6194\n",
            "2025-03-24 12:44:43,376 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5588\n",
            "2025-03-24 12:44:52,025 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6033\n",
            "2025-03-24 12:45:00,708 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5504\n",
            "2025-03-24 12:45:09,130 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5176\n",
            "2025-03-24 12:45:17,650 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5980\n",
            "2025-03-24 12:45:26,186 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6031\n",
            "2025-03-24 12:45:34,854 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5545\n",
            "2025-03-24 12:45:43,633 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4964\n",
            "2025-03-24 12:45:52,397 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5494\n",
            "2025-03-24 12:46:00,918 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5818\n",
            "2025-03-24 12:46:10,049 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6104\n",
            "2025-03-24 12:46:18,660 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5235\n",
            "2025-03-24 12:46:27,234 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6288\n",
            "2025-03-24 12:46:35,803 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6172\n",
            "2025-03-24 12:46:44,678 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6061\n",
            "2025-03-24 12:46:53,685 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5698\n",
            "2025-03-24 12:47:00,179 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4160\n",
            "2025-03-24 12:47:00,853 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1568\n",
            "2025-03-24 12:47:00,854 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:48:02,440 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5873, Val Loss: 0.4092, Val Acc: 0.8371\n",
            "2025-03-24 12:48:02,754 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 12:48:02,754 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 12:48:09,225 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4056\n",
            "2025-03-24 12:48:17,852 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5812\n",
            "2025-03-24 12:48:26,212 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5975\n",
            "2025-03-24 12:48:35,212 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5289\n",
            "2025-03-24 12:48:43,816 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5011\n",
            "2025-03-24 12:48:52,147 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5037\n",
            "2025-03-24 12:49:00,639 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5444\n",
            "2025-03-24 12:49:09,268 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5022\n",
            "2025-03-24 12:49:17,755 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5188\n",
            "2025-03-24 12:49:26,413 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4854\n",
            "2025-03-24 12:49:35,197 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5353\n",
            "2025-03-24 12:49:43,524 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5468\n",
            "2025-03-24 12:49:52,186 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4991\n",
            "2025-03-24 12:50:00,693 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5253\n",
            "2025-03-24 12:50:09,110 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5612\n",
            "2025-03-24 12:50:17,576 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5170\n",
            "2025-03-24 12:50:25,936 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5231\n",
            "2025-03-24 12:50:34,520 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4847\n",
            "2025-03-24 12:50:42,951 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4755\n",
            "2025-03-24 12:50:51,377 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6221\n",
            "2025-03-24 12:50:59,742 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5703\n",
            "2025-03-24 12:51:08,419 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4951\n",
            "2025-03-24 12:51:16,966 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5246\n",
            "2025-03-24 12:51:25,467 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5473\n",
            "2025-03-24 12:51:34,145 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5720\n",
            "2025-03-24 12:51:42,717 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5784\n",
            "2025-03-24 12:51:51,103 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5603\n",
            "2025-03-24 12:51:59,745 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5750\n",
            "2025-03-24 12:52:08,378 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5239\n",
            "2025-03-24 12:52:17,149 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5529\n",
            "2025-03-24 12:52:25,746 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5831\n",
            "2025-03-24 12:52:34,223 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5456\n",
            "2025-03-24 12:52:42,945 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5384\n",
            "2025-03-24 12:52:49,448 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4085\n",
            "2025-03-24 12:52:50,112 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1348\n",
            "2025-03-24 12:52:50,113 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:53:50,793 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5383, Val Loss: 0.3923, Val Acc: 0.8511\n",
            "2025-03-24 12:53:51,111 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 12:53:51,111 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 12:53:57,580 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3807\n",
            "2025-03-24 12:54:06,056 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4973\n",
            "2025-03-24 12:54:14,703 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4759\n",
            "2025-03-24 12:54:23,168 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4518\n",
            "2025-03-24 12:54:31,553 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4720\n",
            "2025-03-24 12:54:39,977 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4936\n",
            "2025-03-24 12:54:48,469 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4467\n",
            "2025-03-24 12:54:56,967 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5952\n",
            "2025-03-24 12:55:05,491 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4525\n",
            "2025-03-24 12:55:13,949 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4083\n",
            "2025-03-24 12:55:22,543 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4623\n",
            "2025-03-24 12:55:31,110 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4950\n",
            "2025-03-24 12:55:39,416 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5234\n",
            "2025-03-24 12:55:47,825 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4696\n",
            "2025-03-24 12:55:56,495 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4898\n",
            "2025-03-24 12:56:05,054 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4788\n",
            "2025-03-24 12:56:13,479 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4257\n",
            "2025-03-24 12:56:22,044 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4771\n",
            "2025-03-24 12:56:30,590 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5913\n",
            "2025-03-24 12:56:39,460 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5059\n",
            "2025-03-24 12:56:47,725 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4951\n",
            "2025-03-24 12:56:56,453 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5501\n",
            "2025-03-24 12:57:04,933 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4797\n",
            "2025-03-24 12:57:13,354 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5074\n",
            "2025-03-24 12:57:22,060 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5140\n",
            "2025-03-24 12:57:30,550 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5421\n",
            "2025-03-24 12:57:39,014 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5071\n",
            "2025-03-24 12:57:47,564 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4730\n",
            "2025-03-24 12:57:55,997 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5003\n",
            "2025-03-24 12:58:04,521 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5548\n",
            "2025-03-24 12:58:12,997 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4588\n",
            "2025-03-24 12:58:21,371 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4833\n",
            "2025-03-24 12:58:29,619 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5322\n",
            "2025-03-24 12:58:36,030 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3550\n",
            "2025-03-24 12:58:36,644 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0881\n",
            "2025-03-24 12:58:36,645 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 12:59:37,934 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4929, Val Loss: 0.3779, Val Acc: 0.8599\n",
            "2025-03-24 12:59:38,243 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 12:59:38,244 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 12:59:44,780 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3457\n",
            "2025-03-24 12:59:53,216 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4458\n",
            "2025-03-24 13:00:01,502 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4296\n",
            "2025-03-24 13:00:10,249 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3997\n",
            "2025-03-24 13:00:18,961 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3958\n",
            "2025-03-24 13:00:27,368 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4214\n",
            "2025-03-24 13:00:36,050 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4087\n",
            "2025-03-24 13:00:44,801 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5165\n",
            "2025-03-24 13:00:53,228 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4645\n",
            "2025-03-24 13:01:01,763 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5091\n",
            "2025-03-24 13:01:10,196 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4666\n",
            "2025-03-24 13:01:18,666 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4875\n",
            "2025-03-24 13:01:27,060 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4845\n",
            "2025-03-24 13:01:35,796 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5083\n",
            "2025-03-24 13:01:44,568 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5207\n",
            "2025-03-24 13:01:53,105 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5229\n",
            "2025-03-24 13:02:01,735 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4119\n",
            "2025-03-24 13:02:10,355 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4845\n",
            "2025-03-24 13:02:18,798 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4554\n",
            "2025-03-24 13:02:27,015 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4441\n",
            "2025-03-24 13:02:35,442 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4854\n",
            "2025-03-24 13:02:43,959 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4868\n",
            "2025-03-24 13:02:52,545 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4421\n",
            "2025-03-24 13:03:01,019 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5227\n",
            "2025-03-24 13:03:10,147 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5071\n",
            "2025-03-24 13:03:18,612 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5616\n",
            "2025-03-24 13:03:27,035 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4427\n",
            "2025-03-24 13:03:35,583 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4646\n",
            "2025-03-24 13:03:44,180 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4415\n",
            "2025-03-24 13:03:52,740 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4830\n",
            "2025-03-24 13:04:01,339 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5505\n",
            "2025-03-24 13:04:09,957 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4719\n",
            "2025-03-24 13:04:18,451 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5276\n",
            "2025-03-24 13:04:24,890 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3607\n",
            "2025-03-24 13:04:25,547 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2688\n",
            "2025-03-24 13:04:25,548 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:05:26,205 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4782, Val Loss: 0.3688, Val Acc: 0.8627\n",
            "2025-03-24 13:05:26,527 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 13:05:26,527 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 13:05:33,117 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2945\n",
            "2025-03-24 13:05:41,566 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4325\n",
            "2025-03-24 13:05:50,110 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4221\n",
            "2025-03-24 13:05:58,649 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4230\n",
            "2025-03-24 13:06:07,356 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4423\n",
            "2025-03-24 13:06:16,098 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4272\n",
            "2025-03-24 13:06:24,879 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4503\n",
            "2025-03-24 13:06:33,890 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4118\n",
            "2025-03-24 13:06:42,570 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3804\n",
            "2025-03-24 13:06:51,148 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4163\n",
            "2025-03-24 13:06:59,699 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4313\n",
            "2025-03-24 13:07:08,479 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5006\n",
            "2025-03-24 13:07:17,078 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4574\n",
            "2025-03-24 13:07:25,771 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4011\n",
            "2025-03-24 13:07:34,469 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4774\n",
            "2025-03-24 13:07:42,875 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4445\n",
            "2025-03-24 13:07:51,668 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5123\n",
            "2025-03-24 13:08:00,235 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4931\n",
            "2025-03-24 13:08:08,887 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4107\n",
            "2025-03-24 13:08:17,490 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4028\n",
            "2025-03-24 13:08:26,445 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4553\n",
            "2025-03-24 13:08:35,022 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4214\n",
            "2025-03-24 13:08:43,491 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4778\n",
            "2025-03-24 13:08:51,827 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4129\n",
            "2025-03-24 13:09:00,271 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4200\n",
            "2025-03-24 13:09:08,685 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4052\n",
            "2025-03-24 13:09:16,998 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4155\n",
            "2025-03-24 13:09:25,469 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4692\n",
            "2025-03-24 13:09:33,926 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4387\n",
            "2025-03-24 13:09:42,339 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4395\n",
            "2025-03-24 13:09:50,819 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5461\n",
            "2025-03-24 13:09:59,226 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4494\n",
            "2025-03-24 13:10:07,630 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4834\n",
            "2025-03-24 13:10:14,218 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3731\n",
            "2025-03-24 13:10:14,845 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0769\n",
            "2025-03-24 13:10:14,846 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:11:18,799 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4420, Val Loss: 0.3693, Val Acc: 0.8664\n",
            "2025-03-24 13:11:18,800 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 13:11:25,396 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3057\n",
            "2025-03-24 13:11:34,150 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3667\n",
            "2025-03-24 13:11:42,986 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3719\n",
            "2025-03-24 13:11:52,008 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4269\n",
            "2025-03-24 13:12:00,780 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3761\n",
            "2025-03-24 13:12:09,470 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4040\n",
            "2025-03-24 13:12:18,333 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4153\n",
            "2025-03-24 13:12:27,174 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3682\n",
            "2025-03-24 13:12:35,783 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4256\n",
            "2025-03-24 13:12:44,559 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3676\n",
            "2025-03-24 13:12:53,168 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3670\n",
            "2025-03-24 13:13:01,852 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4194\n",
            "2025-03-24 13:13:10,715 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4293\n",
            "2025-03-24 13:13:19,523 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4286\n",
            "2025-03-24 13:13:28,277 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4181\n",
            "2025-03-24 13:13:37,117 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3936\n",
            "2025-03-24 13:13:45,865 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3449\n",
            "2025-03-24 13:13:54,750 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4723\n",
            "2025-03-24 13:14:03,509 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4328\n",
            "2025-03-24 13:14:12,285 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4631\n",
            "2025-03-24 13:14:20,947 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5383\n",
            "2025-03-24 13:14:29,933 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4462\n",
            "2025-03-24 13:14:38,676 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3999\n",
            "2025-03-24 13:14:47,550 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4312\n",
            "2025-03-24 13:14:56,340 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3959\n",
            "2025-03-24 13:15:05,249 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4393\n",
            "2025-03-24 13:15:13,731 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4080\n",
            "2025-03-24 13:15:22,228 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4643\n",
            "2025-03-24 13:15:30,526 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3739\n",
            "2025-03-24 13:15:39,087 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4162\n",
            "2025-03-24 13:15:47,678 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4326\n",
            "2025-03-24 13:15:56,407 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4974\n",
            "2025-03-24 13:16:04,874 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4623\n",
            "2025-03-24 13:16:11,055 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3277\n",
            "2025-03-24 13:16:11,680 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0739\n",
            "2025-03-24 13:16:11,681 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:17:12,517 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4179, Val Loss: 0.3755, Val Acc: 0.8664\n",
            "2025-03-24 13:17:12,518 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 13:17:18,955 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3157\n",
            "2025-03-24 13:17:27,491 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3796\n",
            "2025-03-24 13:17:35,890 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4008\n",
            "2025-03-24 13:17:44,336 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3946\n",
            "2025-03-24 13:17:53,118 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3750\n",
            "2025-03-24 13:18:01,599 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3806\n",
            "2025-03-24 13:18:10,245 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3438\n",
            "2025-03-24 13:18:18,956 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3703\n",
            "2025-03-24 13:18:27,470 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3813\n",
            "2025-03-24 13:18:36,007 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3698\n",
            "2025-03-24 13:18:44,542 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3945\n",
            "2025-03-24 13:18:53,073 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3648\n",
            "2025-03-24 13:19:01,854 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3799\n",
            "2025-03-24 13:19:10,471 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4111\n",
            "2025-03-24 13:19:19,047 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4118\n",
            "2025-03-24 13:19:27,557 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3574\n",
            "2025-03-24 13:19:36,043 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3638\n",
            "2025-03-24 13:19:44,445 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3804\n",
            "2025-03-24 13:19:52,814 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3787\n",
            "2025-03-24 13:20:01,245 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3856\n",
            "2025-03-24 13:20:09,808 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3720\n",
            "2025-03-24 13:20:18,288 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3757\n",
            "2025-03-24 13:20:26,629 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4013\n",
            "2025-03-24 13:20:35,115 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3419\n",
            "2025-03-24 13:20:43,786 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3909\n",
            "2025-03-24 13:20:52,198 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3578\n",
            "2025-03-24 13:21:00,658 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4076\n",
            "2025-03-24 13:21:09,247 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3877\n",
            "2025-03-24 13:21:17,803 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3735\n",
            "2025-03-24 13:21:26,212 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3491\n",
            "2025-03-24 13:21:34,748 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4378\n",
            "2025-03-24 13:21:43,300 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4271\n",
            "2025-03-24 13:21:51,721 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4680\n",
            "2025-03-24 13:21:58,068 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-24 13:21:58,672 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1016\n",
            "2025-03-24 13:21:58,672 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:22:59,567 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3857, Val Loss: 0.3866, Val Acc: 0.8683\n",
            "2025-03-24 13:22:59,568 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 13:23:05,992 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2484\n",
            "2025-03-24 13:23:14,418 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3801\n",
            "2025-03-24 13:23:22,888 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3680\n",
            "2025-03-24 13:23:31,579 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3405\n",
            "2025-03-24 13:23:39,968 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3309\n",
            "2025-03-24 13:23:48,371 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3598\n",
            "2025-03-24 13:23:56,955 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3509\n",
            "2025-03-24 13:24:05,457 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3484\n",
            "2025-03-24 13:24:13,927 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3755\n",
            "2025-03-24 13:24:22,429 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3856\n",
            "2025-03-24 13:24:31,078 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3654\n",
            "2025-03-24 13:24:39,929 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3869\n",
            "2025-03-24 13:24:48,551 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3736\n",
            "2025-03-24 13:24:57,041 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3596\n",
            "2025-03-24 13:25:05,542 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3228\n",
            "2025-03-24 13:25:13,996 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3638\n",
            "2025-03-24 13:25:22,416 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3729\n",
            "2025-03-24 13:25:31,120 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3913\n",
            "2025-03-24 13:25:39,392 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3534\n",
            "2025-03-24 13:25:48,015 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4389\n",
            "2025-03-24 13:25:56,402 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4013\n",
            "2025-03-24 13:26:04,942 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3629\n",
            "2025-03-24 13:26:13,407 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3328\n",
            "2025-03-24 13:26:21,947 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3442\n",
            "2025-03-24 13:26:30,368 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3877\n",
            "2025-03-24 13:26:38,812 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3687\n",
            "2025-03-24 13:26:47,280 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3866\n",
            "2025-03-24 13:26:55,722 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3965\n",
            "2025-03-24 13:27:04,378 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3905\n",
            "2025-03-24 13:27:12,906 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4023\n",
            "2025-03-24 13:27:21,465 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4675\n",
            "2025-03-24 13:27:29,790 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4102\n",
            "2025-03-24 13:27:38,265 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3674\n",
            "2025-03-24 13:27:44,770 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2634\n",
            "2025-03-24 13:27:45,403 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1433\n",
            "2025-03-24 13:27:45,404 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:28:46,130 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3746, Val Loss: 0.3837, Val Acc: 0.8609\n",
            "2025-03-24 13:28:46,131 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 13:28:52,597 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2312\n",
            "2025-03-24 13:29:01,056 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3530\n",
            "2025-03-24 13:29:09,468 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3474\n",
            "2025-03-24 13:29:18,141 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3607\n",
            "2025-03-24 13:29:26,517 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3486\n",
            "2025-03-24 13:29:34,872 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3232\n",
            "2025-03-24 13:29:43,242 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3049\n",
            "2025-03-24 13:29:51,866 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3585\n",
            "2025-03-24 13:30:00,547 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3511\n",
            "2025-03-24 13:30:09,032 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3348\n",
            "2025-03-24 13:30:17,662 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3644\n",
            "2025-03-24 13:30:26,234 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3436\n",
            "2025-03-24 13:30:35,046 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3681\n",
            "2025-03-24 13:30:43,589 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3450\n",
            "2025-03-24 13:30:52,322 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3416\n",
            "2025-03-24 13:31:01,040 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3422\n",
            "2025-03-24 13:31:09,547 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3277\n",
            "2025-03-24 13:31:18,107 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3436\n",
            "2025-03-24 13:31:26,834 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3389\n",
            "2025-03-24 13:31:35,292 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3418\n",
            "2025-03-24 13:31:43,841 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3303\n",
            "2025-03-24 13:31:52,321 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3499\n",
            "2025-03-24 13:32:00,833 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3084\n",
            "2025-03-24 13:32:09,527 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3051\n",
            "2025-03-24 13:32:17,787 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3285\n",
            "2025-03-24 13:32:26,137 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3443\n",
            "2025-03-24 13:32:34,691 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3461\n",
            "2025-03-24 13:32:42,921 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-24 13:32:51,356 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3455\n",
            "2025-03-24 13:32:59,820 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3120\n",
            "2025-03-24 13:33:08,177 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3320\n",
            "2025-03-24 13:33:16,776 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3271\n",
            "2025-03-24 13:33:25,397 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3266\n",
            "2025-03-24 13:33:31,673 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2526\n",
            "2025-03-24 13:33:32,344 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0772\n",
            "2025-03-24 13:33:32,345 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:34:34,182 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3367, Val Loss: 0.3729, Val Acc: 0.8692\n",
            "2025-03-24 13:34:34,184 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 13:34:40,734 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2557\n",
            "2025-03-24 13:34:49,372 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3240\n",
            "2025-03-24 13:34:57,977 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3066\n",
            "2025-03-24 13:35:06,569 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3710\n",
            "2025-03-24 13:35:15,309 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3098\n",
            "2025-03-24 13:35:23,970 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3429\n",
            "2025-03-24 13:35:33,180 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2796\n",
            "2025-03-24 13:35:41,768 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2983\n",
            "2025-03-24 13:35:50,727 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3245\n",
            "2025-03-24 13:35:59,148 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-24 13:36:07,934 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-24 13:36:16,525 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3090\n",
            "2025-03-24 13:36:25,339 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3437\n",
            "2025-03-24 13:36:34,273 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3326\n",
            "2025-03-24 13:36:43,095 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3139\n",
            "2025-03-24 13:36:51,911 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3520\n",
            "2025-03-24 13:37:00,745 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3432\n",
            "2025-03-24 13:37:09,281 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3095\n",
            "2025-03-24 13:37:17,888 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3287\n",
            "2025-03-24 13:37:26,715 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3071\n",
            "2025-03-24 13:37:35,399 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3014\n",
            "2025-03-24 13:37:43,945 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3114\n",
            "2025-03-24 13:37:52,629 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2980\n",
            "2025-03-24 13:38:01,310 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2946\n",
            "2025-03-24 13:38:09,953 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3119\n",
            "2025-03-24 13:38:18,715 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3473\n",
            "2025-03-24 13:38:27,391 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3051\n",
            "2025-03-24 13:38:36,102 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3192\n",
            "2025-03-24 13:38:44,901 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3273\n",
            "2025-03-24 13:38:53,701 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3197\n",
            "2025-03-24 13:39:02,294 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2792\n",
            "2025-03-24 13:39:11,077 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2839\n",
            "2025-03-24 13:39:19,692 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-24 13:39:26,083 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2619\n",
            "2025-03-24 13:39:26,703 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1023\n",
            "2025-03-24 13:39:26,704 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:40:28,077 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3169, Val Loss: 0.3828, Val Acc: 0.8730\n",
            "2025-03-24 13:40:28,077 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 13:40:34,668 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2733\n",
            "2025-03-24 13:40:43,352 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2771\n",
            "2025-03-24 13:40:52,023 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3088\n",
            "2025-03-24 13:41:00,670 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2800\n",
            "2025-03-24 13:41:09,435 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-24 13:41:18,247 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3680\n",
            "2025-03-24 13:41:27,262 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2961\n",
            "2025-03-24 13:41:36,260 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3180\n",
            "2025-03-24 13:41:45,040 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3087\n",
            "2025-03-24 13:41:53,523 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3203\n",
            "2025-03-24 13:42:02,009 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-24 13:42:10,697 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3390\n",
            "2025-03-24 13:42:19,356 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3070\n",
            "2025-03-24 13:42:28,087 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3116\n",
            "2025-03-24 13:42:36,828 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3163\n",
            "2025-03-24 13:42:45,286 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3180\n",
            "2025-03-24 13:42:53,875 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2817\n",
            "2025-03-24 13:43:02,591 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3028\n",
            "2025-03-24 13:43:11,018 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2739\n",
            "2025-03-24 13:43:19,603 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-24 13:43:28,040 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3396\n",
            "2025-03-24 13:43:36,820 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3289\n",
            "2025-03-24 13:43:45,482 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-24 13:43:54,013 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3099\n",
            "2025-03-24 13:44:02,408 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3239\n",
            "2025-03-24 13:44:10,918 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2879\n",
            "2025-03-24 13:44:19,347 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2905\n",
            "2025-03-24 13:44:27,892 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 13:44:36,349 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3343\n",
            "2025-03-24 13:44:44,789 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3053\n",
            "2025-03-24 13:44:53,347 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2923\n",
            "2025-03-24 13:45:01,819 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-24 13:45:10,221 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3264\n",
            "2025-03-24 13:45:16,771 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2516\n",
            "2025-03-24 13:45:17,399 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2162\n",
            "2025-03-24 13:45:17,399 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:46:17,948 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3123, Val Loss: 0.3751, Val Acc: 0.8744\n",
            "2025-03-24 13:46:17,949 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 13:46:24,362 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2053\n",
            "2025-03-24 13:46:32,837 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3177\n",
            "2025-03-24 13:46:41,416 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-24 13:46:50,339 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2686\n",
            "2025-03-24 13:46:58,910 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2838\n",
            "2025-03-24 13:47:07,317 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3003\n",
            "2025-03-24 13:47:15,921 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3107\n",
            "2025-03-24 13:47:24,544 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2914\n",
            "2025-03-24 13:47:33,011 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3168\n",
            "2025-03-24 13:47:41,527 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2958\n",
            "2025-03-24 13:47:50,341 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2904\n",
            "2025-03-24 13:47:58,915 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3325\n",
            "2025-03-24 13:48:07,535 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2997\n",
            "2025-03-24 13:48:16,137 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2924\n",
            "2025-03-24 13:48:24,728 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-24 13:48:33,188 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2965\n",
            "2025-03-24 13:48:41,925 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3024\n",
            "2025-03-24 13:48:50,569 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2981\n",
            "2025-03-24 13:48:59,249 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2845\n",
            "2025-03-24 13:49:07,909 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-24 13:49:16,411 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2995\n",
            "2025-03-24 13:49:25,494 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2762\n",
            "2025-03-24 13:49:34,104 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2913\n",
            "2025-03-24 13:49:42,958 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3041\n",
            "2025-03-24 13:49:51,892 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3098\n",
            "2025-03-24 13:50:00,485 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-24 13:50:09,077 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3120\n",
            "2025-03-24 13:50:17,709 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-24 13:50:26,467 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3083\n",
            "2025-03-24 13:50:34,952 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2889\n",
            "2025-03-24 13:50:43,451 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2925\n",
            "2025-03-24 13:50:52,077 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2879\n",
            "2025-03-24 13:51:00,676 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2850\n",
            "2025-03-24 13:51:06,860 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1952\n",
            "2025-03-24 13:51:07,492 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1087\n",
            "2025-03-24 13:51:07,493 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:52:08,892 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2978, Val Loss: 0.3899, Val Acc: 0.8711\n",
            "2025-03-24 13:52:08,892 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 13:52:15,189 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2073\n",
            "2025-03-24 13:52:23,606 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2641\n",
            "2025-03-24 13:52:32,148 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2948\n",
            "2025-03-24 13:52:40,641 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2807\n",
            "2025-03-24 13:52:49,229 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-24 13:52:57,676 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2715\n",
            "2025-03-24 13:53:06,198 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-24 13:53:14,843 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2670\n",
            "2025-03-24 13:53:23,234 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3197\n",
            "2025-03-24 13:53:32,001 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3050\n",
            "2025-03-24 13:53:40,631 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-24 13:53:49,433 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2964\n",
            "2025-03-24 13:53:57,990 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-24 13:54:06,826 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-24 13:54:15,687 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-24 13:54:24,616 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2748\n",
            "2025-03-24 13:54:33,245 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2947\n",
            "2025-03-24 13:54:41,168 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2814\n",
            "2025-03-24 13:54:49,111 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-24 13:54:57,367 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 13:55:05,570 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2832\n",
            "2025-03-24 13:55:13,998 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3198\n",
            "2025-03-24 13:55:22,085 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3075\n",
            "2025-03-24 13:55:30,395 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3177\n",
            "2025-03-24 13:55:38,781 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2735\n",
            "2025-03-24 13:55:46,994 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2802\n",
            "2025-03-24 13:55:55,260 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2953\n",
            "2025-03-24 13:56:03,588 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2762\n",
            "2025-03-24 13:56:11,746 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2922\n",
            "2025-03-24 13:56:19,986 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2857\n",
            "2025-03-24 13:56:28,417 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3107\n",
            "2025-03-24 13:56:36,783 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2765\n",
            "2025-03-24 13:56:44,955 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2698\n",
            "2025-03-24 13:56:51,200 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2138\n",
            "2025-03-24 13:56:51,808 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0604\n",
            "2025-03-24 13:56:51,809 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 13:57:51,898 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2876, Val Loss: 0.3823, Val Acc: 0.8720\n",
            "2025-03-24 13:57:51,898 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 13:57:58,201 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2008\n",
            "2025-03-24 13:58:06,352 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2934\n",
            "2025-03-24 13:58:14,739 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-24 13:58:23,076 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2981\n",
            "2025-03-24 13:58:31,540 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2807\n",
            "2025-03-24 13:58:39,795 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2843\n",
            "2025-03-24 13:58:48,176 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2713\n",
            "2025-03-24 13:58:56,347 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2771\n",
            "2025-03-24 13:59:04,556 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2683\n",
            "2025-03-24 13:59:12,767 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2766\n",
            "2025-03-24 13:59:20,891 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-24 13:59:29,282 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2833\n",
            "2025-03-24 13:59:37,488 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2710\n",
            "2025-03-24 13:59:45,849 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3166\n",
            "2025-03-24 13:59:54,240 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2859\n",
            "2025-03-24 14:00:02,920 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2917\n",
            "2025-03-24 14:00:11,142 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-24 14:00:19,696 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2971\n",
            "2025-03-24 14:00:28,747 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-24 14:00:37,513 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-24 14:00:45,790 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2722\n",
            "2025-03-24 14:00:54,299 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2777\n",
            "2025-03-24 14:01:02,513 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-24 14:01:11,095 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2751\n",
            "2025-03-24 14:01:19,334 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-24 14:01:27,663 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-24 14:01:36,094 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 14:01:44,620 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-24 14:01:53,066 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2980\n",
            "2025-03-24 14:02:00,882 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-24 14:02:08,589 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-24 14:02:16,232 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2925\n",
            "2025-03-24 14:02:24,211 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2481\n",
            "2025-03-24 14:02:30,221 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2051\n",
            "2025-03-24 14:02:30,805 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1627\n",
            "2025-03-24 14:02:30,806 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:03:29,505 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2855, Val Loss: 0.3780, Val Acc: 0.8762\n",
            "2025-03-24 14:03:29,506 - INFO - [TRAIN INFO] Early stopping at epoch 17 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 14:03:29,506 - INFO - [TRAIN INFO] Total Time: 5860.74s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇███▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇███▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇███▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇███▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▅▅▄▄▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇▇▇█▇██████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.28551</td></tr><tr><td>train_val_loss_diff</td><td>-0.09248</td></tr><tr><td>val_accuracy</td><td>0.87622</td></tr><tr><td>val_loss</td><td>0.37799</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_enetb0_224_simple_concat_fold_2</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/8t6uaa5e' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/8t6uaa5e</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_122547-8t6uaa5e\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 14:03:31,791 - INFO - [TRAIN INFO] Fold 2 Training Complete at epoch 17. Total Time: 5863.03s\n",
            "2025-03-24 14:03:31,814 - INFO - [K-FOLD INFO] Fold 2 completed in 5864.73 seconds\n",
            "2025-03-24 14:03:31,816 - INFO - [K-FOLD INFO] ============================== Fold 3/5 ==============================\n",
            "2025-03-24 14:03:31,822 - INFO - [K-FOLD INFO] Fold 3:\n",
            "2025-03-24 14:03:31,822 - INFO -    Train Samples: 8594\n",
            "2025-03-24 14:03:31,823 - INFO -    Validation Samples: 2149\n",
            "2025-03-24 14:03:31,824 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 3\n",
            "2025-03-24 14:03:31,825 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 3:\n",
            "2025-03-24 14:03:31,826 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 14:03:32,495 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 3\n",
            "2025-03-24 14:03:32,498 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 3:\n",
            "2025-03-24 14:03:32,498 - INFO - [K-FOLD INFO] Loss function initialized for Fold 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_enetb0_224_simple_concat\\wandb\\run-20250324_140332-qys4dkz8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/qys4dkz8' target=\"_blank\">experiment_multimodal_enetb0_224_simple_concat_fold_3</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/qys4dkz8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/qys4dkz8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 14:03:33,345 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 14:03:33,345 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 14:03:39,921 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0638\n",
            "2025-03-24 14:03:48,305 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.3514\n",
            "2025-03-24 14:03:56,401 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3834\n",
            "2025-03-24 14:04:04,237 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3352\n",
            "2025-03-24 14:04:12,379 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3227\n",
            "2025-03-24 14:04:20,437 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2471\n",
            "2025-03-24 14:04:28,603 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2242\n",
            "2025-03-24 14:04:36,808 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2531\n",
            "2025-03-24 14:04:45,218 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.1885\n",
            "2025-03-24 14:04:53,412 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2088\n",
            "2025-03-24 14:05:01,653 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2066\n",
            "2025-03-24 14:05:10,010 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1845\n",
            "2025-03-24 14:05:18,118 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1361\n",
            "2025-03-24 14:05:26,472 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1376\n",
            "2025-03-24 14:05:34,577 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.0963\n",
            "2025-03-24 14:05:42,684 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.0975\n",
            "2025-03-24 14:05:51,004 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.0502\n",
            "2025-03-24 14:05:58,976 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0569\n",
            "2025-03-24 14:06:07,261 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0219\n",
            "2025-03-24 14:06:15,225 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0323\n",
            "2025-03-24 14:06:23,504 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.9691\n",
            "2025-03-24 14:06:32,001 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0509\n",
            "2025-03-24 14:06:40,560 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.9643\n",
            "2025-03-24 14:06:48,851 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0141\n",
            "2025-03-24 14:06:57,545 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9750\n",
            "2025-03-24 14:07:05,985 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9792\n",
            "2025-03-24 14:07:14,591 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9828\n",
            "2025-03-24 14:07:23,082 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9444\n",
            "2025-03-24 14:07:31,499 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9638\n",
            "2025-03-24 14:07:39,936 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9347\n",
            "2025-03-24 14:07:48,477 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8757\n",
            "2025-03-24 14:07:56,985 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8966\n",
            "2025-03-24 14:08:05,408 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9272\n",
            "2025-03-24 14:08:11,770 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6487\n",
            "2025-03-24 14:08:12,403 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2003\n",
            "2025-03-24 14:08:12,403 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:09:14,518 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.0941, Val Loss: 0.6982, Val Acc: 0.7417\n",
            "2025-03-24 14:09:14,831 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 14:09:14,832 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 14:09:21,180 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6849\n",
            "2025-03-24 14:09:29,805 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.8901\n",
            "2025-03-24 14:09:38,547 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8382\n",
            "2025-03-24 14:09:47,331 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8735\n",
            "2025-03-24 14:09:56,058 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8145\n",
            "2025-03-24 14:10:04,730 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8103\n",
            "2025-03-24 14:10:13,488 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8130\n",
            "2025-03-24 14:10:22,531 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7969\n",
            "2025-03-24 14:10:31,250 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8557\n",
            "2025-03-24 14:10:40,140 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7274\n",
            "2025-03-24 14:10:48,920 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7871\n",
            "2025-03-24 14:10:57,547 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7186\n",
            "2025-03-24 14:11:06,315 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7355\n",
            "2025-03-24 14:11:14,986 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7292\n",
            "2025-03-24 14:11:23,748 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.8258\n",
            "2025-03-24 14:11:32,304 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7571\n",
            "2025-03-24 14:11:40,897 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8150\n",
            "2025-03-24 14:11:49,133 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6674\n",
            "2025-03-24 14:11:57,688 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7400\n",
            "2025-03-24 14:12:05,813 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7155\n",
            "2025-03-24 14:12:14,298 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7525\n",
            "2025-03-24 14:12:22,812 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6815\n",
            "2025-03-24 14:12:31,159 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7136\n",
            "2025-03-24 14:12:39,668 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7274\n",
            "2025-03-24 14:12:48,275 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7908\n",
            "2025-03-24 14:12:56,946 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6883\n",
            "2025-03-24 14:13:05,309 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6422\n",
            "2025-03-24 14:13:13,873 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6680\n",
            "2025-03-24 14:13:22,449 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7818\n",
            "2025-03-24 14:13:31,115 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6761\n",
            "2025-03-24 14:13:39,643 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6621\n",
            "2025-03-24 14:13:48,262 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7294\n",
            "2025-03-24 14:13:56,657 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6578\n",
            "2025-03-24 14:14:03,060 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5689\n",
            "2025-03-24 14:14:03,678 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1630\n",
            "2025-03-24 14:14:03,678 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:15:05,554 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.7555, Val Loss: 0.5297, Val Acc: 0.8046\n",
            "2025-03-24 14:15:05,868 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 14:15:05,869 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 14:15:12,330 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5139\n",
            "2025-03-24 14:15:20,836 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6782\n",
            "2025-03-24 14:15:29,432 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.7447\n",
            "2025-03-24 14:15:38,030 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6948\n",
            "2025-03-24 14:15:46,488 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6726\n",
            "2025-03-24 14:15:55,039 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6633\n",
            "2025-03-24 14:16:03,813 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6677\n",
            "2025-03-24 14:16:12,421 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6527\n",
            "2025-03-24 14:16:21,017 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6885\n",
            "2025-03-24 14:16:29,592 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6499\n",
            "2025-03-24 14:16:38,065 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6505\n",
            "2025-03-24 14:16:46,520 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6421\n",
            "2025-03-24 14:16:54,965 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5872\n",
            "2025-03-24 14:17:03,645 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7067\n",
            "2025-03-24 14:17:12,203 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5822\n",
            "2025-03-24 14:17:20,610 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6397\n",
            "2025-03-24 14:17:29,126 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7026\n",
            "2025-03-24 14:17:37,642 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6268\n",
            "2025-03-24 14:17:46,193 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6099\n",
            "2025-03-24 14:17:54,768 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6615\n",
            "2025-03-24 14:18:03,148 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6696\n",
            "2025-03-24 14:18:11,736 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6170\n",
            "2025-03-24 14:18:20,178 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6668\n",
            "2025-03-24 14:18:28,521 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6245\n",
            "2025-03-24 14:18:37,044 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7084\n",
            "2025-03-24 14:18:45,437 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5718\n",
            "2025-03-24 14:18:53,771 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6687\n",
            "2025-03-24 14:19:02,145 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6036\n",
            "2025-03-24 14:19:10,675 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6703\n",
            "2025-03-24 14:19:19,173 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6257\n",
            "2025-03-24 14:19:27,722 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5821\n",
            "2025-03-24 14:19:36,277 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6315\n",
            "2025-03-24 14:19:44,757 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5813\n",
            "2025-03-24 14:19:51,134 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4518\n",
            "2025-03-24 14:19:51,775 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1849\n",
            "2025-03-24 14:19:51,776 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:20:53,415 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6487, Val Loss: 0.4563, Val Acc: 0.8269\n",
            "2025-03-24 14:20:53,729 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 14:20:53,730 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 14:21:00,153 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5046\n",
            "2025-03-24 14:21:08,596 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5613\n",
            "2025-03-24 14:21:17,128 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5767\n",
            "2025-03-24 14:21:25,603 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6028\n",
            "2025-03-24 14:21:34,299 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6023\n",
            "2025-03-24 14:21:42,563 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6101\n",
            "2025-03-24 14:21:51,143 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6178\n",
            "2025-03-24 14:21:59,771 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5677\n",
            "2025-03-24 14:22:08,202 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6096\n",
            "2025-03-24 14:22:16,903 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6294\n",
            "2025-03-24 14:22:25,390 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5575\n",
            "2025-03-24 14:22:33,905 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5176\n",
            "2025-03-24 14:22:42,416 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5955\n",
            "2025-03-24 14:22:51,095 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5763\n",
            "2025-03-24 14:22:59,740 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5857\n",
            "2025-03-24 14:23:08,161 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6320\n",
            "2025-03-24 14:23:16,691 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6035\n",
            "2025-03-24 14:23:25,331 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5723\n",
            "2025-03-24 14:23:34,095 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5653\n",
            "2025-03-24 14:23:42,488 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5486\n",
            "2025-03-24 14:23:51,066 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5697\n",
            "2025-03-24 14:23:59,541 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5445\n",
            "2025-03-24 14:24:08,200 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5410\n",
            "2025-03-24 14:24:16,778 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6234\n",
            "2025-03-24 14:24:25,350 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6265\n",
            "2025-03-24 14:24:33,867 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5970\n",
            "2025-03-24 14:24:42,412 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5531\n",
            "2025-03-24 14:24:50,862 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5348\n",
            "2025-03-24 14:24:59,497 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5686\n",
            "2025-03-24 14:25:08,220 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5090\n",
            "2025-03-24 14:25:16,857 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5005\n",
            "2025-03-24 14:25:25,249 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5541\n",
            "2025-03-24 14:25:33,904 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5925\n",
            "2025-03-24 14:25:40,409 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4371\n",
            "2025-03-24 14:25:41,052 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2464\n",
            "2025-03-24 14:25:41,053 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:26:42,547 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5818, Val Loss: 0.4189, Val Acc: 0.8436\n",
            "2025-03-24 14:26:42,864 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 14:26:42,864 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 14:26:49,213 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4221\n",
            "2025-03-24 14:26:57,763 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5174\n",
            "2025-03-24 14:27:06,277 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4873\n",
            "2025-03-24 14:27:14,822 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5039\n",
            "2025-03-24 14:27:23,511 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5016\n",
            "2025-03-24 14:27:31,993 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5723\n",
            "2025-03-24 14:27:40,415 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5687\n",
            "2025-03-24 14:27:49,011 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4575\n",
            "2025-03-24 14:27:57,587 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5197\n",
            "2025-03-24 14:28:05,982 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5170\n",
            "2025-03-24 14:28:14,440 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5595\n",
            "2025-03-24 14:28:23,010 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5048\n",
            "2025-03-24 14:28:31,473 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4996\n",
            "2025-03-24 14:28:39,928 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5014\n",
            "2025-03-24 14:28:48,575 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5100\n",
            "2025-03-24 14:28:56,995 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5731\n",
            "2025-03-24 14:29:05,495 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5105\n",
            "2025-03-24 14:29:14,383 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5697\n",
            "2025-03-24 14:29:22,780 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5662\n",
            "2025-03-24 14:29:31,180 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5779\n",
            "2025-03-24 14:29:39,711 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5468\n",
            "2025-03-24 14:29:48,174 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5387\n",
            "2025-03-24 14:29:56,676 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5577\n",
            "2025-03-24 14:30:05,501 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6441\n",
            "2025-03-24 14:30:14,111 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5591\n",
            "2025-03-24 14:30:22,564 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5127\n",
            "2025-03-24 14:30:30,995 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6156\n",
            "2025-03-24 14:30:39,585 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5262\n",
            "2025-03-24 14:30:48,349 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5430\n",
            "2025-03-24 14:30:57,090 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4747\n",
            "2025-03-24 14:31:05,747 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4821\n",
            "2025-03-24 14:31:14,372 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4826\n",
            "2025-03-24 14:31:22,721 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5843\n",
            "2025-03-24 14:31:29,144 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4204\n",
            "2025-03-24 14:31:29,796 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0901\n",
            "2025-03-24 14:31:29,797 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:32:32,036 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5339, Val Loss: 0.4108, Val Acc: 0.8464\n",
            "2025-03-24 14:32:32,364 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 14:32:32,365 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 14:32:38,927 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3572\n",
            "2025-03-24 14:32:47,322 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4983\n",
            "2025-03-24 14:32:55,912 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5090\n",
            "2025-03-24 14:33:04,546 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5064\n",
            "2025-03-24 14:33:13,224 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4896\n",
            "2025-03-24 14:33:21,669 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5169\n",
            "2025-03-24 14:33:30,501 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5223\n",
            "2025-03-24 14:33:39,186 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5107\n",
            "2025-03-24 14:33:47,900 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4472\n",
            "2025-03-24 14:33:56,630 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5460\n",
            "2025-03-24 14:34:05,149 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4967\n",
            "2025-03-24 14:34:13,551 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4904\n",
            "2025-03-24 14:34:22,222 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4860\n",
            "2025-03-24 14:34:30,724 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5273\n",
            "2025-03-24 14:34:39,161 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4606\n",
            "2025-03-24 14:34:47,876 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4504\n",
            "2025-03-24 14:34:56,144 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4532\n",
            "2025-03-24 14:35:04,762 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4953\n",
            "2025-03-24 14:35:13,094 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4592\n",
            "2025-03-24 14:35:21,521 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4746\n",
            "2025-03-24 14:35:30,155 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5271\n",
            "2025-03-24 14:35:38,666 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5630\n",
            "2025-03-24 14:35:47,252 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4923\n",
            "2025-03-24 14:35:55,860 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5319\n",
            "2025-03-24 14:36:04,431 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4754\n",
            "2025-03-24 14:36:13,053 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4555\n",
            "2025-03-24 14:36:21,520 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5058\n",
            "2025-03-24 14:36:30,043 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5609\n",
            "2025-03-24 14:36:38,511 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4801\n",
            "2025-03-24 14:36:47,096 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4935\n",
            "2025-03-24 14:36:55,549 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5041\n",
            "2025-03-24 14:37:04,148 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4773\n",
            "2025-03-24 14:37:12,634 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4979\n",
            "2025-03-24 14:37:18,896 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4050\n",
            "2025-03-24 14:37:19,527 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0769\n",
            "2025-03-24 14:37:19,527 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:38:21,910 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4961, Val Loss: 0.3901, Val Acc: 0.8571\n",
            "2025-03-24 14:38:22,228 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 14:38:22,229 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 14:38:28,748 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3504\n",
            "2025-03-24 14:38:37,399 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4655\n",
            "2025-03-24 14:38:45,809 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4509\n",
            "2025-03-24 14:38:54,421 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4633\n",
            "2025-03-24 14:39:03,021 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4395\n",
            "2025-03-24 14:39:11,682 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4807\n",
            "2025-03-24 14:39:20,370 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4415\n",
            "2025-03-24 14:39:29,016 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4734\n",
            "2025-03-24 14:39:37,629 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4748\n",
            "2025-03-24 14:39:46,213 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4116\n",
            "2025-03-24 14:39:54,536 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4865\n",
            "2025-03-24 14:40:02,985 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4776\n",
            "2025-03-24 14:40:11,574 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4392\n",
            "2025-03-24 14:40:20,166 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4835\n",
            "2025-03-24 14:40:28,654 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4497\n",
            "2025-03-24 14:40:37,376 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5110\n",
            "2025-03-24 14:40:45,769 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4465\n",
            "2025-03-24 14:40:54,175 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4519\n",
            "2025-03-24 14:41:02,808 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4741\n",
            "2025-03-24 14:41:11,492 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4791\n",
            "2025-03-24 14:41:20,042 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5073\n",
            "2025-03-24 14:41:28,770 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4883\n",
            "2025-03-24 14:41:37,223 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4270\n",
            "2025-03-24 14:41:45,612 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4759\n",
            "2025-03-24 14:41:54,154 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5018\n",
            "2025-03-24 14:42:02,547 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4645\n",
            "2025-03-24 14:42:10,939 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4799\n",
            "2025-03-24 14:42:19,539 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4660\n",
            "2025-03-24 14:42:27,966 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4243\n",
            "2025-03-24 14:42:36,359 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4614\n",
            "2025-03-24 14:42:44,745 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5080\n",
            "2025-03-24 14:42:53,391 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5113\n",
            "2025-03-24 14:43:02,317 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5803\n",
            "2025-03-24 14:43:08,730 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3166\n",
            "2025-03-24 14:43:09,380 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0738\n",
            "2025-03-24 14:43:09,381 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:44:10,825 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4692, Val Loss: 0.3760, Val Acc: 0.8548\n",
            "2025-03-24 14:44:11,136 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 14:44:11,136 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 14:44:17,504 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3243\n",
            "2025-03-24 14:44:25,906 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4461\n",
            "2025-03-24 14:44:34,493 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4881\n",
            "2025-03-24 14:44:42,995 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4046\n",
            "2025-03-24 14:44:51,498 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4488\n",
            "2025-03-24 14:45:00,089 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4499\n",
            "2025-03-24 14:45:08,682 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4015\n",
            "2025-03-24 14:45:17,091 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3995\n",
            "2025-03-24 14:45:25,351 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3945\n",
            "2025-03-24 14:45:34,022 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4232\n",
            "2025-03-24 14:45:42,587 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4021\n",
            "2025-03-24 14:45:51,263 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4098\n",
            "2025-03-24 14:45:59,796 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4301\n",
            "2025-03-24 14:46:08,360 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4852\n",
            "2025-03-24 14:46:16,712 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4107\n",
            "2025-03-24 14:46:25,269 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4638\n",
            "2025-03-24 14:46:33,851 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4037\n",
            "2025-03-24 14:46:42,463 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4241\n",
            "2025-03-24 14:46:51,057 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4048\n",
            "2025-03-24 14:46:59,648 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4844\n",
            "2025-03-24 14:47:08,056 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4796\n",
            "2025-03-24 14:47:16,424 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4782\n",
            "2025-03-24 14:47:24,747 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4182\n",
            "2025-03-24 14:47:33,227 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4806\n",
            "2025-03-24 14:47:41,828 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5123\n",
            "2025-03-24 14:47:50,631 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4142\n",
            "2025-03-24 14:47:59,011 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4704\n",
            "2025-03-24 14:48:07,639 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4996\n",
            "2025-03-24 14:48:16,037 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4498\n",
            "2025-03-24 14:48:24,620 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4716\n",
            "2025-03-24 14:48:33,226 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3845\n",
            "2025-03-24 14:48:41,490 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4106\n",
            "2025-03-24 14:48:49,815 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4540\n",
            "2025-03-24 14:48:56,205 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3486\n",
            "2025-03-24 14:48:56,800 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1200\n",
            "2025-03-24 14:48:56,800 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:49:58,056 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4412, Val Loss: 0.3876, Val Acc: 0.8571\n",
            "2025-03-24 14:49:58,056 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 14:50:04,397 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3226\n",
            "2025-03-24 14:50:13,099 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3909\n",
            "2025-03-24 14:50:21,599 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4271\n",
            "2025-03-24 14:50:30,147 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4387\n",
            "2025-03-24 14:50:38,542 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4415\n",
            "2025-03-24 14:50:46,972 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4151\n",
            "2025-03-24 14:50:55,217 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3975\n",
            "2025-03-24 14:51:03,839 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4290\n",
            "2025-03-24 14:51:12,457 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4512\n",
            "2025-03-24 14:51:20,978 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4323\n",
            "2025-03-24 14:51:29,374 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3798\n",
            "2025-03-24 14:51:37,823 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4345\n",
            "2025-03-24 14:51:46,621 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4450\n",
            "2025-03-24 14:51:55,353 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4492\n",
            "2025-03-24 14:52:03,605 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4583\n",
            "2025-03-24 14:52:12,022 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4137\n",
            "2025-03-24 14:52:20,605 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3976\n",
            "2025-03-24 14:52:29,019 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4588\n",
            "2025-03-24 14:52:37,442 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3961\n",
            "2025-03-24 14:52:45,787 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3857\n",
            "2025-03-24 14:52:54,174 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4021\n",
            "2025-03-24 14:53:02,831 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3466\n",
            "2025-03-24 14:53:11,533 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4334\n",
            "2025-03-24 14:53:19,998 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3913\n",
            "2025-03-24 14:53:28,609 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4414\n",
            "2025-03-24 14:53:37,034 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4395\n",
            "2025-03-24 14:53:45,538 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3965\n",
            "2025-03-24 14:53:54,026 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4541\n",
            "2025-03-24 14:54:02,509 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3950\n",
            "2025-03-24 14:54:10,961 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4108\n",
            "2025-03-24 14:54:19,587 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4061\n",
            "2025-03-24 14:54:28,309 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4419\n",
            "2025-03-24 14:54:36,704 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4610\n",
            "2025-03-24 14:54:43,020 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3257\n",
            "2025-03-24 14:54:43,659 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1500\n",
            "2025-03-24 14:54:43,659 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 14:55:44,866 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4225, Val Loss: 0.3832, Val Acc: 0.8660\n",
            "2025-03-24 14:55:44,867 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 14:55:51,284 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2762\n",
            "2025-03-24 14:55:59,679 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3970\n",
            "2025-03-24 14:56:08,276 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3558\n",
            "2025-03-24 14:56:16,871 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3832\n",
            "2025-03-24 14:56:25,467 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3785\n",
            "2025-03-24 14:56:33,939 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4277\n",
            "2025-03-24 14:56:42,386 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3564\n",
            "2025-03-24 14:56:51,154 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3933\n",
            "2025-03-24 14:56:59,517 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3447\n",
            "2025-03-24 14:57:07,946 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3764\n",
            "2025-03-24 14:57:16,335 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3503\n",
            "2025-03-24 14:57:24,709 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3736\n",
            "2025-03-24 14:57:33,370 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3962\n",
            "2025-03-24 14:57:41,862 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4309\n",
            "2025-03-24 14:57:50,420 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3685\n",
            "2025-03-24 14:57:58,943 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3440\n",
            "2025-03-24 14:58:07,299 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4195\n",
            "2025-03-24 14:58:15,707 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4034\n",
            "2025-03-24 14:58:24,318 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3683\n",
            "2025-03-24 14:58:32,845 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4144\n",
            "2025-03-24 14:58:41,241 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4683\n",
            "2025-03-24 14:58:49,815 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4542\n",
            "2025-03-24 14:58:58,494 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4108\n",
            "2025-03-24 14:59:06,884 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3859\n",
            "2025-03-24 14:59:15,621 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4150\n",
            "2025-03-24 14:59:24,033 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3900\n",
            "2025-03-24 14:59:32,644 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4648\n",
            "2025-03-24 14:59:41,109 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4113\n",
            "2025-03-24 14:59:49,627 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4072\n",
            "2025-03-24 14:59:58,024 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3882\n",
            "2025-03-24 15:00:06,592 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3766\n",
            "2025-03-24 15:00:15,032 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4292\n",
            "2025-03-24 15:00:23,510 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4099\n",
            "2025-03-24 15:00:29,912 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3468\n",
            "2025-03-24 15:00:30,532 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0868\n",
            "2025-03-24 15:00:30,532 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:01:32,153 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3971, Val Loss: 0.3770, Val Acc: 0.8651\n",
            "2025-03-24 15:01:32,153 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 15:01:38,622 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3085\n",
            "2025-03-24 15:01:46,990 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3915\n",
            "2025-03-24 15:01:55,583 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3747\n",
            "2025-03-24 15:02:03,976 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3939\n",
            "2025-03-24 15:02:12,539 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3927\n",
            "2025-03-24 15:02:21,005 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3639\n",
            "2025-03-24 15:02:29,389 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3891\n",
            "2025-03-24 15:02:38,173 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-24 15:02:46,801 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3257\n",
            "2025-03-24 15:02:55,199 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3530\n",
            "2025-03-24 15:03:03,648 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3585\n",
            "2025-03-24 15:03:12,026 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3509\n",
            "2025-03-24 15:03:20,610 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3904\n",
            "2025-03-24 15:03:29,163 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3781\n",
            "2025-03-24 15:03:37,561 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3875\n",
            "2025-03-24 15:03:45,956 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3421\n",
            "2025-03-24 15:03:54,354 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4563\n",
            "2025-03-24 15:04:02,750 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4309\n",
            "2025-03-24 15:04:11,278 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3600\n",
            "2025-03-24 15:04:19,713 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3198\n",
            "2025-03-24 15:04:28,216 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3800\n",
            "2025-03-24 15:04:36,796 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3656\n",
            "2025-03-24 15:04:45,161 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4119\n",
            "2025-03-24 15:04:53,919 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3540\n",
            "2025-03-24 15:05:02,332 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3826\n",
            "2025-03-24 15:05:10,731 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3873\n",
            "2025-03-24 15:05:19,130 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3318\n",
            "2025-03-24 15:05:27,598 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3998\n",
            "2025-03-24 15:05:36,133 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3946\n",
            "2025-03-24 15:05:44,773 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3648\n",
            "2025-03-24 15:05:53,190 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3777\n",
            "2025-03-24 15:06:01,751 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3521\n",
            "2025-03-24 15:06:10,130 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3592\n",
            "2025-03-24 15:06:16,505 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2706\n",
            "2025-03-24 15:06:17,124 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0816\n",
            "2025-03-24 15:06:17,125 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:07:18,550 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3732, Val Loss: 0.3671, Val Acc: 0.8646\n",
            "2025-03-24 15:07:18,860 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 15:07:18,860 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 15:07:25,290 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2790\n",
            "2025-03-24 15:07:33,683 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3370\n",
            "2025-03-24 15:07:42,283 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3494\n",
            "2025-03-24 15:07:50,862 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3580\n",
            "2025-03-24 15:07:59,134 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3447\n",
            "2025-03-24 15:08:07,578 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3240\n",
            "2025-03-24 15:08:16,050 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3678\n",
            "2025-03-24 15:08:24,500 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3407\n",
            "2025-03-24 15:08:33,067 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3325\n",
            "2025-03-24 15:08:41,852 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3683\n",
            "2025-03-24 15:08:50,355 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3420\n",
            "2025-03-24 15:08:58,908 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3446\n",
            "2025-03-24 15:09:07,322 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3261\n",
            "2025-03-24 15:09:15,876 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3578\n",
            "2025-03-24 15:09:24,536 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3549\n",
            "2025-03-24 15:09:33,101 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3269\n",
            "2025-03-24 15:09:41,836 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3477\n",
            "2025-03-24 15:09:50,428 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3197\n",
            "2025-03-24 15:09:58,841 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3419\n",
            "2025-03-24 15:10:07,434 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3570\n",
            "2025-03-24 15:10:16,007 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3176\n",
            "2025-03-24 15:10:24,391 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3564\n",
            "2025-03-24 15:10:32,884 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3332\n",
            "2025-03-24 15:10:41,630 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3714\n",
            "2025-03-24 15:10:50,029 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3500\n",
            "2025-03-24 15:10:58,582 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3750\n",
            "2025-03-24 15:11:07,080 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3851\n",
            "2025-03-24 15:11:15,504 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3959\n",
            "2025-03-24 15:11:24,399 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3306\n",
            "2025-03-24 15:11:32,810 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3473\n",
            "2025-03-24 15:11:41,210 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3561\n",
            "2025-03-24 15:11:49,793 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3945\n",
            "2025-03-24 15:11:58,206 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3846\n",
            "2025-03-24 15:12:04,503 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2696\n",
            "2025-03-24 15:12:05,100 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0927\n",
            "2025-03-24 15:12:05,100 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:13:07,268 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3520, Val Loss: 0.3903, Val Acc: 0.8571\n",
            "2025-03-24 15:13:07,269 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 15:13:13,575 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2690\n",
            "2025-03-24 15:13:22,259 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3092\n",
            "2025-03-24 15:13:30,772 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3450\n",
            "2025-03-24 15:13:39,351 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3392\n",
            "2025-03-24 15:13:47,756 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3181\n",
            "2025-03-24 15:13:56,018 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3168\n",
            "2025-03-24 15:14:04,426 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-24 15:14:12,793 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3347\n",
            "2025-03-24 15:14:21,268 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3772\n",
            "2025-03-24 15:14:29,724 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3137\n",
            "2025-03-24 15:14:38,123 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3427\n",
            "2025-03-24 15:14:46,850 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3548\n",
            "2025-03-24 15:14:55,350 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3391\n",
            "2025-03-24 15:15:03,750 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3254\n",
            "2025-03-24 15:15:12,331 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3046\n",
            "2025-03-24 15:15:20,939 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3210\n",
            "2025-03-24 15:15:29,397 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3017\n",
            "2025-03-24 15:15:37,830 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3029\n",
            "2025-03-24 15:15:46,160 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3559\n",
            "2025-03-24 15:15:54,673 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3362\n",
            "2025-03-24 15:16:03,146 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3072\n",
            "2025-03-24 15:16:11,551 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3476\n",
            "2025-03-24 15:16:20,164 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3261\n",
            "2025-03-24 15:16:28,748 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3813\n",
            "2025-03-24 15:16:37,222 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3460\n",
            "2025-03-24 15:16:45,724 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4046\n",
            "2025-03-24 15:16:54,114 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3364\n",
            "2025-03-24 15:17:02,509 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3359\n",
            "2025-03-24 15:17:11,176 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3134\n",
            "2025-03-24 15:17:20,096 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3459\n",
            "2025-03-24 15:17:28,506 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3640\n",
            "2025-03-24 15:17:36,981 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3707\n",
            "2025-03-24 15:17:45,701 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3299\n",
            "2025-03-24 15:17:52,213 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-24 15:17:52,829 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1158\n",
            "2025-03-24 15:17:52,830 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:18:54,186 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3374, Val Loss: 0.3689, Val Acc: 0.8641\n",
            "2025-03-24 15:18:54,187 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 15:19:00,478 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2422\n",
            "2025-03-24 15:19:08,877 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3355\n",
            "2025-03-24 15:19:17,329 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-24 15:19:25,865 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3416\n",
            "2025-03-24 15:19:34,423 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-24 15:19:42,780 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3570\n",
            "2025-03-24 15:19:51,149 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3169\n",
            "2025-03-24 15:19:59,922 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3197\n",
            "2025-03-24 15:20:08,281 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3242\n",
            "2025-03-24 15:20:16,663 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3324\n",
            "2025-03-24 15:20:25,180 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3240\n",
            "2025-03-24 15:20:33,835 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3714\n",
            "2025-03-24 15:20:42,333 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3442\n",
            "2025-03-24 15:20:50,837 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3197\n",
            "2025-03-24 15:20:59,308 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3140\n",
            "2025-03-24 15:21:07,644 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3475\n",
            "2025-03-24 15:21:16,079 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3055\n",
            "2025-03-24 15:21:24,480 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3383\n",
            "2025-03-24 15:21:32,826 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3674\n",
            "2025-03-24 15:21:41,148 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3053\n",
            "2025-03-24 15:21:49,584 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4069\n",
            "2025-03-24 15:21:57,776 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3238\n",
            "2025-03-24 15:22:06,387 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-24 15:22:15,015 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2990\n",
            "2025-03-24 15:22:23,631 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3305\n",
            "2025-03-24 15:22:31,966 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3595\n",
            "2025-03-24 15:22:40,687 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3414\n",
            "2025-03-24 15:22:49,208 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3258\n",
            "2025-03-24 15:22:57,758 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3393\n",
            "2025-03-24 15:23:06,437 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3487\n",
            "2025-03-24 15:23:14,840 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3376\n",
            "2025-03-24 15:23:23,219 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3250\n",
            "2025-03-24 15:23:31,787 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3267\n",
            "2025-03-24 15:23:38,380 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2359\n",
            "2025-03-24 15:23:39,052 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1105\n",
            "2025-03-24 15:23:39,053 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:24:40,679 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3322, Val Loss: 0.3680, Val Acc: 0.8720\n",
            "2025-03-24 15:24:40,679 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 15:24:47,164 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2289\n",
            "2025-03-24 15:24:55,757 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2996\n",
            "2025-03-24 15:25:04,165 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2907\n",
            "2025-03-24 15:25:12,567 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3548\n",
            "2025-03-24 15:25:21,146 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-24 15:25:29,745 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2777\n",
            "2025-03-24 15:25:38,340 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3267\n",
            "2025-03-24 15:25:46,754 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3270\n",
            "2025-03-24 15:25:55,331 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3020\n",
            "2025-03-24 15:26:03,616 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2988\n",
            "2025-03-24 15:26:12,377 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2902\n",
            "2025-03-24 15:26:20,788 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3072\n",
            "2025-03-24 15:26:29,140 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3348\n",
            "2025-03-24 15:26:37,818 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3040\n",
            "2025-03-24 15:26:46,401 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2949\n",
            "2025-03-24 15:26:55,133 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3204\n",
            "2025-03-24 15:27:03,532 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3177\n",
            "2025-03-24 15:27:11,928 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3755\n",
            "2025-03-24 15:27:20,504 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-24 15:27:29,030 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3035\n",
            "2025-03-24 15:27:37,306 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3512\n",
            "2025-03-24 15:27:45,805 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3084\n",
            "2025-03-24 15:27:54,155 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2942\n",
            "2025-03-24 15:28:02,815 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3077\n",
            "2025-03-24 15:28:11,309 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3244\n",
            "2025-03-24 15:28:19,707 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2980\n",
            "2025-03-24 15:28:28,106 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3008\n",
            "2025-03-24 15:28:36,705 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3197\n",
            "2025-03-24 15:28:45,104 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3065\n",
            "2025-03-24 15:28:53,542 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3305\n",
            "2025-03-24 15:29:01,893 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3071\n",
            "2025-03-24 15:29:10,505 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2917\n",
            "2025-03-24 15:29:19,082 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3142\n",
            "2025-03-24 15:29:25,322 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2403\n",
            "2025-03-24 15:29:25,965 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0858\n",
            "2025-03-24 15:29:25,966 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:30:27,573 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.3128, Val Loss: 0.3960, Val Acc: 0.8739\n",
            "2025-03-24 15:30:27,573 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 15:30:34,070 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2413\n",
            "2025-03-24 15:30:42,459 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3200\n",
            "2025-03-24 15:30:50,856 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3018\n",
            "2025-03-24 15:30:59,439 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3088\n",
            "2025-03-24 15:31:07,894 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3058\n",
            "2025-03-24 15:31:16,463 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2923\n",
            "2025-03-24 15:31:25,155 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-24 15:31:33,800 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2879\n",
            "2025-03-24 15:31:42,433 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3375\n",
            "2025-03-24 15:31:50,844 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-24 15:31:59,241 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-24 15:32:07,639 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-24 15:32:16,208 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3005\n",
            "2025-03-24 15:32:24,665 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2889\n",
            "2025-03-24 15:32:33,137 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3112\n",
            "2025-03-24 15:32:41,459 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-24 15:32:49,877 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-24 15:32:58,469 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3015\n",
            "2025-03-24 15:33:06,873 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2998\n",
            "2025-03-24 15:33:15,417 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3088\n",
            "2025-03-24 15:33:24,200 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3039\n",
            "2025-03-24 15:33:32,705 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2933\n",
            "2025-03-24 15:33:41,210 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2953\n",
            "2025-03-24 15:33:49,737 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2773\n",
            "2025-03-24 15:33:58,071 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3058\n",
            "2025-03-24 15:34:06,691 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3024\n",
            "2025-03-24 15:34:15,233 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3160\n",
            "2025-03-24 15:34:23,683 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3017\n",
            "2025-03-24 15:34:32,286 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2685\n",
            "2025-03-24 15:34:40,623 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-24 15:34:49,016 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2697\n",
            "2025-03-24 15:34:57,569 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2808\n",
            "2025-03-24 15:35:06,168 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2989\n",
            "2025-03-24 15:35:12,419 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2271\n",
            "2025-03-24 15:35:13,096 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1028\n",
            "2025-03-24 15:35:13,097 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:36:14,647 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2970, Val Loss: 0.3674, Val Acc: 0.8664\n",
            "2025-03-24 15:36:14,647 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 15:36:21,009 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2246\n",
            "2025-03-24 15:36:29,615 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2764\n",
            "2025-03-24 15:36:38,208 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2785\n",
            "2025-03-24 15:36:46,627 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2924\n",
            "2025-03-24 15:36:55,251 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2799\n",
            "2025-03-24 15:37:03,535 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3011\n",
            "2025-03-24 15:37:11,944 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2750\n",
            "2025-03-24 15:37:20,652 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-24 15:37:29,138 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3035\n",
            "2025-03-24 15:37:37,722 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-24 15:37:46,136 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3060\n",
            "2025-03-24 15:37:54,534 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-24 15:38:02,980 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-24 15:38:11,366 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2851\n",
            "2025-03-24 15:38:19,851 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3199\n",
            "2025-03-24 15:38:28,511 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-24 15:38:37,107 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3051\n",
            "2025-03-24 15:38:45,517 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2633\n",
            "2025-03-24 15:38:53,917 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-24 15:39:02,299 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-24 15:39:10,692 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2937\n",
            "2025-03-24 15:39:19,326 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2834\n",
            "2025-03-24 15:39:27,757 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2906\n",
            "2025-03-24 15:39:36,314 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2620\n",
            "2025-03-24 15:39:44,929 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2792\n",
            "2025-03-24 15:39:53,533 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2850\n",
            "2025-03-24 15:40:01,902 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2973\n",
            "2025-03-24 15:40:10,284 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2599\n",
            "2025-03-24 15:40:18,767 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2838\n",
            "2025-03-24 15:40:27,277 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-24 15:40:35,868 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2777\n",
            "2025-03-24 15:40:44,278 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2668\n",
            "2025-03-24 15:40:52,623 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2644\n",
            "2025-03-24 15:40:58,945 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1984\n",
            "2025-03-24 15:40:59,540 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0755\n",
            "2025-03-24 15:40:59,541 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:42:00,963 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2800, Val Loss: 0.3750, Val Acc: 0.8637\n",
            "2025-03-24 15:42:00,963 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-24 15:42:07,360 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1947\n",
            "2025-03-24 15:42:15,737 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-24 15:42:24,256 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-24 15:42:32,834 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2437\n",
            "2025-03-24 15:42:41,244 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2769\n",
            "2025-03-24 15:42:49,564 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-24 15:42:58,006 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2630\n",
            "2025-03-24 15:43:06,421 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2814\n",
            "2025-03-24 15:43:15,038 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2541\n",
            "2025-03-24 15:43:23,618 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2576\n",
            "2025-03-24 15:43:32,334 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3185\n",
            "2025-03-24 15:43:40,637 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3094\n",
            "2025-03-24 15:43:49,094 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2763\n",
            "2025-03-24 15:43:57,629 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-24 15:44:06,047 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2657\n",
            "2025-03-24 15:44:14,551 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2526\n",
            "2025-03-24 15:44:23,200 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2965\n",
            "2025-03-24 15:44:31,615 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-24 15:44:40,058 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2825\n",
            "2025-03-24 15:44:48,449 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-24 15:44:56,888 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2706\n",
            "2025-03-24 15:45:05,603 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2757\n",
            "2025-03-24 15:45:14,001 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-24 15:45:22,585 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2630\n",
            "2025-03-24 15:45:31,384 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2506\n",
            "2025-03-24 15:45:39,751 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2627\n",
            "2025-03-24 15:45:48,232 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2627\n",
            "2025-03-24 15:45:56,829 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2499\n",
            "2025-03-24 15:46:05,246 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-24 15:46:13,803 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-24 15:46:22,364 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2841\n",
            "2025-03-24 15:46:31,211 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2808\n",
            "2025-03-24 15:46:39,774 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2968\n",
            "2025-03-24 15:46:45,987 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1942\n",
            "2025-03-24 15:46:46,617 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0984\n",
            "2025-03-24 15:46:46,618 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:47:47,820 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2731, Val Loss: 0.3715, Val Acc: 0.8692\n",
            "2025-03-24 15:47:47,822 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-24 15:47:54,330 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1910\n",
            "2025-03-24 15:48:02,928 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2744\n",
            "2025-03-24 15:48:11,738 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2539\n",
            "2025-03-24 15:48:20,142 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2471\n",
            "2025-03-24 15:48:28,532 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2624\n",
            "2025-03-24 15:48:37,178 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2552\n",
            "2025-03-24 15:48:45,557 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-24 15:48:54,190 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2777\n",
            "2025-03-24 15:49:02,585 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-24 15:49:11,170 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2776\n",
            "2025-03-24 15:49:19,511 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-24 15:49:27,767 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2605\n",
            "2025-03-24 15:49:36,570 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-24 15:49:45,032 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-24 15:49:53,216 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2541\n",
            "2025-03-24 15:50:01,706 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2661\n",
            "2025-03-24 15:50:10,184 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 15:50:18,705 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2792\n",
            "2025-03-24 15:50:27,293 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2705\n",
            "2025-03-24 15:50:35,930 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2690\n",
            "2025-03-24 15:50:44,537 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2637\n",
            "2025-03-24 15:50:52,933 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-24 15:51:01,331 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-24 15:51:09,891 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2507\n",
            "2025-03-24 15:51:18,270 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 15:51:26,776 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2698\n",
            "2025-03-24 15:51:35,368 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-24 15:51:44,079 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-24 15:51:52,476 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-24 15:52:00,950 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2581\n",
            "2025-03-24 15:52:09,330 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2492\n",
            "2025-03-24 15:52:17,944 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-24 15:52:26,471 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2832\n",
            "2025-03-24 15:52:32,685 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2051\n",
            "2025-03-24 15:52:33,315 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0924\n",
            "2025-03-24 15:52:33,315 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:53:34,952 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2675, Val Loss: 0.3717, Val Acc: 0.8739\n",
            "2025-03-24 15:53:34,953 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-24 15:53:41,325 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2180\n",
            "2025-03-24 15:53:49,839 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2456\n",
            "2025-03-24 15:53:58,428 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2444\n",
            "2025-03-24 15:54:06,843 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2513\n",
            "2025-03-24 15:54:15,294 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2377\n",
            "2025-03-24 15:54:23,647 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2504\n",
            "2025-03-24 15:54:32,126 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2402\n",
            "2025-03-24 15:54:40,593 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2863\n",
            "2025-03-24 15:54:49,026 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2413\n",
            "2025-03-24 15:54:57,624 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2663\n",
            "2025-03-24 15:55:06,207 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2520\n",
            "2025-03-24 15:55:14,616 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2511\n",
            "2025-03-24 15:55:23,010 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2874\n",
            "2025-03-24 15:55:31,415 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-24 15:55:39,813 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2715\n",
            "2025-03-24 15:55:48,201 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2677\n",
            "2025-03-24 15:55:56,632 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2568\n",
            "2025-03-24 15:56:05,274 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2672\n",
            "2025-03-24 15:56:13,688 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-24 15:56:22,091 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2489\n",
            "2025-03-24 15:56:30,488 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2550\n",
            "2025-03-24 15:56:38,995 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-24 15:56:47,584 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2552\n",
            "2025-03-24 15:56:56,021 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-24 15:57:04,435 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2589\n",
            "2025-03-24 15:57:12,884 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-24 15:57:21,463 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2512\n",
            "2025-03-24 15:57:29,979 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2815\n",
            "2025-03-24 15:57:38,371 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-24 15:57:46,959 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-24 15:57:55,372 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2594\n",
            "2025-03-24 15:58:03,955 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2458\n",
            "2025-03-24 15:58:12,365 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2671\n",
            "2025-03-24 15:58:18,580 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1809\n",
            "2025-03-24 15:58:19,195 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1234\n",
            "2025-03-24 15:58:19,196 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 15:59:20,648 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2612, Val Loss: 0.3687, Val Acc: 0.8753\n",
            "2025-03-24 15:59:20,649 - INFO - [TRAIN INFO] ============================== Epoch 21/50 ==============================\n",
            "2025-03-24 15:59:27,090 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1739\n",
            "2025-03-24 15:59:35,518 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2729\n",
            "2025-03-24 15:59:43,846 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2592\n",
            "2025-03-24 15:59:52,209 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2550\n",
            "2025-03-24 16:00:00,805 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2479\n",
            "2025-03-24 16:00:09,198 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-24 16:00:17,929 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2356\n",
            "2025-03-24 16:00:26,322 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2475\n",
            "2025-03-24 16:00:34,903 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2726\n",
            "2025-03-24 16:00:43,371 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-24 16:00:52,093 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2475\n",
            "2025-03-24 16:01:00,571 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-24 16:01:08,995 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-24 16:01:17,344 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2394\n",
            "2025-03-24 16:01:25,736 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2437\n",
            "2025-03-24 16:01:34,101 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2424\n",
            "2025-03-24 16:01:42,501 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2519\n",
            "2025-03-24 16:01:50,900 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2488\n",
            "2025-03-24 16:01:59,297 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2439\n",
            "2025-03-24 16:02:07,690 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2722\n",
            "2025-03-24 16:02:16,277 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2475\n",
            "2025-03-24 16:02:24,746 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2610\n",
            "2025-03-24 16:02:33,441 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-24 16:02:41,953 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2335\n",
            "2025-03-24 16:02:50,508 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-24 16:02:59,138 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-24 16:03:07,445 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2711\n",
            "2025-03-24 16:03:15,950 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2683\n",
            "2025-03-24 16:03:24,465 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2583\n",
            "2025-03-24 16:03:32,866 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2524\n",
            "2025-03-24 16:03:41,428 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2493\n",
            "2025-03-24 16:03:50,039 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2584\n",
            "2025-03-24 16:03:58,462 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2493\n",
            "2025-03-24 16:04:04,727 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1949\n",
            "2025-03-24 16:04:05,352 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0915\n",
            "2025-03-24 16:04:05,353 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:05:06,981 - INFO - [TRAIN INFO] Epoch 21/50, Train Loss: 0.2558, Val Loss: 0.3715, Val Acc: 0.8716\n",
            "2025-03-24 16:05:06,981 - INFO - [TRAIN INFO] Early stopping at epoch 21 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 16:05:06,982 - INFO - [TRAIN INFO] Total Time: 7293.64s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▁▂▃▃▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇███████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇███████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇███████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇███████▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▅▅▄▄▄▃▃▃▃▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▇▇▇█▇▇▇▇███▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>21</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.25584</td></tr><tr><td>train_val_loss_diff</td><td>-0.11563</td></tr><tr><td>val_accuracy</td><td>0.87157</td></tr><tr><td>val_loss</td><td>0.37148</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_enetb0_224_simple_concat_fold_3</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/qys4dkz8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/qys4dkz8</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_140332-qys4dkz8\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 16:05:09,438 - INFO - [TRAIN INFO] Fold 3 Training Complete at epoch 21. Total Time: 7296.09s\n",
            "2025-03-24 16:05:09,461 - INFO - [K-FOLD INFO] Fold 3 completed in 7297.65 seconds\n",
            "2025-03-24 16:05:09,462 - INFO - [K-FOLD INFO] ============================== Fold 4/5 ==============================\n",
            "2025-03-24 16:05:09,464 - INFO - [K-FOLD INFO] Fold 4:\n",
            "2025-03-24 16:05:09,465 - INFO -    Train Samples: 8595\n",
            "2025-03-24 16:05:09,466 - INFO -    Validation Samples: 2148\n",
            "2025-03-24 16:05:09,467 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 4\n",
            "2025-03-24 16:05:09,468 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 4:\n",
            "2025-03-24 16:05:09,469 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 16:05:10,014 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 4\n",
            "2025-03-24 16:05:10,016 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 4:\n",
            "2025-03-24 16:05:10,017 - INFO - [K-FOLD INFO] Loss function initialized for Fold 4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_enetb0_224_simple_concat\\wandb\\run-20250324_160510-ybjlgww8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ybjlgww8' target=\"_blank\">experiment_multimodal_enetb0_224_simple_concat_fold_4</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ybjlgww8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ybjlgww8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 16:05:10,689 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 16:05:10,689 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 16:05:17,150 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0688\n",
            "2025-03-24 16:05:25,977 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.3877\n",
            "2025-03-24 16:05:34,746 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3467\n",
            "2025-03-24 16:05:43,198 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3522\n",
            "2025-03-24 16:05:51,710 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3043\n",
            "2025-03-24 16:06:00,224 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2945\n",
            "2025-03-24 16:06:08,787 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2516\n",
            "2025-03-24 16:06:17,377 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2216\n",
            "2025-03-24 16:06:26,017 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2055\n",
            "2025-03-24 16:06:34,680 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2205\n",
            "2025-03-24 16:06:43,301 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1321\n",
            "2025-03-24 16:06:51,812 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1632\n",
            "2025-03-24 16:07:00,282 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1865\n",
            "2025-03-24 16:07:08,642 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1521\n",
            "2025-03-24 16:07:17,156 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.0665\n",
            "2025-03-24 16:07:25,777 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.0368\n",
            "2025-03-24 16:07:34,366 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.0747\n",
            "2025-03-24 16:07:42,977 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.1115\n",
            "2025-03-24 16:07:51,551 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0265\n",
            "2025-03-24 16:08:00,039 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0825\n",
            "2025-03-24 16:08:08,588 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0101\n",
            "2025-03-24 16:08:17,083 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.9699\n",
            "2025-03-24 16:08:25,783 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0490\n",
            "2025-03-24 16:08:34,430 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.9721\n",
            "2025-03-24 16:08:43,175 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9334\n",
            "2025-03-24 16:08:51,778 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9415\n",
            "2025-03-24 16:09:00,452 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9748\n",
            "2025-03-24 16:09:08,935 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9493\n",
            "2025-03-24 16:09:17,537 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9660\n",
            "2025-03-24 16:09:26,150 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9229\n",
            "2025-03-24 16:09:34,554 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9243\n",
            "2025-03-24 16:09:42,957 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9491\n",
            "2025-03-24 16:09:51,353 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7969\n",
            "2025-03-24 16:09:57,665 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6792\n",
            "2025-03-24 16:09:58,416 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2356\n",
            "2025-03-24 16:09:58,417 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:10:58,801 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.0951, Val Loss: 0.6863, Val Acc: 0.7486\n",
            "2025-03-24 16:10:59,097 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 16:10:59,098 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 16:11:05,637 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6405\n",
            "2025-03-24 16:11:14,129 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.8858\n",
            "2025-03-24 16:11:22,524 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8381\n",
            "2025-03-24 16:11:30,695 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8017\n",
            "2025-03-24 16:11:39,435 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.9101\n",
            "2025-03-24 16:11:47,880 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8317\n",
            "2025-03-24 16:11:56,627 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8491\n",
            "2025-03-24 16:12:05,289 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7363\n",
            "2025-03-24 16:12:14,082 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7508\n",
            "2025-03-24 16:12:22,903 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8294\n",
            "2025-03-24 16:12:31,597 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8227\n",
            "2025-03-24 16:12:40,081 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7900\n",
            "2025-03-24 16:12:48,576 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7120\n",
            "2025-03-24 16:12:57,186 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7712\n",
            "2025-03-24 16:13:05,879 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7489\n",
            "2025-03-24 16:13:14,288 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7888\n",
            "2025-03-24 16:13:22,689 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7938\n",
            "2025-03-24 16:13:31,089 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6790\n",
            "2025-03-24 16:13:39,478 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7797\n",
            "2025-03-24 16:13:48,020 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7222\n",
            "2025-03-24 16:13:56,563 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7533\n",
            "2025-03-24 16:14:05,078 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6627\n",
            "2025-03-24 16:14:13,793 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6704\n",
            "2025-03-24 16:14:22,677 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6794\n",
            "2025-03-24 16:14:31,468 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7246\n",
            "2025-03-24 16:14:40,136 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7264\n",
            "2025-03-24 16:14:48,866 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6429\n",
            "2025-03-24 16:14:57,480 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7441\n",
            "2025-03-24 16:15:06,016 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7960\n",
            "2025-03-24 16:15:14,653 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7227\n",
            "2025-03-24 16:15:23,214 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7072\n",
            "2025-03-24 16:15:31,961 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6528\n",
            "2025-03-24 16:15:40,653 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6374\n",
            "2025-03-24 16:15:47,245 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5929\n",
            "2025-03-24 16:15:47,889 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1705\n",
            "2025-03-24 16:15:47,889 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:16:48,816 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.7575, Val Loss: 0.5175, Val Acc: 0.8115\n",
            "2025-03-24 16:16:49,137 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 16:16:49,137 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 16:16:55,764 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4679\n",
            "2025-03-24 16:17:04,620 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6767\n",
            "2025-03-24 16:17:13,282 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5640\n",
            "2025-03-24 16:17:21,972 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6276\n",
            "2025-03-24 16:17:30,492 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6905\n",
            "2025-03-24 16:17:39,055 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7180\n",
            "2025-03-24 16:17:47,516 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6240\n",
            "2025-03-24 16:17:56,183 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7036\n",
            "2025-03-24 16:18:04,648 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6743\n",
            "2025-03-24 16:18:13,257 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6376\n",
            "2025-03-24 16:18:22,187 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6226\n",
            "2025-03-24 16:18:30,791 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6355\n",
            "2025-03-24 16:18:39,349 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6227\n",
            "2025-03-24 16:18:49,592 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6337\n",
            "2025-03-24 16:18:58,114 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6423\n",
            "2025-03-24 16:19:06,474 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7254\n",
            "2025-03-24 16:19:14,938 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6795\n",
            "2025-03-24 16:19:23,773 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6564\n",
            "2025-03-24 16:19:32,378 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6012\n",
            "2025-03-24 16:19:40,977 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6209\n",
            "2025-03-24 16:19:49,628 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6487\n",
            "2025-03-24 16:19:58,375 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7251\n",
            "2025-03-24 16:20:07,092 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6499\n",
            "2025-03-24 16:20:15,948 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6591\n",
            "2025-03-24 16:20:24,363 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6395\n",
            "2025-03-24 16:20:32,961 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6643\n",
            "2025-03-24 16:20:41,740 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6694\n",
            "2025-03-24 16:20:50,434 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6421\n",
            "2025-03-24 16:20:58,961 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6352\n",
            "2025-03-24 16:21:07,597 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6289\n",
            "2025-03-24 16:21:16,219 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6287\n",
            "2025-03-24 16:21:24,913 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6072\n",
            "2025-03-24 16:21:33,321 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6778\n",
            "2025-03-24 16:21:39,586 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4280\n",
            "2025-03-24 16:21:40,270 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1353\n",
            "2025-03-24 16:21:40,271 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:22:41,040 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6478, Val Loss: 0.4547, Val Acc: 0.8291\n",
            "2025-03-24 16:22:41,347 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 16:22:41,347 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 16:22:47,693 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4769\n",
            "2025-03-24 16:22:56,182 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6145\n",
            "2025-03-24 16:23:04,825 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5261\n",
            "2025-03-24 16:23:13,499 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6098\n",
            "2025-03-24 16:23:21,924 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5671\n",
            "2025-03-24 16:23:30,562 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6020\n",
            "2025-03-24 16:23:39,102 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6434\n",
            "2025-03-24 16:23:47,679 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5731\n",
            "2025-03-24 16:23:56,310 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5415\n",
            "2025-03-24 16:24:04,959 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5534\n",
            "2025-03-24 16:24:13,445 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5820\n",
            "2025-03-24 16:24:21,946 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5286\n",
            "2025-03-24 16:24:30,494 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5509\n",
            "2025-03-24 16:24:39,122 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5435\n",
            "2025-03-24 16:24:47,554 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5621\n",
            "2025-03-24 16:24:56,170 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6037\n",
            "2025-03-24 16:25:04,794 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6439\n",
            "2025-03-24 16:25:13,461 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5980\n",
            "2025-03-24 16:25:22,119 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5856\n",
            "2025-03-24 16:25:30,560 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6021\n",
            "2025-03-24 16:25:39,059 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6409\n",
            "2025-03-24 16:25:47,644 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6002\n",
            "2025-03-24 16:25:56,224 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5580\n",
            "2025-03-24 16:26:04,852 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5800\n",
            "2025-03-24 16:26:13,439 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5601\n",
            "2025-03-24 16:26:22,037 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5270\n",
            "2025-03-24 16:26:30,452 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5851\n",
            "2025-03-24 16:26:39,242 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5755\n",
            "2025-03-24 16:26:47,702 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5574\n",
            "2025-03-24 16:26:56,131 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6107\n",
            "2025-03-24 16:27:04,601 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5575\n",
            "2025-03-24 16:27:13,422 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6325\n",
            "2025-03-24 16:27:22,019 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5683\n",
            "2025-03-24 16:27:28,437 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3688\n",
            "2025-03-24 16:27:29,128 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1594\n",
            "2025-03-24 16:27:29,129 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:28:29,782 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5804, Val Loss: 0.4190, Val Acc: 0.8454\n",
            "2025-03-24 16:28:30,092 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 16:28:30,093 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 16:28:36,609 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3582\n",
            "2025-03-24 16:28:45,186 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5568\n",
            "2025-03-24 16:28:53,768 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5055\n",
            "2025-03-24 16:29:02,408 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5632\n",
            "2025-03-24 16:29:11,087 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6013\n",
            "2025-03-24 16:29:19,797 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4935\n",
            "2025-03-24 16:29:28,358 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6168\n",
            "2025-03-24 16:29:36,741 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5748\n",
            "2025-03-24 16:29:45,422 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5668\n",
            "2025-03-24 16:29:53,966 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5400\n",
            "2025-03-24 16:30:02,587 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4937\n",
            "2025-03-24 16:30:11,011 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5329\n",
            "2025-03-24 16:30:19,664 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5606\n",
            "2025-03-24 16:30:28,179 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5090\n",
            "2025-03-24 16:30:36,774 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4860\n",
            "2025-03-24 16:30:45,260 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5967\n",
            "2025-03-24 16:30:53,978 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5083\n",
            "2025-03-24 16:31:02,396 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4992\n",
            "2025-03-24 16:31:10,954 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4930\n",
            "2025-03-24 16:31:19,415 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4708\n",
            "2025-03-24 16:31:27,995 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5710\n",
            "2025-03-24 16:31:36,479 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5778\n",
            "2025-03-24 16:31:45,109 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5276\n",
            "2025-03-24 16:31:53,731 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4969\n",
            "2025-03-24 16:32:02,340 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5462\n",
            "2025-03-24 16:32:10,746 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5501\n",
            "2025-03-24 16:32:19,385 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5503\n",
            "2025-03-24 16:32:27,980 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5089\n",
            "2025-03-24 16:32:36,542 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5285\n",
            "2025-03-24 16:32:45,121 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5395\n",
            "2025-03-24 16:32:53,595 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5227\n",
            "2025-03-24 16:33:02,518 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5123\n",
            "2025-03-24 16:33:11,113 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4763\n",
            "2025-03-24 16:33:17,527 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3741\n",
            "2025-03-24 16:33:18,178 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2025\n",
            "2025-03-24 16:33:18,178 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:34:19,065 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5337, Val Loss: 0.3962, Val Acc: 0.8557\n",
            "2025-03-24 16:34:19,368 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 16:34:19,369 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 16:34:25,969 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3954\n",
            "2025-03-24 16:34:34,705 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4338\n",
            "2025-03-24 16:34:43,313 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4643\n",
            "2025-03-24 16:34:51,938 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4966\n",
            "2025-03-24 16:35:00,740 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5063\n",
            "2025-03-24 16:35:09,678 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4512\n",
            "2025-03-24 16:35:18,289 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4910\n",
            "2025-03-24 16:35:27,084 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4544\n",
            "2025-03-24 16:35:36,084 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5076\n",
            "2025-03-24 16:35:44,704 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4760\n",
            "2025-03-24 16:35:53,386 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5197\n",
            "2025-03-24 16:36:02,107 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4607\n",
            "2025-03-24 16:36:10,677 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5277\n",
            "2025-03-24 16:36:19,326 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4889\n",
            "2025-03-24 16:36:28,259 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4397\n",
            "2025-03-24 16:36:36,851 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4629\n",
            "2025-03-24 16:36:45,343 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4874\n",
            "2025-03-24 16:36:54,046 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5574\n",
            "2025-03-24 16:37:02,657 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4436\n",
            "2025-03-24 16:37:11,093 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4617\n",
            "2025-03-24 16:37:19,852 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4764\n",
            "2025-03-24 16:37:28,253 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4815\n",
            "2025-03-24 16:37:36,840 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4820\n",
            "2025-03-24 16:37:45,510 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4959\n",
            "2025-03-24 16:37:54,130 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4713\n",
            "2025-03-24 16:38:02,803 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5083\n",
            "2025-03-24 16:38:11,301 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5429\n",
            "2025-03-24 16:38:19,925 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5369\n",
            "2025-03-24 16:38:28,413 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4688\n",
            "2025-03-24 16:38:36,905 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4556\n",
            "2025-03-24 16:38:45,616 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4707\n",
            "2025-03-24 16:38:54,319 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5026\n",
            "2025-03-24 16:39:03,009 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4277\n",
            "2025-03-24 16:39:09,419 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3440\n",
            "2025-03-24 16:39:10,066 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1482\n",
            "2025-03-24 16:39:10,066 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:40:12,354 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4841, Val Loss: 0.3929, Val Acc: 0.8566\n",
            "2025-03-24 16:40:12,657 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 16:40:12,657 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 16:40:19,002 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3615\n",
            "2025-03-24 16:40:27,581 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5135\n",
            "2025-03-24 16:40:36,198 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4199\n",
            "2025-03-24 16:40:44,697 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4589\n",
            "2025-03-24 16:40:53,378 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4701\n",
            "2025-03-24 16:41:01,976 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4506\n",
            "2025-03-24 16:41:10,585 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4263\n",
            "2025-03-24 16:41:18,965 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4497\n",
            "2025-03-24 16:41:27,392 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4535\n",
            "2025-03-24 16:41:36,179 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4311\n",
            "2025-03-24 16:41:44,698 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5015\n",
            "2025-03-24 16:41:53,269 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4229\n",
            "2025-03-24 16:42:01,746 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4451\n",
            "2025-03-24 16:42:10,392 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4895\n",
            "2025-03-24 16:42:18,958 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5023\n",
            "2025-03-24 16:42:27,553 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4235\n",
            "2025-03-24 16:42:36,171 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4245\n",
            "2025-03-24 16:42:44,631 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4174\n",
            "2025-03-24 16:42:53,347 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4684\n",
            "2025-03-24 16:43:01,867 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4550\n",
            "2025-03-24 16:43:10,408 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4638\n",
            "2025-03-24 16:43:18,938 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4918\n",
            "2025-03-24 16:43:27,511 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4409\n",
            "2025-03-24 16:43:36,123 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5295\n",
            "2025-03-24 16:43:44,563 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4548\n",
            "2025-03-24 16:43:53,132 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4661\n",
            "2025-03-24 16:44:01,697 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4413\n",
            "2025-03-24 16:44:10,199 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4722\n",
            "2025-03-24 16:44:18,757 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4514\n",
            "2025-03-24 16:44:27,159 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4807\n",
            "2025-03-24 16:44:35,718 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3798\n",
            "2025-03-24 16:44:44,358 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5120\n",
            "2025-03-24 16:44:52,797 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5346\n",
            "2025-03-24 16:44:59,270 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3669\n",
            "2025-03-24 16:44:59,941 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1267\n",
            "2025-03-24 16:44:59,942 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:46:00,757 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4622, Val Loss: 0.3964, Val Acc: 0.8538\n",
            "2025-03-24 16:46:00,757 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 16:46:07,317 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3578\n",
            "2025-03-24 16:46:16,087 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4274\n",
            "2025-03-24 16:46:24,595 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4121\n",
            "2025-03-24 16:46:32,617 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4529\n",
            "2025-03-24 16:46:40,441 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4241\n",
            "2025-03-24 16:46:48,275 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3976\n",
            "2025-03-24 16:46:55,992 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4862\n",
            "2025-03-24 16:47:03,609 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4315\n",
            "2025-03-24 16:47:11,271 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4523\n",
            "2025-03-24 16:47:19,055 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4798\n",
            "2025-03-24 16:47:26,864 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4687\n",
            "2025-03-24 16:47:34,470 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4627\n",
            "2025-03-24 16:47:42,172 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4183\n",
            "2025-03-24 16:47:50,161 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4833\n",
            "2025-03-24 16:47:58,256 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3904\n",
            "2025-03-24 16:48:06,056 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4415\n",
            "2025-03-24 16:48:13,846 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4332\n",
            "2025-03-24 16:48:21,604 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4625\n",
            "2025-03-24 16:48:29,071 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4302\n",
            "2025-03-24 16:48:37,027 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4461\n",
            "2025-03-24 16:48:44,837 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4326\n",
            "2025-03-24 16:48:52,806 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4676\n",
            "2025-03-24 16:49:00,579 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4639\n",
            "2025-03-24 16:49:08,602 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4495\n",
            "2025-03-24 16:49:17,489 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4754\n",
            "2025-03-24 16:49:29,426 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4751\n",
            "2025-03-24 16:49:39,617 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4939\n",
            "2025-03-24 16:49:48,371 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5178\n",
            "2025-03-24 16:49:56,540 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4187\n",
            "2025-03-24 16:50:04,578 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4042\n",
            "2025-03-24 16:50:12,821 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4362\n",
            "2025-03-24 16:50:21,218 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4151\n",
            "2025-03-24 16:50:29,115 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3962\n",
            "2025-03-24 16:50:35,006 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3559\n",
            "2025-03-24 16:50:35,629 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0822\n",
            "2025-03-24 16:50:35,630 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:51:33,270 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4457, Val Loss: 0.3762, Val Acc: 0.8645\n",
            "2025-03-24 16:51:33,584 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 16:51:33,585 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 16:51:39,751 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2793\n",
            "2025-03-24 16:51:47,990 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4290\n",
            "2025-03-24 16:51:55,947 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3455\n",
            "2025-03-24 16:52:03,777 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4330\n",
            "2025-03-24 16:52:12,167 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3816\n",
            "2025-03-24 16:52:20,134 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4126\n",
            "2025-03-24 16:52:27,925 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4165\n",
            "2025-03-24 16:52:36,112 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4862\n",
            "2025-03-24 16:52:43,923 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4174\n",
            "2025-03-24 16:52:51,926 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4616\n",
            "2025-03-24 16:52:59,908 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3543\n",
            "2025-03-24 16:53:07,697 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4063\n",
            "2025-03-24 16:53:15,496 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4054\n",
            "2025-03-24 16:53:23,244 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3858\n",
            "2025-03-24 16:53:31,146 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4094\n",
            "2025-03-24 16:53:39,074 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3646\n",
            "2025-03-24 16:53:47,026 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4311\n",
            "2025-03-24 16:53:54,757 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3990\n",
            "2025-03-24 16:54:02,508 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3977\n",
            "2025-03-24 16:54:10,466 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3596\n",
            "2025-03-24 16:54:18,745 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4064\n",
            "2025-03-24 16:54:27,450 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4160\n",
            "2025-03-24 16:54:36,333 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4257\n",
            "2025-03-24 16:54:44,932 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3715\n",
            "2025-03-24 16:54:53,156 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3964\n",
            "2025-03-24 16:55:01,511 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4649\n",
            "2025-03-24 16:55:09,892 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3729\n",
            "2025-03-24 16:55:18,400 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4006\n",
            "2025-03-24 16:55:26,692 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3714\n",
            "2025-03-24 16:55:35,068 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3951\n",
            "2025-03-24 16:55:43,282 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4298\n",
            "2025-03-24 16:55:51,711 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4122\n",
            "2025-03-24 16:56:00,101 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4376\n",
            "2025-03-24 16:56:06,322 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2790\n",
            "2025-03-24 16:56:06,947 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2059\n",
            "2025-03-24 16:56:06,948 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 16:57:07,165 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4077, Val Loss: 0.3933, Val Acc: 0.8650\n",
            "2025-03-24 16:57:07,166 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 16:57:13,357 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2963\n",
            "2025-03-24 16:57:21,732 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3813\n",
            "2025-03-24 16:57:30,079 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3659\n",
            "2025-03-24 16:57:38,482 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3821\n",
            "2025-03-24 16:57:46,876 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3583\n",
            "2025-03-24 16:57:55,271 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3521\n",
            "2025-03-24 16:58:03,262 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3775\n",
            "2025-03-24 16:58:11,263 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3524\n",
            "2025-03-24 16:58:19,224 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3700\n",
            "2025-03-24 16:58:27,187 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3705\n",
            "2025-03-24 16:58:35,024 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4104\n",
            "2025-03-24 16:58:42,939 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3658\n",
            "2025-03-24 16:58:50,815 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3885\n",
            "2025-03-24 16:58:58,694 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4207\n",
            "2025-03-24 16:59:06,701 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3712\n",
            "2025-03-24 16:59:14,665 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3942\n",
            "2025-03-24 16:59:23,082 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4329\n",
            "2025-03-24 16:59:31,402 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3525\n",
            "2025-03-24 16:59:39,889 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3508\n",
            "2025-03-24 16:59:49,039 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3969\n",
            "2025-03-24 16:59:57,808 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4014\n",
            "2025-03-24 17:00:06,556 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3746\n",
            "2025-03-24 17:00:15,289 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4141\n",
            "2025-03-24 17:00:23,867 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3899\n",
            "2025-03-24 17:00:32,830 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3606\n",
            "2025-03-24 17:00:41,389 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3488\n",
            "2025-03-24 17:00:50,288 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3477\n",
            "2025-03-24 17:00:59,019 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3943\n",
            "2025-03-24 17:01:07,624 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3923\n",
            "2025-03-24 17:01:16,221 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3724\n",
            "2025-03-24 17:01:24,611 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3751\n",
            "2025-03-24 17:01:33,011 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3560\n",
            "2025-03-24 17:01:41,705 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4008\n",
            "2025-03-24 17:01:48,447 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3013\n",
            "2025-03-24 17:01:49,174 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1206\n",
            "2025-03-24 17:01:49,175 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:02:48,977 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3805, Val Loss: 0.3997, Val Acc: 0.8580\n",
            "2025-03-24 17:02:48,977 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 17:02:54,948 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2819\n",
            "2025-03-24 17:03:03,335 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3715\n",
            "2025-03-24 17:03:11,404 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3144\n",
            "2025-03-24 17:03:19,602 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3568\n",
            "2025-03-24 17:03:27,734 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3610\n",
            "2025-03-24 17:03:35,777 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3582\n",
            "2025-03-24 17:03:43,725 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3696\n",
            "2025-03-24 17:03:51,970 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3681\n",
            "2025-03-24 17:03:59,937 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3955\n",
            "2025-03-24 17:04:08,161 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3454\n",
            "2025-03-24 17:04:16,170 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3499\n",
            "2025-03-24 17:04:24,581 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3511\n",
            "2025-03-24 17:04:32,958 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3659\n",
            "2025-03-24 17:04:41,348 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3696\n",
            "2025-03-24 17:04:49,482 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3364\n",
            "2025-03-24 17:04:57,524 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4027\n",
            "2025-03-24 17:05:05,548 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3511\n",
            "2025-03-24 17:05:13,682 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3581\n",
            "2025-03-24 17:05:21,712 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4224\n",
            "2025-03-24 17:05:29,709 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3239\n",
            "2025-03-24 17:05:37,867 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3510\n",
            "2025-03-24 17:05:46,189 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3571\n",
            "2025-03-24 17:05:54,528 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3707\n",
            "2025-03-24 17:06:02,932 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3201\n",
            "2025-03-24 17:06:11,323 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4024\n",
            "2025-03-24 17:06:19,300 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3817\n",
            "2025-03-24 17:06:27,564 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3320\n",
            "2025-03-24 17:06:35,710 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3719\n",
            "2025-03-24 17:06:44,148 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3759\n",
            "2025-03-24 17:06:52,576 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3331\n",
            "2025-03-24 17:07:00,963 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3742\n",
            "2025-03-24 17:07:09,308 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3928\n",
            "2025-03-24 17:07:17,466 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3439\n",
            "2025-03-24 17:07:23,893 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3119\n",
            "2025-03-24 17:07:24,493 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1669\n",
            "2025-03-24 17:07:24,495 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:08:24,116 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3656, Val Loss: 0.4179, Val Acc: 0.8627\n",
            "2025-03-24 17:08:24,117 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 17:08:30,487 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2262\n",
            "2025-03-24 17:08:38,880 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-24 17:08:47,079 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3560\n",
            "2025-03-24 17:08:55,066 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3630\n",
            "2025-03-24 17:09:03,874 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3511\n",
            "2025-03-24 17:09:12,636 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3211\n",
            "2025-03-24 17:09:21,431 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3279\n",
            "2025-03-24 17:09:29,868 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3014\n",
            "2025-03-24 17:09:38,070 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3085\n",
            "2025-03-24 17:09:46,287 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3454\n",
            "2025-03-24 17:09:54,589 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3347\n",
            "2025-03-24 17:10:02,478 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3487\n",
            "2025-03-24 17:10:10,610 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3154\n",
            "2025-03-24 17:10:18,727 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3347\n",
            "2025-03-24 17:10:26,885 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3213\n",
            "2025-03-24 17:10:34,968 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3636\n",
            "2025-03-24 17:10:43,006 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3286\n",
            "2025-03-24 17:10:51,007 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3490\n",
            "2025-03-24 17:10:59,181 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3354\n",
            "2025-03-24 17:11:07,195 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3865\n",
            "2025-03-24 17:11:15,228 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3257\n",
            "2025-03-24 17:11:23,464 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3927\n",
            "2025-03-24 17:11:31,425 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3482\n",
            "2025-03-24 17:11:39,619 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3601\n",
            "2025-03-24 17:11:47,697 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3667\n",
            "2025-03-24 17:11:55,782 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3549\n",
            "2025-03-24 17:12:03,981 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3257\n",
            "2025-03-24 17:12:12,810 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3674\n",
            "2025-03-24 17:12:21,540 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3700\n",
            "2025-03-24 17:12:30,214 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3350\n",
            "2025-03-24 17:12:38,888 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3393\n",
            "2025-03-24 17:12:47,761 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4068\n",
            "2025-03-24 17:12:56,711 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3837\n",
            "2025-03-24 17:13:03,153 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2964\n",
            "2025-03-24 17:13:03,829 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1050\n",
            "2025-03-24 17:13:03,829 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:14:05,627 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3464, Val Loss: 0.3999, Val Acc: 0.8659\n",
            "2025-03-24 17:14:05,628 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 17:14:11,946 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2366\n",
            "2025-03-24 17:14:20,173 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3327\n",
            "2025-03-24 17:14:28,827 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3252\n",
            "2025-03-24 17:14:37,332 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3499\n",
            "2025-03-24 17:14:45,967 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3314\n",
            "2025-03-24 17:14:54,371 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3203\n",
            "2025-03-24 17:15:02,961 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3131\n",
            "2025-03-24 17:15:11,361 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3123\n",
            "2025-03-24 17:15:19,701 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2932\n",
            "2025-03-24 17:15:28,224 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3287\n",
            "2025-03-24 17:15:36,658 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3271\n",
            "2025-03-24 17:15:45,157 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2942\n",
            "2025-03-24 17:15:53,758 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3031\n",
            "2025-03-24 17:16:02,336 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3444\n",
            "2025-03-24 17:16:10,776 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3060\n",
            "2025-03-24 17:16:19,355 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3074\n",
            "2025-03-24 17:16:28,132 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2883\n",
            "2025-03-24 17:16:36,548 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3527\n",
            "2025-03-24 17:16:45,093 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3541\n",
            "2025-03-24 17:16:53,483 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3486\n",
            "2025-03-24 17:17:01,956 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3325\n",
            "2025-03-24 17:17:10,527 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2957\n",
            "2025-03-24 17:17:19,145 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3236\n",
            "2025-03-24 17:17:27,730 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2997\n",
            "2025-03-24 17:17:36,323 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3276\n",
            "2025-03-24 17:17:44,909 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-24 17:17:53,160 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3260\n",
            "2025-03-24 17:18:01,751 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-24 17:18:10,188 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2929\n",
            "2025-03-24 17:18:18,904 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2827\n",
            "2025-03-24 17:18:27,517 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3038\n",
            "2025-03-24 17:18:36,085 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3478\n",
            "2025-03-24 17:18:44,703 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3114\n",
            "2025-03-24 17:18:51,176 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2246\n",
            "2025-03-24 17:18:51,835 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0833\n",
            "2025-03-24 17:18:51,836 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:19:52,321 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3165, Val Loss: 0.4012, Val Acc: 0.8734\n",
            "2025-03-24 17:19:52,321 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 17:19:58,713 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2396\n",
            "2025-03-24 17:20:07,319 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2982\n",
            "2025-03-24 17:20:15,716 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2841\n",
            "2025-03-24 17:20:24,174 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3293\n",
            "2025-03-24 17:20:32,561 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-24 17:20:41,110 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2957\n",
            "2025-03-24 17:20:49,764 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2970\n",
            "2025-03-24 17:20:58,449 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2632\n",
            "2025-03-24 17:21:06,865 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3090\n",
            "2025-03-24 17:21:15,445 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2992\n",
            "2025-03-24 17:21:23,709 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2954\n",
            "2025-03-24 17:21:32,264 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2758\n",
            "2025-03-24 17:21:40,766 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-24 17:21:49,442 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-24 17:21:57,843 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3255\n",
            "2025-03-24 17:22:06,432 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3066\n",
            "2025-03-24 17:22:14,950 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2814\n",
            "2025-03-24 17:22:23,434 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2804\n",
            "2025-03-24 17:22:31,860 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3304\n",
            "2025-03-24 17:22:40,267 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3291\n",
            "2025-03-24 17:22:48,886 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-24 17:22:57,422 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3386\n",
            "2025-03-24 17:23:06,022 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2876\n",
            "2025-03-24 17:23:14,420 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3186\n",
            "2025-03-24 17:23:22,901 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3219\n",
            "2025-03-24 17:23:31,476 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2877\n",
            "2025-03-24 17:23:40,027 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-24 17:23:48,807 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2960\n",
            "2025-03-24 17:23:57,252 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-24 17:24:05,934 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-24 17:24:14,428 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3124\n",
            "2025-03-24 17:24:23,045 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2754\n",
            "2025-03-24 17:24:31,387 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3159\n",
            "2025-03-24 17:24:37,983 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2262\n",
            "2025-03-24 17:24:38,635 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0985\n",
            "2025-03-24 17:24:38,636 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:25:38,909 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3000, Val Loss: 0.3949, Val Acc: 0.8785\n",
            "2025-03-24 17:25:38,911 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 17:25:45,331 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2027\n",
            "2025-03-24 17:25:53,787 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2677\n",
            "2025-03-24 17:26:02,369 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2783\n",
            "2025-03-24 17:26:10,970 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-24 17:26:19,545 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-24 17:26:27,964 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3102\n",
            "2025-03-24 17:26:36,233 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-24 17:26:44,829 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-24 17:26:53,548 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3079\n",
            "2025-03-24 17:27:01,959 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-24 17:27:10,527 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2655\n",
            "2025-03-24 17:27:19,105 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3111\n",
            "2025-03-24 17:27:27,727 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-24 17:27:36,243 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3324\n",
            "2025-03-24 17:27:44,781 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2719\n",
            "2025-03-24 17:27:53,310 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3108\n",
            "2025-03-24 17:28:01,933 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2783\n",
            "2025-03-24 17:28:10,555 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2765\n",
            "2025-03-24 17:28:18,932 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3124\n",
            "2025-03-24 17:28:27,565 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2833\n",
            "2025-03-24 17:28:36,143 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2812\n",
            "2025-03-24 17:28:44,795 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2803\n",
            "2025-03-24 17:28:53,281 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2754\n",
            "2025-03-24 17:29:01,908 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-24 17:29:10,489 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2969\n",
            "2025-03-24 17:29:19,081 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2814\n",
            "2025-03-24 17:29:27,480 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-24 17:29:36,302 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3029\n",
            "2025-03-24 17:29:44,693 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2673\n",
            "2025-03-24 17:29:53,200 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2912\n",
            "2025-03-24 17:30:01,583 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2473\n",
            "2025-03-24 17:30:10,005 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2912\n",
            "2025-03-24 17:30:18,685 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2866\n",
            "2025-03-24 17:30:25,200 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1998\n",
            "2025-03-24 17:30:25,833 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0770\n",
            "2025-03-24 17:30:25,834 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:31:26,430 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2855, Val Loss: 0.4036, Val Acc: 0.8724\n",
            "2025-03-24 17:31:26,430 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 17:31:32,856 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2143\n",
            "2025-03-24 17:31:41,264 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2642\n",
            "2025-03-24 17:31:49,770 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2644\n",
            "2025-03-24 17:31:58,258 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2730\n",
            "2025-03-24 17:32:06,658 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2574\n",
            "2025-03-24 17:32:15,213 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2572\n",
            "2025-03-24 17:32:23,502 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2561\n",
            "2025-03-24 17:32:31,929 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3251\n",
            "2025-03-24 17:32:40,482 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2839\n",
            "2025-03-24 17:32:49,120 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2598\n",
            "2025-03-24 17:32:57,643 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-24 17:33:06,227 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-24 17:33:14,862 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2792\n",
            "2025-03-24 17:33:23,161 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2965\n",
            "2025-03-24 17:33:31,497 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2801\n",
            "2025-03-24 17:33:40,027 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-24 17:33:48,621 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-24 17:33:57,120 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2612\n",
            "2025-03-24 17:34:05,609 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2562\n",
            "2025-03-24 17:34:14,032 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3062\n",
            "2025-03-24 17:34:22,615 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-24 17:34:31,409 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2831\n",
            "2025-03-24 17:34:39,814 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2711\n",
            "2025-03-24 17:34:48,303 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2770\n",
            "2025-03-24 17:34:57,005 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2814\n",
            "2025-03-24 17:35:05,573 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2851\n",
            "2025-03-24 17:35:13,989 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2974\n",
            "2025-03-24 17:35:22,560 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3086\n",
            "2025-03-24 17:35:31,182 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-24 17:35:39,767 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2892\n",
            "2025-03-24 17:35:48,367 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3054\n",
            "2025-03-24 17:35:57,183 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2552\n",
            "2025-03-24 17:36:05,785 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2955\n",
            "2025-03-24 17:36:12,175 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2160\n",
            "2025-03-24 17:36:12,822 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1499\n",
            "2025-03-24 17:36:12,823 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:37:14,274 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2829, Val Loss: 0.4174, Val Acc: 0.8673\n",
            "2025-03-24 17:37:14,274 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 17:37:20,719 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2075\n",
            "2025-03-24 17:37:29,352 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2770\n",
            "2025-03-24 17:37:38,057 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2652\n",
            "2025-03-24 17:37:46,348 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-24 17:37:54,830 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-24 17:38:03,540 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2840\n",
            "2025-03-24 17:38:11,945 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-24 17:38:20,513 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-24 17:38:28,776 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2862\n",
            "2025-03-24 17:38:37,401 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-24 17:38:46,000 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2687\n",
            "2025-03-24 17:38:54,539 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2972\n",
            "2025-03-24 17:39:03,093 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2631\n",
            "2025-03-24 17:39:11,775 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2778\n",
            "2025-03-24 17:39:20,161 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2770\n",
            "2025-03-24 17:39:28,585 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-24 17:39:36,917 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2531\n",
            "2025-03-24 17:39:45,406 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2835\n",
            "2025-03-24 17:39:53,918 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2541\n",
            "2025-03-24 17:40:02,507 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2715\n",
            "2025-03-24 17:40:10,916 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3222\n",
            "2025-03-24 17:40:19,306 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2772\n",
            "2025-03-24 17:40:27,707 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2955\n",
            "2025-03-24 17:40:36,150 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2862\n",
            "2025-03-24 17:40:44,509 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2862\n",
            "2025-03-24 17:40:53,073 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2680\n",
            "2025-03-24 17:41:01,697 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3251\n",
            "2025-03-24 17:41:10,249 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2557\n",
            "2025-03-24 17:41:18,691 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-24 17:41:27,057 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2870\n",
            "2025-03-24 17:41:35,562 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2915\n",
            "2025-03-24 17:41:43,940 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-24 17:41:52,675 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-24 17:41:58,880 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2035\n",
            "2025-03-24 17:41:59,510 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0940\n",
            "2025-03-24 17:41:59,511 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:43:00,000 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2813, Val Loss: 0.4100, Val Acc: 0.8673\n",
            "2025-03-24 17:43:00,001 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-24 17:43:06,450 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2245\n",
            "2025-03-24 17:43:14,853 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2598\n",
            "2025-03-24 17:43:23,250 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2902\n",
            "2025-03-24 17:43:31,725 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2947\n",
            "2025-03-24 17:43:40,298 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2666\n",
            "2025-03-24 17:43:49,033 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2922\n",
            "2025-03-24 17:43:57,440 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2784\n",
            "2025-03-24 17:44:05,842 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-24 17:44:14,232 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2860\n",
            "2025-03-24 17:44:22,821 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2889\n",
            "2025-03-24 17:44:31,301 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-24 17:44:40,029 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2764\n",
            "2025-03-24 17:44:48,612 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2691\n",
            "2025-03-24 17:44:56,937 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2446\n",
            "2025-03-24 17:45:05,390 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2561\n",
            "2025-03-24 17:45:13,776 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2609\n",
            "2025-03-24 17:45:22,037 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2838\n",
            "2025-03-24 17:45:31,138 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2691\n",
            "2025-03-24 17:45:39,569 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2745\n",
            "2025-03-24 17:45:47,968 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2915\n",
            "2025-03-24 17:45:56,485 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-24 17:46:05,016 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-24 17:46:13,539 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2765\n",
            "2025-03-24 17:46:21,995 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2620\n",
            "2025-03-24 17:46:30,544 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2511\n",
            "2025-03-24 17:46:39,041 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2722\n",
            "2025-03-24 17:46:47,698 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-24 17:46:56,039 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2825\n",
            "2025-03-24 17:47:04,586 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-24 17:47:13,161 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-24 17:47:21,617 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2677\n",
            "2025-03-24 17:47:30,817 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-24 17:47:39,375 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2726\n",
            "2025-03-24 17:47:45,821 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2150\n",
            "2025-03-24 17:47:46,469 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0685\n",
            "2025-03-24 17:47:46,469 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:48:47,000 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2743, Val Loss: 0.4060, Val Acc: 0.8669\n",
            "2025-03-24 17:48:47,001 - INFO - [TRAIN INFO] Early stopping at epoch 18 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 17:48:47,002 - INFO - [TRAIN INFO] Total Time: 6216.31s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▂▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇████▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▅▅▅▄▄▄▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇▇▇▇▇▇███▇▇▇</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▁▂▂▂▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.27431</td></tr><tr><td>train_val_loss_diff</td><td>-0.13166</td></tr><tr><td>val_accuracy</td><td>0.86685</td></tr><tr><td>val_loss</td><td>0.40597</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_enetb0_224_simple_concat_fold_4</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ybjlgww8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ybjlgww8</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_160510-ybjlgww8\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 17:48:49,310 - INFO - [TRAIN INFO] Fold 4 Training Complete at epoch 18. Total Time: 6218.62s\n",
            "2025-03-24 17:48:49,334 - INFO - [K-FOLD INFO] Fold 4 completed in 6219.87 seconds\n",
            "2025-03-24 17:48:49,336 - INFO - [K-FOLD INFO] ============================== Fold 5/5 ==============================\n",
            "2025-03-24 17:48:49,344 - INFO - [K-FOLD INFO] Fold 5:\n",
            "2025-03-24 17:48:49,345 - INFO -    Train Samples: 8595\n",
            "2025-03-24 17:48:49,345 - INFO -    Validation Samples: 2148\n",
            "2025-03-24 17:48:49,346 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 5\n",
            "2025-03-24 17:48:49,348 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 5:\n",
            "2025-03-24 17:48:49,349 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 17:48:50,160 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 5\n",
            "2025-03-24 17:48:50,163 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 5:\n",
            "2025-03-24 17:48:50,163 - INFO - [K-FOLD INFO] Loss function initialized for Fold 5\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_enetb0_224_simple_concat\\wandb\\run-20250324_174850-l1vbik9x</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/l1vbik9x' target=\"_blank\">experiment_multimodal_enetb0_224_simple_concat_fold_5</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/l1vbik9x' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/l1vbik9x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 17:48:52,707 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 17:48:52,708 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 17:48:59,346 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0814\n",
            "2025-03-24 17:49:07,796 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.3920\n",
            "2025-03-24 17:49:16,345 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3912\n",
            "2025-03-24 17:49:24,941 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3036\n",
            "2025-03-24 17:49:33,342 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3421\n",
            "2025-03-24 17:49:41,932 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3000\n",
            "2025-03-24 17:49:50,383 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2492\n",
            "2025-03-24 17:49:59,131 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2465\n",
            "2025-03-24 17:50:07,528 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2022\n",
            "2025-03-24 17:50:16,101 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2024\n",
            "2025-03-24 17:50:24,538 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1810\n",
            "2025-03-24 17:50:33,074 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1625\n",
            "2025-03-24 17:50:41,474 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1042\n",
            "2025-03-24 17:50:49,966 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.0823\n",
            "2025-03-24 17:50:58,329 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.0840\n",
            "2025-03-24 17:51:06,799 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.0132\n",
            "2025-03-24 17:51:15,307 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.0523\n",
            "2025-03-24 17:51:23,797 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0240\n",
            "2025-03-24 17:51:32,197 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.9968\n",
            "2025-03-24 17:51:40,745 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0137\n",
            "2025-03-24 17:51:49,297 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0616\n",
            "2025-03-24 17:51:57,898 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.9600\n",
            "2025-03-24 17:52:06,480 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.9776\n",
            "2025-03-24 17:52:14,891 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.9634\n",
            "2025-03-24 17:52:23,351 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9465\n",
            "2025-03-24 17:52:31,754 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9673\n",
            "2025-03-24 17:52:40,483 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9143\n",
            "2025-03-24 17:52:48,873 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9481\n",
            "2025-03-24 17:52:57,535 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9461\n",
            "2025-03-24 17:53:06,045 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9735\n",
            "2025-03-24 17:53:14,470 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8960\n",
            "2025-03-24 17:53:22,871 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9290\n",
            "2025-03-24 17:53:31,269 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9537\n",
            "2025-03-24 17:53:37,792 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6619\n",
            "2025-03-24 17:53:38,475 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2100\n",
            "2025-03-24 17:53:38,475 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 17:54:39,129 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.0884, Val Loss: 0.6657, Val Acc: 0.7644\n",
            "2025-03-24 17:54:39,419 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 17:54:39,419 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 17:54:45,843 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6739\n",
            "2025-03-24 17:54:54,439 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.8512\n",
            "2025-03-24 17:55:02,838 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8439\n",
            "2025-03-24 17:55:11,388 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8474\n",
            "2025-03-24 17:55:19,850 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7833\n",
            "2025-03-24 17:55:28,425 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8479\n",
            "2025-03-24 17:55:37,102 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7837\n",
            "2025-03-24 17:55:45,593 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8087\n",
            "2025-03-24 17:55:54,229 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7665\n",
            "2025-03-24 17:56:02,811 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7444\n",
            "2025-03-24 17:56:11,221 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7628\n",
            "2025-03-24 17:56:19,723 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7762\n",
            "2025-03-24 17:56:28,174 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8923\n",
            "2025-03-24 17:56:36,594 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7969\n",
            "2025-03-24 17:56:45,006 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7783\n",
            "2025-03-24 17:56:53,459 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7331\n",
            "2025-03-24 17:57:01,848 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7193\n",
            "2025-03-24 17:57:10,296 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7282\n",
            "2025-03-24 17:57:18,797 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7760\n",
            "2025-03-24 17:57:27,198 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7851\n",
            "2025-03-24 17:57:35,705 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7055\n",
            "2025-03-24 17:57:44,265 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7266\n",
            "2025-03-24 17:57:52,769 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7996\n",
            "2025-03-24 17:58:01,385 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6341\n",
            "2025-03-24 17:58:09,960 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6947\n",
            "2025-03-24 17:58:18,557 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7492\n",
            "2025-03-24 17:58:27,181 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7419\n",
            "2025-03-24 17:58:35,771 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6750\n",
            "2025-03-24 17:58:44,273 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6535\n",
            "2025-03-24 17:58:52,973 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7575\n",
            "2025-03-24 17:59:01,487 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6985\n",
            "2025-03-24 17:59:09,930 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6857\n",
            "2025-03-24 17:59:18,386 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7533\n",
            "2025-03-24 17:59:24,763 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5166\n",
            "2025-03-24 17:59:25,447 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2077\n",
            "2025-03-24 17:59:25,448 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:00:26,451 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.7614, Val Loss: 0.4978, Val Acc: 0.8166\n",
            "2025-03-24 18:00:26,766 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 18:00:26,766 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 18:00:33,143 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4975\n",
            "2025-03-24 18:00:41,729 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6802\n",
            "2025-03-24 18:00:50,250 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6286\n",
            "2025-03-24 18:00:58,927 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6663\n",
            "2025-03-24 18:01:07,573 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7186\n",
            "2025-03-24 18:01:16,138 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6581\n",
            "2025-03-24 18:01:24,554 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6589\n",
            "2025-03-24 18:01:33,278 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6887\n",
            "2025-03-24 18:01:41,755 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6084\n",
            "2025-03-24 18:01:50,279 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6296\n",
            "2025-03-24 18:01:58,901 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6683\n",
            "2025-03-24 18:02:07,485 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6997\n",
            "2025-03-24 18:02:16,224 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6769\n",
            "2025-03-24 18:02:24,710 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6869\n",
            "2025-03-24 18:02:33,277 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6197\n",
            "2025-03-24 18:02:41,888 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6451\n",
            "2025-03-24 18:02:50,341 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5683\n",
            "2025-03-24 18:02:58,663 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6039\n",
            "2025-03-24 18:03:07,134 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6065\n",
            "2025-03-24 18:03:15,769 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7104\n",
            "2025-03-24 18:03:24,495 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6798\n",
            "2025-03-24 18:03:33,080 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6210\n",
            "2025-03-24 18:03:41,564 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6682\n",
            "2025-03-24 18:03:49,997 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6259\n",
            "2025-03-24 18:03:58,454 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5606\n",
            "2025-03-24 18:04:06,846 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6574\n",
            "2025-03-24 18:04:15,182 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6282\n",
            "2025-03-24 18:04:23,610 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6262\n",
            "2025-03-24 18:04:32,376 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5638\n",
            "2025-03-24 18:04:40,941 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6940\n",
            "2025-03-24 18:04:49,318 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6113\n",
            "2025-03-24 18:04:57,678 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6465\n",
            "2025-03-24 18:05:06,124 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6551\n",
            "2025-03-24 18:05:12,558 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5116\n",
            "2025-03-24 18:05:13,223 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1331\n",
            "2025-03-24 18:05:13,224 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:06:13,970 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6460, Val Loss: 0.4352, Val Acc: 0.8361\n",
            "2025-03-24 18:06:14,289 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 18:06:14,290 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 18:06:20,596 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4263\n",
            "2025-03-24 18:06:29,008 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6281\n",
            "2025-03-24 18:06:37,603 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5285\n",
            "2025-03-24 18:06:46,216 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5560\n",
            "2025-03-24 18:06:54,935 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6368\n",
            "2025-03-24 18:07:03,420 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5536\n",
            "2025-03-24 18:07:11,814 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6104\n",
            "2025-03-24 18:07:20,217 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5661\n",
            "2025-03-24 18:07:28,699 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5942\n",
            "2025-03-24 18:07:37,047 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6382\n",
            "2025-03-24 18:07:45,796 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5703\n",
            "2025-03-24 18:07:54,284 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5878\n",
            "2025-03-24 18:08:02,752 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5556\n",
            "2025-03-24 18:08:11,388 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5495\n",
            "2025-03-24 18:08:19,798 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5296\n",
            "2025-03-24 18:08:28,424 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5530\n",
            "2025-03-24 18:08:37,188 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5646\n",
            "2025-03-24 18:08:45,666 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5758\n",
            "2025-03-24 18:08:54,234 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5481\n",
            "2025-03-24 18:09:02,895 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5708\n",
            "2025-03-24 18:09:11,386 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5813\n",
            "2025-03-24 18:09:19,784 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6101\n",
            "2025-03-24 18:09:28,178 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5756\n",
            "2025-03-24 18:09:36,755 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5526\n",
            "2025-03-24 18:09:45,215 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6679\n",
            "2025-03-24 18:09:53,776 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5818\n",
            "2025-03-24 18:10:02,371 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5994\n",
            "2025-03-24 18:10:10,958 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5133\n",
            "2025-03-24 18:10:19,355 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5684\n",
            "2025-03-24 18:10:27,626 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6199\n",
            "2025-03-24 18:10:36,402 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5737\n",
            "2025-03-24 18:10:44,956 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6140\n",
            "2025-03-24 18:10:53,352 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5850\n",
            "2025-03-24 18:10:59,769 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3931\n",
            "2025-03-24 18:11:00,432 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1219\n",
            "2025-03-24 18:11:00,433 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:12:01,328 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5778, Val Loss: 0.3993, Val Acc: 0.8520\n",
            "2025-03-24 18:12:01,642 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 18:12:01,643 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 18:12:08,105 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4568\n",
            "2025-03-24 18:12:16,722 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5305\n",
            "2025-03-24 18:12:25,123 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4962\n",
            "2025-03-24 18:12:33,517 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5474\n",
            "2025-03-24 18:12:42,104 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5297\n",
            "2025-03-24 18:12:50,526 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5791\n",
            "2025-03-24 18:12:59,012 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4925\n",
            "2025-03-24 18:13:07,595 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5646\n",
            "2025-03-24 18:13:16,107 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5485\n",
            "2025-03-24 18:13:24,583 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4738\n",
            "2025-03-24 18:13:33,009 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5242\n",
            "2025-03-24 18:13:41,540 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5766\n",
            "2025-03-24 18:13:50,099 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5753\n",
            "2025-03-24 18:13:58,447 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4824\n",
            "2025-03-24 18:14:07,093 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5520\n",
            "2025-03-24 18:14:15,682 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5226\n",
            "2025-03-24 18:14:24,090 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5878\n",
            "2025-03-24 18:14:32,297 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5069\n",
            "2025-03-24 18:14:40,975 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5226\n",
            "2025-03-24 18:14:49,683 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4810\n",
            "2025-03-24 18:14:58,250 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5239\n",
            "2025-03-24 18:15:06,774 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5180\n",
            "2025-03-24 18:15:15,097 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5447\n",
            "2025-03-24 18:15:23,529 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4828\n",
            "2025-03-24 18:15:31,988 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5286\n",
            "2025-03-24 18:15:40,652 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5145\n",
            "2025-03-24 18:15:49,250 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5552\n",
            "2025-03-24 18:15:57,726 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5207\n",
            "2025-03-24 18:16:06,460 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5483\n",
            "2025-03-24 18:16:15,032 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4923\n",
            "2025-03-24 18:16:23,498 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6037\n",
            "2025-03-24 18:16:32,341 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5233\n",
            "2025-03-24 18:16:40,831 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5689\n",
            "2025-03-24 18:16:47,095 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4004\n",
            "2025-03-24 18:16:47,759 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2145\n",
            "2025-03-24 18:16:47,760 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:17:48,764 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5360, Val Loss: 0.3727, Val Acc: 0.8627\n",
            "2025-03-24 18:17:49,085 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 18:17:49,085 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 18:17:55,817 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3640\n",
            "2025-03-24 18:18:04,293 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5340\n",
            "2025-03-24 18:18:12,878 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4775\n",
            "2025-03-24 18:18:21,606 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5162\n",
            "2025-03-24 18:18:30,014 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4829\n",
            "2025-03-24 18:18:38,411 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5835\n",
            "2025-03-24 18:18:47,006 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5164\n",
            "2025-03-24 18:18:55,432 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4908\n",
            "2025-03-24 18:19:04,059 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4672\n",
            "2025-03-24 18:19:12,665 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5300\n",
            "2025-03-24 18:19:21,156 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5071\n",
            "2025-03-24 18:19:29,799 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4415\n",
            "2025-03-24 18:19:38,415 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5175\n",
            "2025-03-24 18:19:47,021 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5167\n",
            "2025-03-24 18:19:55,435 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4965\n",
            "2025-03-24 18:20:04,175 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5278\n",
            "2025-03-24 18:20:12,579 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4153\n",
            "2025-03-24 18:20:20,980 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5330\n",
            "2025-03-24 18:20:29,372 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4817\n",
            "2025-03-24 18:20:37,690 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4981\n",
            "2025-03-24 18:20:46,039 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4723\n",
            "2025-03-24 18:20:54,603 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5228\n",
            "2025-03-24 18:21:03,069 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4654\n",
            "2025-03-24 18:21:11,615 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4490\n",
            "2025-03-24 18:21:20,157 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4765\n",
            "2025-03-24 18:21:28,756 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5434\n",
            "2025-03-24 18:21:37,434 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5011\n",
            "2025-03-24 18:21:45,948 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5205\n",
            "2025-03-24 18:21:54,531 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4857\n",
            "2025-03-24 18:22:02,924 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5181\n",
            "2025-03-24 18:22:11,317 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5008\n",
            "2025-03-24 18:22:19,678 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5598\n",
            "2025-03-24 18:22:28,174 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4414\n",
            "2025-03-24 18:22:34,730 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3880\n",
            "2025-03-24 18:22:35,378 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1071\n",
            "2025-03-24 18:22:35,379 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:23:37,108 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4993, Val Loss: 0.3634, Val Acc: 0.8682\n",
            "2025-03-24 18:23:37,417 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 18:23:37,417 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 18:23:43,874 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3123\n",
            "2025-03-24 18:23:52,371 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4698\n",
            "2025-03-24 18:24:00,917 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4568\n",
            "2025-03-24 18:24:09,473 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4377\n",
            "2025-03-24 18:24:17,870 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3991\n",
            "2025-03-24 18:24:26,517 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4614\n",
            "2025-03-24 18:24:34,925 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4454\n",
            "2025-03-24 18:24:43,342 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4910\n",
            "2025-03-24 18:24:51,896 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4317\n",
            "2025-03-24 18:25:00,409 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4725\n",
            "2025-03-24 18:25:09,198 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4930\n",
            "2025-03-24 18:25:17,829 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3970\n",
            "2025-03-24 18:25:26,545 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4496\n",
            "2025-03-24 18:25:35,010 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4475\n",
            "2025-03-24 18:25:43,666 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5102\n",
            "2025-03-24 18:25:52,271 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4332\n",
            "2025-03-24 18:26:01,016 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4752\n",
            "2025-03-24 18:26:09,681 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5211\n",
            "2025-03-24 18:26:18,454 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4261\n",
            "2025-03-24 18:26:27,052 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4778\n",
            "2025-03-24 18:26:35,504 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5025\n",
            "2025-03-24 18:26:43,816 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5132\n",
            "2025-03-24 18:26:52,655 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4316\n",
            "2025-03-24 18:27:01,117 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5144\n",
            "2025-03-24 18:27:09,582 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5212\n",
            "2025-03-24 18:27:18,254 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4763\n",
            "2025-03-24 18:27:26,649 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4445\n",
            "2025-03-24 18:27:35,045 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4741\n",
            "2025-03-24 18:27:43,442 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4977\n",
            "2025-03-24 18:27:52,034 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4349\n",
            "2025-03-24 18:28:00,464 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4885\n",
            "2025-03-24 18:28:09,103 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5318\n",
            "2025-03-24 18:28:17,437 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4509\n",
            "2025-03-24 18:28:23,916 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4090\n",
            "2025-03-24 18:28:24,568 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1185\n",
            "2025-03-24 18:28:24,568 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:29:25,441 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4687, Val Loss: 0.3589, Val Acc: 0.8706\n",
            "2025-03-24 18:29:25,748 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 18:29:25,749 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 18:29:32,168 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3107\n",
            "2025-03-24 18:29:40,618 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4222\n",
            "2025-03-24 18:29:49,327 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3952\n",
            "2025-03-24 18:29:57,991 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4303\n",
            "2025-03-24 18:30:06,405 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4221\n",
            "2025-03-24 18:30:15,138 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4443\n",
            "2025-03-24 18:30:23,782 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4839\n",
            "2025-03-24 18:30:32,294 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4611\n",
            "2025-03-24 18:30:41,092 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4338\n",
            "2025-03-24 18:30:49,949 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3873\n",
            "2025-03-24 18:30:58,780 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4333\n",
            "2025-03-24 18:31:07,550 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4450\n",
            "2025-03-24 18:31:16,311 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4769\n",
            "2025-03-24 18:31:25,301 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4896\n",
            "2025-03-24 18:31:33,903 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3855\n",
            "2025-03-24 18:31:42,527 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4712\n",
            "2025-03-24 18:31:51,367 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4390\n",
            "2025-03-24 18:32:00,151 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4509\n",
            "2025-03-24 18:32:08,944 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4303\n",
            "2025-03-24 18:32:17,760 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4904\n",
            "2025-03-24 18:32:26,221 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4753\n",
            "2025-03-24 18:32:34,628 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4286\n",
            "2025-03-24 18:32:43,355 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3583\n",
            "2025-03-24 18:32:51,749 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4183\n",
            "2025-03-24 18:33:00,282 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4238\n",
            "2025-03-24 18:33:08,743 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4275\n",
            "2025-03-24 18:33:17,350 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4570\n",
            "2025-03-24 18:33:25,786 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4920\n",
            "2025-03-24 18:33:34,145 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4286\n",
            "2025-03-24 18:33:42,598 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4101\n",
            "2025-03-24 18:33:50,935 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4866\n",
            "2025-03-24 18:33:59,547 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4080\n",
            "2025-03-24 18:34:08,030 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4298\n",
            "2025-03-24 18:34:14,325 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3907\n",
            "2025-03-24 18:34:15,009 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1356\n",
            "2025-03-24 18:34:15,009 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:35:16,951 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4407, Val Loss: 0.3631, Val Acc: 0.8664\n",
            "2025-03-24 18:35:16,952 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 18:35:23,482 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2698\n",
            "2025-03-24 18:35:32,129 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4428\n",
            "2025-03-24 18:35:40,896 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4117\n",
            "2025-03-24 18:35:49,556 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3427\n",
            "2025-03-24 18:35:58,289 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4280\n",
            "2025-03-24 18:36:06,938 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3770\n",
            "2025-03-24 18:36:15,727 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4203\n",
            "2025-03-24 18:36:24,133 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4217\n",
            "2025-03-24 18:36:32,517 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3911\n",
            "2025-03-24 18:36:41,206 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4105\n",
            "2025-03-24 18:36:49,952 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3469\n",
            "2025-03-24 18:36:58,778 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4269\n",
            "2025-03-24 18:37:07,404 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4119\n",
            "2025-03-24 18:37:16,241 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4064\n",
            "2025-03-24 18:37:24,982 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4083\n",
            "2025-03-24 18:37:33,859 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3689\n",
            "2025-03-24 18:37:42,590 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4779\n",
            "2025-03-24 18:37:51,261 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4162\n",
            "2025-03-24 18:37:59,707 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4094\n",
            "2025-03-24 18:38:08,126 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4097\n",
            "2025-03-24 18:38:16,656 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4080\n",
            "2025-03-24 18:38:25,250 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4464\n",
            "2025-03-24 18:38:33,902 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3866\n",
            "2025-03-24 18:38:42,631 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3975\n",
            "2025-03-24 18:38:51,235 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4480\n",
            "2025-03-24 18:38:59,748 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4149\n",
            "2025-03-24 18:39:08,240 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4143\n",
            "2025-03-24 18:39:17,025 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4428\n",
            "2025-03-24 18:39:25,473 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4648\n",
            "2025-03-24 18:39:34,035 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4323\n",
            "2025-03-24 18:39:42,814 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3886\n",
            "2025-03-24 18:39:51,623 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3851\n",
            "2025-03-24 18:40:00,367 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4231\n",
            "2025-03-24 18:40:06,924 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3293\n",
            "2025-03-24 18:40:07,574 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1011\n",
            "2025-03-24 18:40:07,574 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:41:09,454 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4113, Val Loss: 0.3835, Val Acc: 0.8673\n",
            "2025-03-24 18:41:09,455 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 18:41:15,800 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2839\n",
            "2025-03-24 18:41:24,374 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3908\n",
            "2025-03-24 18:41:33,198 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4542\n",
            "2025-03-24 18:41:42,014 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3807\n",
            "2025-03-24 18:41:50,695 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3927\n",
            "2025-03-24 18:41:59,787 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3464\n",
            "2025-03-24 18:42:08,736 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3722\n",
            "2025-03-24 18:42:17,457 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3734\n",
            "2025-03-24 18:42:26,065 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3528\n",
            "2025-03-24 18:42:34,770 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3925\n",
            "2025-03-24 18:42:43,599 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3510\n",
            "2025-03-24 18:42:52,089 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3457\n",
            "2025-03-24 18:43:00,772 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3174\n",
            "2025-03-24 18:43:09,363 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3693\n",
            "2025-03-24 18:43:18,161 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3933\n",
            "2025-03-24 18:43:26,969 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3950\n",
            "2025-03-24 18:43:36,154 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4063\n",
            "2025-03-24 18:43:44,957 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4166\n",
            "2025-03-24 18:43:53,839 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3874\n",
            "2025-03-24 18:44:02,745 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3639\n",
            "2025-03-24 18:44:11,435 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4343\n",
            "2025-03-24 18:44:20,331 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3723\n",
            "2025-03-24 18:44:29,091 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4143\n",
            "2025-03-24 18:44:37,941 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4285\n",
            "2025-03-24 18:44:46,595 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4078\n",
            "2025-03-24 18:44:55,291 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3864\n",
            "2025-03-24 18:45:04,126 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4328\n",
            "2025-03-24 18:45:12,880 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4214\n",
            "2025-03-24 18:45:21,920 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4187\n",
            "2025-03-24 18:45:30,922 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4228\n",
            "2025-03-24 18:45:40,310 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3781\n",
            "2025-03-24 18:45:49,520 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3720\n",
            "2025-03-24 18:45:58,697 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4143\n",
            "2025-03-24 18:46:05,540 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3258\n",
            "2025-03-24 18:46:06,218 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1071\n",
            "2025-03-24 18:46:06,219 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:47:09,768 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3918, Val Loss: 0.3533, Val Acc: 0.8794\n",
            "2025-03-24 18:47:10,060 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 18:47:10,060 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 18:47:16,676 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2757\n",
            "2025-03-24 18:47:25,462 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3710\n",
            "2025-03-24 18:47:34,274 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3306\n",
            "2025-03-24 18:47:42,768 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3423\n",
            "2025-03-24 18:47:51,883 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3146\n",
            "2025-03-24 18:48:00,744 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3455\n",
            "2025-03-24 18:48:09,875 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4069\n",
            "2025-03-24 18:48:18,931 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3364\n",
            "2025-03-24 18:48:28,107 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3562\n",
            "2025-03-24 18:48:37,036 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3822\n",
            "2025-03-24 18:48:46,272 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3525\n",
            "2025-03-24 18:48:55,256 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3419\n",
            "2025-03-24 18:49:04,217 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3849\n",
            "2025-03-24 18:49:13,254 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4000\n",
            "2025-03-24 18:49:22,050 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4033\n",
            "2025-03-24 18:49:30,803 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3554\n",
            "2025-03-24 18:49:39,649 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3608\n",
            "2025-03-24 18:49:48,573 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3764\n",
            "2025-03-24 18:49:57,634 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3861\n",
            "2025-03-24 18:50:06,396 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4102\n",
            "2025-03-24 18:50:15,457 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3985\n",
            "2025-03-24 18:50:24,567 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3555\n",
            "2025-03-24 18:50:33,518 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3338\n",
            "2025-03-24 18:50:42,320 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3722\n",
            "2025-03-24 18:50:51,424 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4029\n",
            "2025-03-24 18:51:00,469 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3748\n",
            "2025-03-24 18:51:09,590 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3824\n",
            "2025-03-24 18:51:18,609 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3834\n",
            "2025-03-24 18:51:27,603 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3708\n",
            "2025-03-24 18:51:36,540 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3506\n",
            "2025-03-24 18:51:45,614 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4147\n",
            "2025-03-24 18:51:54,607 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3640\n",
            "2025-03-24 18:52:03,448 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3365\n",
            "2025-03-24 18:52:10,153 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3013\n",
            "2025-03-24 18:52:10,849 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0706\n",
            "2025-03-24 18:52:10,850 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:53:15,599 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3687, Val Loss: 0.3804, Val Acc: 0.8776\n",
            "2025-03-24 18:53:15,599 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 18:53:22,169 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2465\n",
            "2025-03-24 18:53:30,704 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3534\n",
            "2025-03-24 18:53:39,173 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3467\n",
            "2025-03-24 18:53:47,652 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3883\n",
            "2025-03-24 18:53:56,571 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3217\n",
            "2025-03-24 18:54:05,361 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3677\n",
            "2025-03-24 18:54:14,226 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3317\n",
            "2025-03-24 18:54:22,907 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3369\n",
            "2025-03-24 18:54:31,330 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3695\n",
            "2025-03-24 18:54:39,769 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3580\n",
            "2025-03-24 18:54:48,195 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3481\n",
            "2025-03-24 18:54:57,016 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3532\n",
            "2025-03-24 18:55:05,741 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3389\n",
            "2025-03-24 18:55:14,332 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3263\n",
            "2025-03-24 18:55:22,867 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3258\n",
            "2025-03-24 18:55:30,947 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3380\n",
            "2025-03-24 18:55:39,470 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3780\n",
            "2025-03-24 18:55:47,927 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3442\n",
            "2025-03-24 18:55:56,304 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3455\n",
            "2025-03-24 18:56:04,841 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3989\n",
            "2025-03-24 18:56:13,079 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3199\n",
            "2025-03-24 18:56:21,696 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3495\n",
            "2025-03-24 18:56:30,010 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2972\n",
            "2025-03-24 18:56:38,093 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3773\n",
            "2025-03-24 18:56:46,487 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3409\n",
            "2025-03-24 18:56:55,115 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3145\n",
            "2025-03-24 18:57:03,730 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3207\n",
            "2025-03-24 18:57:12,511 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3602\n",
            "2025-03-24 18:57:21,309 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3923\n",
            "2025-03-24 18:57:29,705 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3608\n",
            "2025-03-24 18:57:38,294 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3962\n",
            "2025-03-24 18:57:46,535 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3491\n",
            "2025-03-24 18:57:54,908 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3753\n",
            "2025-03-24 18:58:01,493 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2569\n",
            "2025-03-24 18:58:02,143 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1063\n",
            "2025-03-24 18:58:02,144 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 18:59:01,798 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3507, Val Loss: 0.3550, Val Acc: 0.8748\n",
            "2025-03-24 18:59:01,799 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 18:59:08,482 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-24 18:59:17,569 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3402\n",
            "2025-03-24 18:59:26,483 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3394\n",
            "2025-03-24 18:59:35,432 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2961\n",
            "2025-03-24 18:59:44,058 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3215\n",
            "2025-03-24 18:59:52,507 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3298\n",
            "2025-03-24 19:00:00,937 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3164\n",
            "2025-03-24 19:00:09,254 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3374\n",
            "2025-03-24 19:00:17,655 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3086\n",
            "2025-03-24 19:00:26,017 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3190\n",
            "2025-03-24 19:00:34,504 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3257\n",
            "2025-03-24 19:00:42,844 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3872\n",
            "2025-03-24 19:00:52,244 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3699\n",
            "2025-03-24 19:01:01,206 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3448\n",
            "2025-03-24 19:01:09,861 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3697\n",
            "2025-03-24 19:01:18,535 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3262\n",
            "2025-03-24 19:01:27,435 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3610\n",
            "2025-03-24 19:01:35,618 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3925\n",
            "2025-03-24 19:01:44,229 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3378\n",
            "2025-03-24 19:01:52,617 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3414\n",
            "2025-03-24 19:02:01,022 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3599\n",
            "2025-03-24 19:02:09,555 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3085\n",
            "2025-03-24 19:02:18,028 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3511\n",
            "2025-03-24 19:02:26,418 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3686\n",
            "2025-03-24 19:02:34,835 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3570\n",
            "2025-03-24 19:02:43,820 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3515\n",
            "2025-03-24 19:02:52,874 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3144\n",
            "2025-03-24 19:03:02,014 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3619\n",
            "2025-03-24 19:03:10,627 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3351\n",
            "2025-03-24 19:03:19,227 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3941\n",
            "2025-03-24 19:03:27,867 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3310\n",
            "2025-03-24 19:03:36,398 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3410\n",
            "2025-03-24 19:03:45,441 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3564\n",
            "2025-03-24 19:03:51,843 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-24 19:03:52,491 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1498\n",
            "2025-03-24 19:03:52,492 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 19:04:54,266 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3455, Val Loss: 0.3674, Val Acc: 0.8780\n",
            "2025-03-24 19:04:54,266 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 19:05:00,734 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2565\n",
            "2025-03-24 19:05:09,364 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3185\n",
            "2025-03-24 19:05:17,793 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3718\n",
            "2025-03-24 19:05:26,158 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3754\n",
            "2025-03-24 19:05:34,633 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3011\n",
            "2025-03-24 19:05:42,907 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3311\n",
            "2025-03-24 19:05:51,407 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3061\n",
            "2025-03-24 19:05:59,950 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3071\n",
            "2025-03-24 19:06:08,319 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-24 19:06:16,749 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3166\n",
            "2025-03-24 19:06:25,341 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-24 19:06:33,731 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3148\n",
            "2025-03-24 19:06:42,140 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3037\n",
            "2025-03-24 19:06:50,686 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3370\n",
            "2025-03-24 19:06:59,072 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3610\n",
            "2025-03-24 19:07:07,290 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-24 19:07:15,737 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3341\n",
            "2025-03-24 19:07:24,530 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3226\n",
            "2025-03-24 19:07:33,444 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3381\n",
            "2025-03-24 19:07:42,171 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3323\n",
            "2025-03-24 19:07:50,975 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3076\n",
            "2025-03-24 19:07:59,306 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-24 19:08:07,714 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3284\n",
            "2025-03-24 19:08:16,011 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3428\n",
            "2025-03-24 19:08:24,525 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3394\n",
            "2025-03-24 19:08:33,082 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3555\n",
            "2025-03-24 19:08:41,501 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3537\n",
            "2025-03-24 19:08:49,887 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-24 19:08:58,352 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3126\n",
            "2025-03-24 19:09:06,713 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3724\n",
            "2025-03-24 19:09:15,482 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3156\n",
            "2025-03-24 19:09:23,828 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3150\n",
            "2025-03-24 19:09:32,286 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3197\n",
            "2025-03-24 19:09:38,489 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2172\n",
            "2025-03-24 19:09:39,121 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1147\n",
            "2025-03-24 19:09:39,123 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 19:10:40,383 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3283, Val Loss: 0.3610, Val Acc: 0.8762\n",
            "2025-03-24 19:10:40,384 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 19:10:46,863 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2265\n",
            "2025-03-24 19:10:55,259 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3117\n",
            "2025-03-24 19:11:03,498 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2930\n",
            "2025-03-24 19:11:12,243 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3136\n",
            "2025-03-24 19:11:20,693 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2885\n",
            "2025-03-24 19:11:29,076 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2895\n",
            "2025-03-24 19:11:37,807 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3132\n",
            "2025-03-24 19:11:46,640 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3342\n",
            "2025-03-24 19:11:55,347 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2995\n",
            "2025-03-24 19:12:03,792 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3223\n",
            "2025-03-24 19:12:12,266 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3119\n",
            "2025-03-24 19:12:20,811 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2887\n",
            "2025-03-24 19:12:29,190 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3064\n",
            "2025-03-24 19:12:37,560 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2992\n",
            "2025-03-24 19:12:45,989 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2748\n",
            "2025-03-24 19:12:54,372 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3562\n",
            "2025-03-24 19:13:02,742 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2827\n",
            "2025-03-24 19:13:11,214 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3184\n",
            "2025-03-24 19:13:19,559 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2984\n",
            "2025-03-24 19:13:27,973 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2785\n",
            "2025-03-24 19:13:36,359 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3045\n",
            "2025-03-24 19:13:44,761 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-24 19:13:53,410 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3125\n",
            "2025-03-24 19:14:01,841 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2741\n",
            "2025-03-24 19:14:10,220 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2883\n",
            "2025-03-24 19:14:18,478 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3239\n",
            "2025-03-24 19:14:26,793 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-24 19:14:35,191 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2853\n",
            "2025-03-24 19:14:43,348 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-24 19:14:51,612 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3063\n",
            "2025-03-24 19:14:59,989 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3012\n",
            "2025-03-24 19:15:08,358 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2851\n",
            "2025-03-24 19:15:16,787 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3008\n",
            "2025-03-24 19:15:23,105 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2129\n",
            "2025-03-24 19:15:23,753 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0975\n",
            "2025-03-24 19:15:23,754 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 19:16:24,589 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.3001, Val Loss: 0.3643, Val Acc: 0.8822\n",
            "2025-03-24 19:16:24,589 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 19:16:30,758 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2517\n",
            "2025-03-24 19:16:39,047 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2904\n",
            "2025-03-24 19:16:47,446 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2680\n",
            "2025-03-24 19:16:55,834 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3072\n",
            "2025-03-24 19:17:03,896 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2851\n",
            "2025-03-24 19:17:12,484 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2804\n",
            "2025-03-24 19:17:20,946 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-24 19:17:29,094 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2524\n",
            "2025-03-24 19:17:37,352 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2947\n",
            "2025-03-24 19:17:45,515 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2987\n",
            "2025-03-24 19:17:53,948 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3084\n",
            "2025-03-24 19:18:01,988 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2766\n",
            "2025-03-24 19:18:09,894 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-24 19:18:17,833 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2833\n",
            "2025-03-24 19:18:25,855 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2860\n",
            "2025-03-24 19:18:34,063 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2831\n",
            "2025-03-24 19:18:42,286 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-24 19:18:50,533 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2847\n",
            "2025-03-24 19:18:58,630 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2990\n",
            "2025-03-24 19:19:07,109 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3449\n",
            "2025-03-24 19:19:15,318 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-24 19:19:23,314 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2659\n",
            "2025-03-24 19:19:31,498 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2987\n",
            "2025-03-24 19:19:39,652 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-24 19:19:48,134 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2923\n",
            "2025-03-24 19:19:56,068 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2954\n",
            "2025-03-24 19:20:04,176 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3033\n",
            "2025-03-24 19:20:12,285 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2898\n",
            "2025-03-24 19:20:20,233 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2977\n",
            "2025-03-24 19:20:28,235 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2794\n",
            "2025-03-24 19:20:36,181 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2627\n",
            "2025-03-24 19:20:44,167 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-24 19:20:52,071 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2929\n",
            "2025-03-24 19:20:58,034 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2235\n",
            "2025-03-24 19:20:58,684 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0805\n",
            "2025-03-24 19:20:58,684 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 19:21:57,224 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2897, Val Loss: 0.3659, Val Acc: 0.8831\n",
            "2025-03-24 19:21:57,225 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 19:22:03,437 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2416\n",
            "2025-03-24 19:22:11,264 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2550\n",
            "2025-03-24 19:22:19,650 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3027\n",
            "2025-03-24 19:22:27,855 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-24 19:22:35,800 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3066\n",
            "2025-03-24 19:22:44,109 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2765\n",
            "2025-03-24 19:22:52,267 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-24 19:23:00,375 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-24 19:23:08,252 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2546\n",
            "2025-03-24 19:23:16,396 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-24 19:23:24,477 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3038\n",
            "2025-03-24 19:23:32,631 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-24 19:23:40,444 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2945\n",
            "2025-03-24 19:23:48,319 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2687\n",
            "2025-03-24 19:23:56,230 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2625\n",
            "2025-03-24 19:24:04,336 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-24 19:24:12,436 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2696\n",
            "2025-03-24 19:24:20,813 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2975\n",
            "2025-03-24 19:24:29,014 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2702\n",
            "2025-03-24 19:24:37,178 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-24 19:24:45,364 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-24 19:24:53,606 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2585\n",
            "2025-03-24 19:25:01,807 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2769\n",
            "2025-03-24 19:25:10,202 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2765\n",
            "2025-03-24 19:25:18,832 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2889\n",
            "2025-03-24 19:25:27,430 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2618\n",
            "2025-03-24 19:25:35,794 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-24 19:25:44,399 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-24 19:25:53,355 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-24 19:26:01,552 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2927\n",
            "2025-03-24 19:26:09,784 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-24 19:26:17,726 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2881\n",
            "2025-03-24 19:26:25,694 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-24 19:26:31,774 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2087\n",
            "2025-03-24 19:26:32,398 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0914\n",
            "2025-03-24 19:26:32,399 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 19:27:33,701 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2798, Val Loss: 0.3720, Val Acc: 0.8794\n",
            "2025-03-24 19:27:33,701 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-24 19:27:40,211 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1926\n",
            "2025-03-24 19:27:49,146 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2819\n",
            "2025-03-24 19:27:57,848 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2711\n",
            "2025-03-24 19:28:06,264 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2824\n",
            "2025-03-24 19:28:14,748 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2548\n",
            "2025-03-24 19:28:23,373 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2680\n",
            "2025-03-24 19:28:31,896 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2516\n",
            "2025-03-24 19:28:40,340 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2911\n",
            "2025-03-24 19:28:49,117 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2586\n",
            "2025-03-24 19:28:57,727 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-24 19:29:06,334 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2816\n",
            "2025-03-24 19:29:15,015 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2904\n",
            "2025-03-24 19:29:23,918 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2837\n",
            "2025-03-24 19:29:32,310 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-24 19:29:41,074 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2835\n",
            "2025-03-24 19:29:49,902 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2506\n",
            "2025-03-24 19:29:58,305 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2433\n",
            "2025-03-24 19:30:06,751 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2635\n",
            "2025-03-24 19:30:15,608 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2493\n",
            "2025-03-24 19:30:24,502 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2721\n",
            "2025-03-24 19:30:33,286 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2906\n",
            "2025-03-24 19:30:42,025 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-24 19:30:50,856 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2537\n",
            "2025-03-24 19:30:59,595 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2721\n",
            "2025-03-24 19:31:08,287 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2771\n",
            "2025-03-24 19:31:17,084 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2671\n",
            "2025-03-24 19:31:25,868 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2789\n",
            "2025-03-24 19:31:34,871 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2502\n",
            "2025-03-24 19:31:43,484 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2668\n",
            "2025-03-24 19:31:52,436 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2556\n",
            "2025-03-24 19:32:01,275 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2780\n",
            "2025-03-24 19:32:10,387 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2748\n",
            "2025-03-24 19:32:18,929 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2857\n",
            "2025-03-24 19:32:25,659 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1948\n",
            "2025-03-24 19:32:26,292 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0957\n",
            "2025-03-24 19:32:26,293 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 19:33:27,189 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2709, Val Loss: 0.3791, Val Acc: 0.8804\n",
            "2025-03-24 19:33:27,190 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-24 19:33:33,510 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1889\n",
            "2025-03-24 19:33:42,042 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2633\n",
            "2025-03-24 19:33:50,637 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-24 19:33:59,220 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2468\n",
            "2025-03-24 19:34:07,735 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2502\n",
            "2025-03-24 19:34:16,626 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2539\n",
            "2025-03-24 19:34:25,196 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-24 19:34:33,992 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-24 19:34:42,982 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2608\n",
            "2025-03-24 19:34:51,784 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2687\n",
            "2025-03-24 19:35:00,416 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2571\n",
            "2025-03-24 19:35:09,005 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-24 19:35:17,395 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-24 19:35:26,037 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2527\n",
            "2025-03-24 19:35:34,671 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2768\n",
            "2025-03-24 19:35:43,238 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-24 19:35:51,965 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2755\n",
            "2025-03-24 19:36:00,561 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-24 19:36:09,008 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2630\n",
            "2025-03-24 19:36:17,601 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2781\n",
            "2025-03-24 19:36:25,851 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2672\n",
            "2025-03-24 19:36:34,776 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2555\n",
            "2025-03-24 19:36:43,459 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-24 19:36:52,183 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2560\n",
            "2025-03-24 19:37:00,790 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2771\n",
            "2025-03-24 19:37:09,372 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-24 19:37:18,128 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2890\n",
            "2025-03-24 19:37:26,944 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2575\n",
            "2025-03-24 19:37:35,758 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2599\n",
            "2025-03-24 19:37:44,559 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2619\n",
            "2025-03-24 19:37:53,255 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2580\n",
            "2025-03-24 19:38:01,969 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2463\n",
            "2025-03-24 19:38:10,521 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2512\n",
            "2025-03-24 19:38:17,033 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1925\n",
            "2025-03-24 19:38:17,698 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0909\n",
            "2025-03-24 19:38:17,699 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 19:39:17,088 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2649, Val Loss: 0.3768, Val Acc: 0.8785\n",
            "2025-03-24 19:39:17,089 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-24 19:39:23,515 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1909\n",
            "2025-03-24 19:39:31,730 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2549\n",
            "2025-03-24 19:39:39,465 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2669\n",
            "2025-03-24 19:39:47,299 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2628\n",
            "2025-03-24 19:39:55,467 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2565\n",
            "2025-03-24 19:40:04,028 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-24 19:40:12,424 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2527\n",
            "2025-03-24 19:40:20,473 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2466\n",
            "2025-03-24 19:40:29,113 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-24 19:40:37,305 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2531\n",
            "2025-03-24 19:40:45,191 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2683\n",
            "2025-03-24 19:40:53,440 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-24 19:41:01,705 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2788\n",
            "2025-03-24 19:41:10,146 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2494\n",
            "2025-03-24 19:41:18,204 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2409\n",
            "2025-03-24 19:41:26,836 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-24 19:41:35,584 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2929\n",
            "2025-03-24 19:41:44,525 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2448\n",
            "2025-03-24 19:41:52,861 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2513\n",
            "2025-03-24 19:42:01,584 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-24 19:42:10,283 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2497\n",
            "2025-03-24 19:42:18,915 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-24 19:42:27,488 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2818\n",
            "2025-03-24 19:42:36,061 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-24 19:42:44,293 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2645\n",
            "2025-03-24 19:42:52,287 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-24 19:43:00,469 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2561\n",
            "2025-03-24 19:43:09,075 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2307\n",
            "2025-03-24 19:43:17,531 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-24 19:43:26,215 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2879\n",
            "2025-03-24 19:43:34,858 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3044\n",
            "2025-03-24 19:43:43,409 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2580\n",
            "2025-03-24 19:43:52,158 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2503\n",
            "2025-03-24 19:43:58,578 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2051\n",
            "2025-03-24 19:43:59,247 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0656\n",
            "2025-03-24 19:43:59,247 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 19:45:00,033 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2629, Val Loss: 0.3767, Val Acc: 0.8780\n",
            "2025-03-24 19:45:00,034 - INFO - [TRAIN INFO] Early stopping at epoch 20 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 19:45:00,034 - INFO - [TRAIN INFO] Total Time: 6967.33s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▁▂▃▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▅▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇▇▇███████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▂▁▂▁▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.2629</td></tr><tr><td>train_val_loss_diff</td><td>-0.1138</td></tr><tr><td>val_accuracy</td><td>0.87803</td></tr><tr><td>val_loss</td><td>0.37669</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_enetb0_224_simple_concat_fold_5</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/l1vbik9x' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/l1vbik9x</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_174850-l1vbik9x\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:45:02,274 - INFO - [TRAIN INFO] Fold 5 Training Complete at epoch 20. Total Time: 6969.57s\n",
            "2025-03-24 19:45:02,297 - INFO - [K-FOLD INFO] Fold 5 completed in 6972.96 seconds\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "logging.info(\"[K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "\n",
        "    fold_start_time = time.time()  # Start timing for this fold\n",
        "    logging.info(f\"[K-FOLD INFO] ============================== Fold {fold+1}/{K_FOLDS} ==============================\")\n",
        "\n",
        "    # Get train and validation subsets\n",
        "    train_texts_fold = train_texts[train_idx]\n",
        "    val_texts_fold = train_texts[val_idx]\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "    train_image_paths_fold = train_image_paths[train_idx]\n",
        "    val_image_paths_fold = train_image_paths[val_idx]\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train Samples: {len(train_texts_fold)}\")\n",
        "    logging.info(f\"   Validation Samples: {len(val_texts_fold)}\")\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_image_dataset = ImageDataset(train_image_paths_fold, train_labels_fold, transform[\"train\"])\n",
        "    val_image_dataset = ImageDataset(val_image_paths_fold, val_labels_fold, transform[\"val\"])\n",
        "    \n",
        "    train_text_dataset = CustomTextDataset(train_texts_fold, train_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "    val_text_dataset = CustomTextDataset(val_texts_fold, val_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # Create multimodal datasets\n",
        "    train_multimodal_dataset = MultimodalDataset(train_image_dataset, train_text_dataset)\n",
        "    val_multimodal_dataset = MultimodalDataset(val_image_dataset, val_text_dataset)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Created multimodal datasets for Fold {fold+1}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    dataloaders = {\n",
        "        \"train_loader\": DataLoader(train_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=True),\n",
        "        \"val_loader\": DataLoader(val_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    }\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] DataLoaders initialized for Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train batches: {len(dataloaders['train_loader'])}, Validation batches: {len(dataloaders['val_loader'])}\")\n",
        "\n",
        "    # Initialize model, optimizer, and criterion\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Model initialized on {device} for Fold {fold+1}\")\n",
        "\n",
        "    # Define Optimizer using AdamW\n",
        "    optimizer = optim.AdamW([\n",
        "        {\"params\": model.image_model.features[-3:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_IMAGE, \"weight_decay\": WEIGHT_DECAY_IMAGE},  # Unfrozen EfficientNet layer\n",
        "        {\"params\": model.text_model.transformer.layer[-2:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_TEXT, \"weight_decay\": WEIGHT_DECAY_TEXT},  # Unfrozen DistilBERT layer\n",
        "        {\"params\": model.image_fc.parameters(), \"lr\": LEARNING_RATE_IMAGE, \"weight_decay\": 0}, \n",
        "        {\"params\": model.text_fc.parameters(), \"lr\": LEARNING_RATE_TEXT, \"weight_decay\": 0},\n",
        "        {\"params\": model.fusion_fc.parameters(), \"lr\": LEARNING_RATE_FUSION, \"weight_decay\": WEIGHT_DECAY_FUSION},  \n",
        "        {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER, \"weight_decay\": WEIGHT_DECAY_CLASSIFIER}  \n",
        "    ], betas=(0.9, 0.999), eps=1e-8)  # Default AdamW betas and eps\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Optimizer initialized for Fold {fold+1}:\")\n",
        "    \n",
        "    # Define Loss Function\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING_PREDICTION) \n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Loss function initialized for Fold {fold+1}\")\n",
        "\n",
        "    # Train model for this fold\n",
        "    train_model(model, dataloaders, criterion, optimizer, device, fold)\n",
        "\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Measure Fold Time\n",
        "    fold_time = time.time() - fold_start_time\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1} completed in {fold_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for fold in range(K_FOLDS):\n",
        "#     logging.info(f\"\\n[TEST INFO] Evaluating Fold {fold + 1} on Test Set...\")\n",
        "\n",
        "#     # Load best model for the fold\n",
        "#     model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "#     model_path = f\"best_model_fold_{fold + 1}.pth\"\n",
        "    \n",
        "#     try:\n",
        "#         model.load_state_dict(torch.load(model_path))\n",
        "#         logging.info(f\"[TEST INFO] Loaded best model for Fold {fold + 1} from {model_path}\")\n",
        "#     except FileNotFoundError:\n",
        "#         logging.error(f\"[ERROR] Model file {model_path} not found! Skipping Fold {fold + 1} evaluation.\")\n",
        "#         continue  # Skip to the next fold if model file is missing\n",
        "\n",
        "#     model.eval()  # Set to evaluation mode\n",
        "\n",
        "#     # Evaluate model on test data\n",
        "#     test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "\n",
        "#     # Log test set performance for the fold\n",
        "#     logging.info(f\"[TEST INFO] Fold {fold + 1} Test Performance:\")\n",
        "#     logging.info(f\"   Test Loss: {test_loss:.4f}\")\n",
        "#     logging.info(f\"   Test Accuracy: {test_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "enel645_torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
