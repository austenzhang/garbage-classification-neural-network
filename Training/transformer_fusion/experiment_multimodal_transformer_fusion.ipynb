{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLOOi0GQM5B"
      },
      "source": [
        "## Garbage Classification Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
        "from torchvision.models.efficientnet import EfficientNet_B0_Weights\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import wandb\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import time\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "NOTES = '''\n",
        "'''\n",
        "\n",
        "# ========================================= GLOBAL CONFIGURATION ================================================\n",
        "# Data Directories\n",
        "DATA_DIR = r\"C:\\NN Data\\garbage_data\\kfold_garbage_data\"\n",
        "CLASSES = [\"Black\", \"Blue\", \"Green\", \"TTR\"]\n",
        "\n",
        "# ========================================= Experiment Settings =========================================\n",
        "WANDB_RUN_NAME = \"experiment_multimodal_transformer_fusion\"\n",
        "MODEL_NAME = \"experiment_multimodal_transformer_fusion\"\n",
        "\n",
        "# ========================================= Data Settings =========================================\n",
        "IMAGE_SIZE = (224, 224)  # Input image size for EfficientNetV2-S\n",
        "NUM_CLASSES = 4  # Number of output classes for classification\n",
        "MAX_LEN = 40  # Maximum token length for DistilBERT tokenizer\n",
        "TEST_SIZE = 0.2  # Test dataset size split\n",
        "K_FOLDS = 5  # Number of folds for stratified k-fold cross-validation\n",
        "\n",
        "# ========================================= Training Hyperparameters =========================================\n",
        "BATCH_SIZE = 64  # Number of samples per batch\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "EPOCHS = 50  # Maximum number of training epochs\n",
        "DROPOUT_IMAGE = 0.2 # Reduce from 0.3\n",
        "DROPOUT_TEXT = 0.1 # Reduce from 0.2\n",
        "DROPOUT_FUSION = 0.2 \n",
        "DROPOUT_CLASSIFIER = 0.1\n",
        "PATIENCE = 10  # Number of epochs to wait before early stopping\n",
        "CONVERGENCE_THRESHOLD = 0.001  # Minimum improvement in validation loss to continue training\n",
        "\n",
        "# ========================================= Optimization Settings =========================================\n",
        "OPTIMIZER = \"AdamW\"\n",
        "LR_SCHEDULING_FACTOR = 0.3\n",
        "LEARNING_RATE_UNFREEZE_IMAGE = 1e-5\n",
        "LEARNING_RATE_UNFREEZE_TEXT = 1e-5\n",
        "LEARNING_RATE_FUSION = 1e-3\n",
        "LEARNING_RATE_CLASSIFIER = 5e-3\n",
        "LEARNING_RATE_IMAGE = 0.001 # # EfficientNetB0\n",
        "LEARNING_RATE_TEXT = 0.00002 # DistilBERT Uncased\n",
        "WEIGHT_DECAY_TEXT = 1e-3  # Reduce from 1e-2\n",
        "WEIGHT_DECAY_IMAGE = 1e-4  # Reduce from 1e-3\n",
        "WEIGHT_DECAY_FUSION = 4e-4 \n",
        "WEIGHT_DECAY_CLASSIFIER = 1e-3  # Reduce from 1e-4\n",
        "LABEL_SMOOTHING_PREDICTION = 0.05 # Reduce from 0.1\n",
        "\n",
        "# ========================================= System Settings =========================================\n",
        "NUM_WORKERS = 4  # Dataloader parallelization\n",
        "\n",
        "# Wandb Configuration\n",
        "WANDB_CONFIG = {\n",
        "    \"entity\": \"shcau-university-of-calgary-in-alberta\",\n",
        "    \"project\": \"transfer_learning_garbage\",\n",
        "    \"name\": WANDB_RUN_NAME,\n",
        "    \"tags\": [\"distilBERT\", \"efficientnet\", \"CVPR_2024_dataset\"],\n",
        "    \"notes\": NOTES,\n",
        "    \"config\": {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"dataset\": \"CVPR_2024_dataset\",\n",
        "        \"image_size\": IMAGE_SIZE,\n",
        "        \"num_workers\": NUM_WORKERS,\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"max_len\": MAX_LEN,\n",
        "        \"learning_rate_image\": LEARNING_RATE_IMAGE,\n",
        "        \"learning_rate_text\": LEARNING_RATE_TEXT,\n",
        "        \"learning_rate_fusion\": LEARNING_RATE_FUSION,\n",
        "        \"learning_rate_classifier\": LEARNING_RATE_CLASSIFIER,\n",
        "        \"learning_rate_unfreeze_image\": LEARNING_RATE_UNFREEZE_IMAGE, # learning rate for unfrozen EfficientNet layers\n",
        "        \"learning_rate_unfreeze_text\": LEARNING_RATE_UNFREEZE_TEXT, # learning rate for unfrozen DistilBERT layers\n",
        "        \"dropout_image\": DROPOUT_IMAGE,\n",
        "        \"dropout_text\": DROPOUT_TEXT,\n",
        "        \"dropout_classifier\": DROPOUT_CLASSIFIER,\n",
        "        \"convergence_threshold\": CONVERGENCE_THRESHOLD,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"weight_decay_text\": WEIGHT_DECAY_TEXT,\n",
        "        \"weight_decay_image\": WEIGHT_DECAY_IMAGE,\n",
        "        \"weight_decay_classifier\": WEIGHT_DECAY_CLASSIFIER,\n",
        "        \"label_smoothing_prediction\": LABEL_SMOOTHING_PREDICTION,\n",
        "        \"optimizer\": OPTIMIZER \n",
        "    },\n",
        "    \"job_type\": \"train\",\n",
        "    \"resume\": \"allow\",\n",
        "}\n",
        "\n",
        "# Normalization Stats\n",
        "NORMALIZATION_STATS = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOG_FILE = \"experiment_multimodal_transformer_fusion.txt\"  # Log file name\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Log everything (INFO and above)\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(LOG_FILE, mode='w'),  # Overwrite log file on each run\n",
        "        logging.StreamHandler(sys.stdout)  # Print log messages to console too\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:05:09,735 - INFO - [CONFIG] ============================== Experiment Configuration ==============================\n",
            "2025-03-24 01:05:09,735 - INFO - [CONFIG] Experiment Name: experiment_multimodal_transformer_fusion\n",
            "2025-03-24 01:05:09,735 - INFO - [CONFIG] Entity: shcau-university-of-calgary-in-alberta\n",
            "2025-03-24 01:05:09,737 - INFO - [CONFIG] Project: transfer_learning_garbage\n",
            "2025-03-24 01:05:09,738 - INFO - [CONFIG] Tags: distilBERT, efficientnet, CVPR_2024_dataset\n",
            "2025-03-24 01:05:09,738 - INFO - [CONFIG] Notes: \n",
            "\n",
            "2025-03-24 01:05:09,739 - INFO - [CONFIG] Job Type: train\n",
            "2025-03-24 01:05:09,740 - INFO - [CONFIG] Resume: allow\n",
            "2025-03-24 01:05:09,740 - INFO - [CONFIG] ------------------------------ Hyperparameters ------------------------------\n",
            "2025-03-24 01:05:09,741 - INFO - [CONFIG] epochs: 50\n",
            "2025-03-24 01:05:09,741 - INFO - [CONFIG] batch_size: 64\n",
            "2025-03-24 01:05:09,742 - INFO - [CONFIG] dataset: CVPR_2024_dataset\n",
            "2025-03-24 01:05:09,742 - INFO - [CONFIG] image_size: (224, 224)\n",
            "2025-03-24 01:05:09,743 - INFO - [CONFIG] num_workers: 4\n",
            "2025-03-24 01:05:09,743 - INFO - [CONFIG] num_classes: 4\n",
            "2025-03-24 01:05:09,744 - INFO - [CONFIG] max_len: 40\n",
            "2025-03-24 01:05:09,744 - INFO - [CONFIG] learning_rate_image: 0.001\n",
            "2025-03-24 01:05:09,745 - INFO - [CONFIG] learning_rate_text: 2e-05\n",
            "2025-03-24 01:05:09,746 - INFO - [CONFIG] learning_rate_fusion: 0.001\n",
            "2025-03-24 01:05:09,746 - INFO - [CONFIG] learning_rate_classifier: 0.005\n",
            "2025-03-24 01:05:09,747 - INFO - [CONFIG] learning_rate_unfreeze_image: 1e-05\n",
            "2025-03-24 01:05:09,747 - INFO - [CONFIG] learning_rate_unfreeze_text: 1e-05\n",
            "2025-03-24 01:05:09,748 - INFO - [CONFIG] dropout_image: 0.2\n",
            "2025-03-24 01:05:09,749 - INFO - [CONFIG] dropout_text: 0.1\n",
            "2025-03-24 01:05:09,749 - INFO - [CONFIG] dropout_classifier: 0.1\n",
            "2025-03-24 01:05:09,750 - INFO - [CONFIG] convergence_threshold: 0.001\n",
            "2025-03-24 01:05:09,750 - INFO - [CONFIG] patience: 10\n",
            "2025-03-24 01:05:09,751 - INFO - [CONFIG] weight_decay_text: 0.001\n",
            "2025-03-24 01:05:09,751 - INFO - [CONFIG] weight_decay_image: 0.0001\n",
            "2025-03-24 01:05:09,752 - INFO - [CONFIG] weight_decay_classifier: 0.001\n",
            "2025-03-24 01:05:09,753 - INFO - [CONFIG] label_smoothing_prediction: 0.05\n",
            "2025-03-24 01:05:09,754 - INFO - [CONFIG] optimizer: AdamW\n",
            "2025-03-24 01:05:09,754 - INFO - [CONFIG] =============================================================================\n"
          ]
        }
      ],
      "source": [
        "# Log the configuration\n",
        "logging.info(\"[CONFIG] ============================== Experiment Configuration ==============================\")\n",
        "\n",
        "# Log top-level keys\n",
        "logging.info(f\"[CONFIG] Experiment Name: {WANDB_CONFIG['name']}\")\n",
        "logging.info(f\"[CONFIG] Entity: {WANDB_CONFIG['entity']}\")\n",
        "logging.info(f\"[CONFIG] Project: {WANDB_CONFIG['project']}\")\n",
        "logging.info(f\"[CONFIG] Tags: {', '.join(WANDB_CONFIG['tags'])}\")\n",
        "logging.info(f\"[CONFIG] Notes: {WANDB_CONFIG['notes']}\")\n",
        "logging.info(f\"[CONFIG] Job Type: {WANDB_CONFIG['job_type']}\")\n",
        "logging.info(f\"[CONFIG] Resume: {WANDB_CONFIG['resume']}\")\n",
        "\n",
        "# Log nested configuration (under 'config')\n",
        "logging.info(\"[CONFIG] ------------------------------ Hyperparameters ------------------------------\")\n",
        "for key, value in WANDB_CONFIG[\"config\"].items():\n",
        "    logging.info(f\"[CONFIG] {key}: {value}\")\n",
        "\n",
        "logging.info(\"[CONFIG] =============================================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_wandb(fold):\n",
        "    \"\"\"Initialize wandb for each fold with a unique run name.\"\"\"\n",
        "    wandb.init(\n",
        "        entity=WANDB_CONFIG[\"entity\"],\n",
        "        project=WANDB_CONFIG[\"project\"],\n",
        "        name=f\"{WANDB_RUN_NAME}_fold_{fold + 1}\",\n",
        "        tags=WANDB_CONFIG[\"tags\"],\n",
        "        notes=WANDB_CONFIG[\"notes\"],\n",
        "        config=WANDB_CONFIG[\"config\"],\n",
        "        job_type=WANDB_CONFIG[\"job_type\"],\n",
        "        resume=WANDB_CONFIG[\"resume\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SpaCy for lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load NLTK stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Standardize text, remove stopwords, and apply lemmatization.\"\"\"\n",
        "    # 1. Standardize text (lowercasing & trimming spaces)\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # 2. Remove stopwords\n",
        "    text_tokens = text.split()\n",
        "    text = \" \".join([word for word in text_tokens if word not in stop_words])\n",
        "\n",
        "    # 3. Lemmatization\n",
        "    doc = nlp(text)\n",
        "    text = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "    return text\n",
        "\n",
        "def read_text_files_with_labels_and_image_paths(path):\n",
        "    \"\"\"Extract text from file names, apply preprocessing, and return labels with image paths.\"\"\"\n",
        "    texts, labels, image_paths = [], [], []\n",
        "    class_folders = sorted(os.listdir(path))\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = sorted(os.listdir(class_path))  # Sort to ensure order consistency\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    # Extract filename without extension\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "\n",
        "                    # Replace underscores with spaces\n",
        "                    text = file_name_no_ext.replace(\"_\", \" \")\n",
        "\n",
        "                    # Remove numbers\n",
        "                    text_without_digits = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "                    # Apply preprocessing\n",
        "                    preprocessed_text = preprocess_text(text_without_digits)\n",
        "\n",
        "                    texts.append(preprocessed_text)\n",
        "                    labels.append(label_map[class_name])\n",
        "                    image_paths.append(file_path)\n",
        "\n",
        "    return np.array(texts), np.array(labels), np.array(image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:05:10,376 - INFO - [INFO] Extracting text, labels, and image paths...\n"
          ]
        }
      ],
      "source": [
        "logging.info(\"[INFO] Extracting text, labels, and image paths...\")\n",
        "texts, labels, image_paths= read_text_files_with_labels_and_image_paths(DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    \"\"\"Dataset class for text data.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "    \n",
        "# Custom dataset class for images\n",
        "class ImageDataset(Dataset):\n",
        "    \"\"\"Dataset class for image data.\"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    \"\"\"Dataset class for multimodal data (image + text).\"\"\"\n",
        "    def __init__(self, image_dataset, text_dataset):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_dataset), len(self.text_dataset))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.image_dataset[idx]\n",
        "        text_data = self.text_dataset[idx]\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": text_data[\"input_ids\"],\n",
        "            \"attention_mask\": text_data[\"attention_mask\"],\n",
        "            \"label\": label\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    \"\"\"Multimodal model combining EfficientNetB0 and DistilBERT with Transformer-based fusion.\"\"\"\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # ----------- Image Feature Extractor (EfficientNetB0) -----------\n",
        "        self.image_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        \n",
        "        # Freeze all layers except the last one\n",
        "        for param in self.image_model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.image_model.features[-3:].parameters():  # Unfreeze last feature layer\n",
        "            param.requires_grad = True\n",
        "\n",
        "        num_ftrs = self.image_model.classifier[1].in_features\n",
        "        self.image_model.classifier = nn.Identity()  # Remove classifier\n",
        "        self.image_fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_IMAGE)  # Use global dropout for image features\n",
        "        )\n",
        "\n",
        "        # ----------- Text Feature Extractor (DistilBERT) -----------\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # Freeze all layers except the last transformer layer\n",
        "        for param in self.text_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.text_model.transformer.layer[-2:].parameters():  # Unfreeze last transformer layer\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(self.text_model.config.hidden_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_TEXT)  # Use global dropout for text features\n",
        "        )\n",
        "\n",
        "        # ----------- Normalize Features -----------\n",
        "        self.text_norm = nn.LayerNorm(512)\n",
        "        self.image_norm = nn.LayerNorm(512)\n",
        "\n",
        "        # ----------- Transformer Fusion -----------\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=512,  # Match the feature dimension\n",
        "            nhead=8,       # Number of attention heads\n",
        "            dim_feedforward=4096,  # Feedforward dimension\n",
        "            dropout=0.1,   # Dropout rate\n",
        "            activation=\"gelu\",  # Use GELU activation\n",
        "            norm_first=True  # Enable layer normalization\n",
        "        )\n",
        "        self.transformer_fusion = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=4  # Number of encoder layers\n",
        "        )\n",
        "\n",
        "        # ----------- Fusion Layers -----------\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512, 512),  # Increase dimension\n",
        "            nn.BatchNorm1d(512),  # Add batch normalization\n",
        "            nn.ReLU(),            # Use RELU activation\n",
        "\n",
        "            nn.Linear(512, 256),  # Intermediate layer\n",
        "            nn.BatchNorm1d(256),  # Batch normalization\n",
        "            nn.ReLU(),            # RELU activation\n",
        "        )\n",
        "\n",
        "        # ----------- Classifier Layer -----------\n",
        "        self.dropout = nn.Dropout(DROPOUT_CLASSIFIER)\n",
        "        self.classifier = nn.Linear(256, num_classes)  # Final output layer\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, image_inputs):\n",
        "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(text_output.last_hidden_state[:, 0, :])\n",
        "        text_features = self.text_norm(text_features)\n",
        "        image_features = self.image_fc(self.image_model(image_inputs))\n",
        "        image_features = self.image_norm(image_features)\n",
        "        combined = torch.cat((image_features.unsqueeze(1), text_features.unsqueeze(1)), dim=1)\n",
        "        fused_features = self.transformer_fusion(combined)\n",
        "        fused_features = fused_features.mean(dim=1)\n",
        "        fused_features = self.fusion_fc(fused_features)\n",
        "        output = self.classifier(self.dropout(fused_features))\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:08,693 - INFO - First 4 samples of dataset:\n",
            "\n",
            "2025-03-24 01:06:08,694 - INFO - Texts: ['aero bar wrapper' 'break glass' 'break rubber' 'butter paper']\n",
            "2025-03-24 01:06:08,694 - INFO - Labels: [0 0 0 0]\n",
            "2025-03-24 01:06:08,695 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Aero_bar_wrapper_1.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_Glass_5291.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_rubber_7263.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Butter_Paper_9976.png']\n",
            "2025-03-24 01:06:08,696 - INFO - \n",
            "Last 4 samples of dataset:\n",
            "\n",
            "2025-03-24 01:06:08,696 - INFO - Texts: ['wristwatch' 'xbox controller' 'xbox one controller' 'zipper file bag']\n",
            "2025-03-24 01:06:08,697 - INFO - Labels: [3 3 3 3]\n",
            "2025-03-24 01:06:08,698 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\wristwatch_3782.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_controller_2047.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_one_controller_2048.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\zipper_file_bag_2049.png']\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "texts, labels, image_paths = read_text_files_with_labels_and_image_paths(DATA_DIR)\n",
        "\n",
        "# Log first and last 4 samples\n",
        "logging.info(\"First 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[:4]}\")\n",
        "logging.info(f\"Labels: {labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[-4:]}\")\n",
        "logging.info(f\"Labels: {labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split into test set and development set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:08,734 - INFO - First 4 samples of test set:\n",
            "\n",
            "2025-03-24 01:06:08,735 - INFO - Texts: ['ballast light' 'old phone' 'milk jug lid tab' 'dirty dish sponge']\n",
            "2025-03-24 01:06:08,736 - INFO - Labels: [3 3 0 0]\n",
            "2025-03-24 01:06:08,737 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\ballast_light_286.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\Old_Phones_7828.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\milk_jug_lid_tab_1137.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\dirty_dish_sponge_437.png']\n",
            "2025-03-24 01:06:08,737 - INFO - \n",
            "Last 4 samples of test set:\n",
            "\n",
            "2025-03-24 01:06:08,738 - INFO - Texts: ['empty glass jar' 'non - stretchy plastic' 'backpack' 'piece break glass']\n",
            "2025-03-24 01:06:08,740 - INFO - Labels: [1 0 3 0]\n",
            "2025-03-24 01:06:08,741 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Blue\\\\empty_glass_jar_1609.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\non-stretchy plastic.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\backpack_216.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\piece_of_broken_glass_1315.png']\n"
          ]
        }
      ],
      "source": [
        "# Split into a test set and development set\n",
        "train_texts, test_texts, train_labels, test_labels, train_image_paths, test_image_paths = train_test_split(\n",
        "    texts, labels, image_paths, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Log first 4 samples of test set\n",
        "logging.info(\"First 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[:4]}\")\n",
        "logging.info(f\"Labels: {test_labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[-4:]}\")\n",
        "logging.info(f\"Labels: {test_labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE), \n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Apply correct normalization\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Only resize + normalize\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Only resize + normalize\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Tokenizer for DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoader for test set\n",
        "\n",
        "Create the dataloader for the test set and set aside for model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test dataset\n",
        "test_image_dataset = ImageDataset(test_image_paths, test_labels, transform[\"test\"])\n",
        "test_text_dataset = CustomTextDataset(test_texts, test_labels, tokenizer, max_len=MAX_LEN)  # Ensure tokenizer is defined\n",
        "test_multimodal_dataset = MultimodalDataset(test_image_dataset, test_text_dataset)\n",
        "\n",
        "# DataLoader for test set\n",
        "test_loader = DataLoader(test_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a peek at a batch in the test set to verify that data has been correctly organized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:10,815 - INFO - [INFO] One Batch Sample Inspection:\n",
            "2025-03-24 01:06:10,816 - INFO -    Images Shape: torch.Size([64, 3, 224, 224])\n",
            "2025-03-24 01:06:10,816 - INFO -    Input IDs Shape: torch.Size([64, 40])\n",
            "2025-03-24 01:06:10,817 - INFO -    Attention Mask Shape: torch.Size([64, 40])\n",
            "2025-03-24 01:06:10,818 - INFO -    Labels Shape: torch.Size([64])\n",
            "2025-03-24 01:06:10,818 - INFO - \n",
            "[INFO] First Sample:\n",
            "2025-03-24 01:06:10,823 - INFO -    Image Tensor: tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         ...,\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
            "\n",
            "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         ...,\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
            "\n",
            "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         ...,\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]])\n",
            "2025-03-24 01:06:10,824 - INFO -    Input IDs: tensor([  101, 28030,  2422,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "2025-03-24 01:06:10,826 - INFO -    Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "2025-03-24 01:06:10,826 - INFO -    Label: 3\n"
          ]
        }
      ],
      "source": [
        "# Get one batch\n",
        "for batch in test_loader:\n",
        "    images = batch[\"image\"]  # Image tensor\n",
        "    input_ids = batch[\"input_ids\"]  # Tokenized text tensor\n",
        "    attention_mask = batch[\"attention_mask\"]  # Attention mask\n",
        "    labels = batch[\"label\"]  # Labels tensor\n",
        "\n",
        "    # Log shapes of tensors\n",
        "    logging.info(\"[INFO] One Batch Sample Inspection:\")\n",
        "    logging.info(f\"   Images Shape: {images.shape}\")\n",
        "    logging.info(f\"   Input IDs Shape: {input_ids.shape}\")\n",
        "    logging.info(f\"   Attention Mask Shape: {attention_mask.shape}\")\n",
        "    logging.info(f\"   Labels Shape: {labels.shape}\")\n",
        "\n",
        "    # Log first sample details\n",
        "    logging.info(\"\\n[INFO] First Sample:\")\n",
        "    logging.info(f\"   Image Tensor: {images[0]}\")\n",
        "    logging.info(f\"   Input IDs: {input_ids[0]}\")\n",
        "    logging.info(f\"   Attention Mask: {attention_mask[0]}\")\n",
        "    logging.info(f\"   Label: {labels[0]}\")\n",
        "\n",
        "    break  # Stop after inspecting one batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Stratified K-Fold on the development set to split into train/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:10,844 - INFO - [INFO] Fold 1/5\n",
            "2025-03-24 01:06:10,845 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 01:06:10,846 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 01:06:10,847 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 01:06:10,847 - INFO - [INFO] Fold 2/5\n",
            "2025-03-24 01:06:10,848 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 01:06:10,850 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 01:06:10,851 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 01:06:10,852 - INFO - [INFO] Fold 3/5\n",
            "2025-03-24 01:06:10,853 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 01:06:10,854 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 01:06:10,855 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 01:06:10,856 - INFO - [INFO] Fold 4/5\n",
            "2025-03-24 01:06:10,857 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 01:06:10,859 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-24 01:06:10,859 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n",
            "2025-03-24 01:06:10,860 - INFO - [INFO] Fold 5/5\n",
            "2025-03-24 01:06:10,861 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 01:06:10,862 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-24 01:06:10,863 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    logging.info(f\"[INFO] Fold {fold + 1}/{K_FOLDS}\")\n",
        "\n",
        "    # Extract labels for current fold\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "\n",
        "    # Log class distributions\n",
        "    logging.info(\"[INFO] Class Distributions:\")\n",
        "    logging.info(f\"   Train Class Distribution: {Counter(train_labels_fold)}\")\n",
        "    logging.info(f\"   Validation Class Distribution: {Counter(val_labels_fold)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify k-fold was applied correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:10,880 - INFO - [INFO] No data leakage detected in Fold 1\n",
            "2025-03-24 01:06:10,882 - INFO - [INFO] No data leakage detected in Fold 2\n",
            "2025-03-24 01:06:10,883 - INFO - [INFO] No data leakage detected in Fold 3\n",
            "2025-03-24 01:06:10,885 - INFO - [INFO] No data leakage detected in Fold 4\n",
            "2025-03-24 01:06:10,886 - INFO - [INFO] No data leakage detected in Fold 5\n"
          ]
        }
      ],
      "source": [
        "# Ensure no data leakage in folds\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    train_set = set(train_idx)\n",
        "    val_set = set(val_idx)\n",
        "\n",
        "    # Check for intersection (should be empty)\n",
        "    intersection = train_set.intersection(val_set)\n",
        "    assert len(intersection) == 0, f\"Data leakage detected in Fold {fold + 1}\"\n",
        "\n",
        "    logging.info(f\"[INFO] No data leakage detected in Fold {fold + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Move data to the appropriate device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)  # Compute batch loss\n",
        "\n",
        "            # Aggregate loss for averaging\n",
        "            total_loss += loss.item() * labels.size(0)  # Multiply by batch size for proper averaging\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total  # Normalize loss over total samples\n",
        "    accuracy = correct / total  # Compute accuracy\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Weight Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adaptive_weight_decay(epoch, warmup_epochs=5, decay_factors=(0.1, 1.0)):\n",
        "    \"\"\"\n",
        "    Returns a scaled weight decay based on epoch number.\n",
        "    During warm-up, it applies a lower decay (decay_factors[0]).\n",
        "    After warm-up, it applies full weight decay (decay_factors[1]).\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return decay_factors[0]  # Use lower decay during warm-up\n",
        "    return decay_factors[1]  # Use normal decay afterward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_warmup_lr(epoch, warmup_epochs, base_lr):\n",
        "    \"\"\"\n",
        "    Linear warmup schedule for the learning rate.\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return base_lr * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        return base_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True):\n",
        "    initialize_wandb(fold)\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    best_val_loss = float(\"inf\")  # Track best validation loss\n",
        "    epochs_without_improvement = 0  # Track epochs without improvement until equals patience\n",
        "\n",
        "    # ================ ReduceLROnPlateau Scheduler ================\n",
        "    plateau_scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=LR_SCHEDULING_FACTOR, patience=3, verbose=True\n",
        "    )\n",
        "    \n",
        "    # AMP GradScaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    epoch_start_time = time.time()  # Start total training timer\n",
        "    logging.info(\"[TRAIN INFO] Starting Training...\")\n",
        "\n",
        "    # Warmup settings\n",
        "    WARMUP_EPOCHS = 8  # Number of epochs for warmup\n",
        "    base_lr_image = LEARNING_RATE_IMAGE  # Base learning rate for EfficientNet\n",
        "    base_lr_text = LEARNING_RATE_TEXT  # Base learning rate for DistilBERT\n",
        "    base_lr_fusion = LEARNING_RATE_FUSION  # Base learning rate for fusion layer\n",
        "    base_lr_classifier = LEARNING_RATE_CLASSIFIER  # Base learning rate for classifier\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        logging.info(f\"[TRAIN INFO] ============================== Epoch {epoch + 1}/{EPOCHS} ==============================\")\n",
        "        \n",
        "        # Apply learning rate warmup\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            warmup_lr_image = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_image)\n",
        "            warmup_lr_text = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_text)\n",
        "            warmup_lr_fusion = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_fusion)\n",
        "            warmup_lr_classifier = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_classifier)\n",
        "\n",
        "            # Update learning rates for each parameter group\n",
        "            optimizer.param_groups[0][\"lr\"] = warmup_lr_image  # Unfrozen EfficientNet layer\n",
        "            optimizer.param_groups[1][\"lr\"] = warmup_lr_text  # Unfrozen DistilBERT layer\n",
        "            optimizer.param_groups[2][\"lr\"] = warmup_lr_image  # Image FC layer\n",
        "            optimizer.param_groups[3][\"lr\"] = warmup_lr_text  # Text FC layer\n",
        "            optimizer.param_groups[4][\"lr\"] = warmup_lr_fusion  # Fusion layer\n",
        "            optimizer.param_groups[5][\"lr\"] = warmup_lr_classifier  # Classifier layer\n",
        "\n",
        "        model.train()  # Set model to training modes\n",
        "        total_train_loss = 0  # Track total training loss for the epoch\n",
        "        batch_train_loss = 0  # Track batch loss for gradient accumulation\n",
        "        step = 0  # Track the number of batches processed\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Training phase\n",
        "        for step, batch in enumerate(dataloaders[\"train_loader\"], 1):\n",
        "            # Move data to device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask, images)  # Send inputs to network and receive outputs\n",
        "                loss = criterion(outputs, labels) / GRAD_ACCUM_STEPS  # Compute loss (no normalization for gradient accumulation)\n",
        "\n",
        "            # Backward pass and optimizer step\n",
        "            scaler.scale(loss).backward()  # Scale loss and backpropagate\n",
        "\n",
        "            batch_train_loss += loss.item()\n",
        "            total_train_loss += loss.item() * GRAD_ACCUM_STEPS  # Undo normalization for total loss\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            # Perform optimizer step before learning rate scheduler step\n",
        "            if step % GRAD_ACCUM_STEPS == 0 or step == len(dataloaders[\"train_loader\"]):\n",
        "                # Gradient Clipping\n",
        "                scaler.unscale_(optimizer)  # Unscale gradients before clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to a max norm of 1.0\n",
        "\n",
        "                # Optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Log batch loss\n",
        "                logging.info(f\"[TRAIN INFO] Batch {step}/{len(dataloaders['train_loader'])}, Accumulated loss over {GRAD_ACCUM_STEPS} batches: {batch_train_loss:.4f}\")\n",
        "                batch_train_loss = 0  # Reset batch loss for the next accumulation\n",
        "\n",
        "        # Validation step to see how well model performs this epoch\n",
        "        logging.info(f\"[TRAIN INFO] Evaluating model...\")\n",
        "        val_loss, val_acc = evaluate_model(model, dataloaders[\"val_loader\"], device)\n",
        "        avg_train_loss = total_train_loss / len(dataloaders[\"train_loader\"])\n",
        "\n",
        "        # **Learning Rate Scheduler Handling**\n",
        "        plateau_scheduler.step(val_loss)  \n",
        "\n",
        "        # Log weight decay and learning rate updates\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_val_loss_diff\": avg_train_loss - val_loss,  # Track overfitting tendency\n",
        "            \"early_stopping_epochs\": epochs_without_improvement,  # Track early stopping\n",
        "            \"learning_rate_image\": optimizer.param_groups[0][\"lr\"],  # Log learning rates\n",
        "            \"learning_rate_text\": optimizer.param_groups[1][\"lr\"],\n",
        "            \"learning_rate_fusion\": optimizer.param_groups[4][\"lr\"],\n",
        "            \"learning_rate_classifier\": optimizer.param_groups[5][\"lr\"],\n",
        "        })\n",
        "\n",
        "        logging.info(f\"[TRAIN INFO] Epoch {epoch + 1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss - CONVERGENCE_THRESHOLD:  # If loss improves, save the model\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0  # Reset epochs without improvement counter for patience\n",
        "            torch.save(model.state_dict(), f\"{MODEL_NAME}_fold_{fold+1}.pth\")\n",
        "            logging.info(f\"[TRAIN INFO] Best Model Saved for Fold {fold + 1}\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1  # Increment until patience reached\n",
        "\n",
        "        # Early stopping if no improvement for epochs\n",
        "        if epochs_without_improvement >= PATIENCE:\n",
        "            total_training_time = time.time() - epoch_start_time\n",
        "            logging.info(f\"[TRAIN INFO] Early stopping at epoch {epoch + 1} as validation loss did not improve for {PATIENCE} epochs.\")\n",
        "            logging.info(f\"[TRAIN INFO] Total Time: {total_training_time:.2f}s\")\n",
        "            wandb.finish()\n",
        "            break\n",
        "\n",
        "    total_training_time = time.time() - epoch_start_time\n",
        "    logging.info(f\"[TRAIN INFO] Fold {fold + 1} Training Complete at epoch {epoch + 1}. Total Time: {total_training_time:.2f}s\")\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:10,962 - INFO - [K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\n",
            "2025-03-24 01:06:10,965 - INFO - [K-FOLD INFO] ============================== Fold 2/5 ==============================\n",
            "2025-03-24 01:06:10,969 - INFO - [K-FOLD INFO] Fold 2:\n",
            "2025-03-24 01:06:10,970 - INFO -    Train Samples: 8594\n",
            "2025-03-24 01:06:10,971 - INFO -    Validation Samples: 2149\n",
            "2025-03-24 01:06:10,972 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 2\n",
            "2025-03-24 01:06:10,972 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 2:\n",
            "2025-03-24 01:06:10,973 - INFO -    Train batches: 135, Validation batches: 34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arkzs\\miniforge3\\envs\\enel645_torch_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:11,857 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 2\n",
            "2025-03-24 01:06:11,858 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 2:\n",
            "2025-03-24 01:06:11,859 - INFO - [K-FOLD INFO] Loss function initialized for Fold 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: shcau (shcau-university-of-calgary-in-alberta) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_transformer_fusion\\wandb\\run-20250324_010613-ew7pjung</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ew7pjung' target=\"_blank\">experiment_multimodal_transformer_fusion_fold_2</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ew7pjung' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ew7pjung</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arkzs\\miniforge3\\envs\\enel645_torch_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_47316\\836902376.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:14,194 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 01:06:14,194 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_47316\\836902376.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 01:06:21,363 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0925\n",
            "2025-03-24 01:06:29,730 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4218\n",
            "2025-03-24 01:06:38,055 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.4156\n",
            "2025-03-24 01:06:46,292 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.4337\n",
            "2025-03-24 01:06:54,856 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3772\n",
            "2025-03-24 01:07:03,883 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3550\n",
            "2025-03-24 01:07:12,657 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.3010\n",
            "2025-03-24 01:07:21,367 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2747\n",
            "2025-03-24 01:07:30,029 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2587\n",
            "2025-03-24 01:07:38,774 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2709\n",
            "2025-03-24 01:07:47,434 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2294\n",
            "2025-03-24 01:07:55,753 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.2671\n",
            "2025-03-24 01:08:04,654 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1981\n",
            "2025-03-24 01:08:13,474 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1729\n",
            "2025-03-24 01:08:22,083 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1406\n",
            "2025-03-24 01:08:30,845 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.2033\n",
            "2025-03-24 01:08:39,856 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1714\n",
            "2025-03-24 01:08:48,649 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.1218\n",
            "2025-03-24 01:08:57,457 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0731\n",
            "2025-03-24 01:09:06,294 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.1544\n",
            "2025-03-24 01:09:15,446 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.1997\n",
            "2025-03-24 01:09:24,382 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.1137\n",
            "2025-03-24 01:09:33,238 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0963\n",
            "2025-03-24 01:09:42,030 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0874\n",
            "2025-03-24 01:09:51,217 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 1.0874\n",
            "2025-03-24 01:10:00,205 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.1333\n",
            "2025-03-24 01:10:09,220 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 1.0656\n",
            "2025-03-24 01:10:18,026 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 1.0611\n",
            "2025-03-24 01:10:27,001 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 1.0147\n",
            "2025-03-24 01:10:35,817 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 1.0490\n",
            "2025-03-24 01:10:44,744 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 1.0386\n",
            "2025-03-24 01:10:53,531 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 1.0264\n",
            "2025-03-24 01:11:02,303 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9686\n",
            "2025-03-24 01:11:08,977 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7602\n",
            "2025-03-24 01:11:09,748 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2266\n",
            "2025-03-24 01:11:09,749 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:12:12,802 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1811, Val Loss: 0.8465, Val Acc: 0.6882\n",
            "2025-03-24 01:12:13,142 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 01:12:13,142 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 01:12:20,172 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.7320\n",
            "2025-03-24 01:12:28,951 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9978\n",
            "2025-03-24 01:12:37,783 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.9121\n",
            "2025-03-24 01:12:46,682 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9607\n",
            "2025-03-24 01:12:55,687 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.9861\n",
            "2025-03-24 01:13:04,524 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.9433\n",
            "2025-03-24 01:13:13,166 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8931\n",
            "2025-03-24 01:13:21,759 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.9307\n",
            "2025-03-24 01:13:30,173 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8594\n",
            "2025-03-24 01:13:38,572 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8641\n",
            "2025-03-24 01:13:47,033 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8470\n",
            "2025-03-24 01:13:55,760 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.9847\n",
            "2025-03-24 01:14:04,319 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7633\n",
            "2025-03-24 01:14:13,037 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.9257\n",
            "2025-03-24 01:14:21,746 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.8541\n",
            "2025-03-24 01:14:30,468 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8394\n",
            "2025-03-24 01:14:39,155 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.9128\n",
            "2025-03-24 01:14:47,711 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.8336\n",
            "2025-03-24 01:14:56,538 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.8240\n",
            "2025-03-24 01:15:05,020 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8201\n",
            "2025-03-24 01:15:13,352 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7989\n",
            "2025-03-24 01:15:22,012 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7712\n",
            "2025-03-24 01:15:30,925 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.8642\n",
            "2025-03-24 01:15:39,715 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8526\n",
            "2025-03-24 01:15:48,473 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.8346\n",
            "2025-03-24 01:15:57,083 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.8952\n",
            "2025-03-24 01:16:05,475 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.8102\n",
            "2025-03-24 01:16:13,906 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7508\n",
            "2025-03-24 01:16:22,576 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.8597\n",
            "2025-03-24 01:16:31,226 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7629\n",
            "2025-03-24 01:16:39,812 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8113\n",
            "2025-03-24 01:16:48,496 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8626\n",
            "2025-03-24 01:16:57,095 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7935\n",
            "2025-03-24 01:17:03,682 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5534\n",
            "2025-03-24 01:17:04,337 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1837\n",
            "2025-03-24 01:17:04,338 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:18:05,890 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8619, Val Loss: 0.6279, Val Acc: 0.7576\n",
            "2025-03-24 01:18:06,285 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 01:18:06,285 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 01:18:12,871 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5139\n",
            "2025-03-24 01:18:21,296 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.7244\n",
            "2025-03-24 01:18:29,794 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8216\n",
            "2025-03-24 01:18:38,452 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.7456\n",
            "2025-03-24 01:18:46,995 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7545\n",
            "2025-03-24 01:18:55,461 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7797\n",
            "2025-03-24 01:19:03,862 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7423\n",
            "2025-03-24 01:19:12,456 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6400\n",
            "2025-03-24 01:19:21,041 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7515\n",
            "2025-03-24 01:19:29,727 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7192\n",
            "2025-03-24 01:19:38,254 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8660\n",
            "2025-03-24 01:19:46,895 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7140\n",
            "2025-03-24 01:19:55,360 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7508\n",
            "2025-03-24 01:20:03,884 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6441\n",
            "2025-03-24 01:20:12,335 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6897\n",
            "2025-03-24 01:20:21,069 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6913\n",
            "2025-03-24 01:20:29,645 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6660\n",
            "2025-03-24 01:20:38,014 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7242\n",
            "2025-03-24 01:20:46,451 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7018\n",
            "2025-03-24 01:20:55,061 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6447\n",
            "2025-03-24 01:21:03,460 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6978\n",
            "2025-03-24 01:21:12,022 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7650\n",
            "2025-03-24 01:21:20,477 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6785\n",
            "2025-03-24 01:21:28,301 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7655\n",
            "2025-03-24 01:21:35,973 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6900\n",
            "2025-03-24 01:21:44,402 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7179\n",
            "2025-03-24 01:21:52,805 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6101\n",
            "2025-03-24 01:22:01,198 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6579\n",
            "2025-03-24 01:22:09,660 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6906\n",
            "2025-03-24 01:22:18,090 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7158\n",
            "2025-03-24 01:22:26,793 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6508\n",
            "2025-03-24 01:22:35,388 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6804\n",
            "2025-03-24 01:22:43,673 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7577\n",
            "2025-03-24 01:22:49,517 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5185\n",
            "2025-03-24 01:22:50,070 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1937\n",
            "2025-03-24 01:22:50,071 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:23:46,575 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.7133, Val Loss: 0.5130, Val Acc: 0.8055\n",
            "2025-03-24 01:23:46,925 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 01:23:46,925 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 01:23:52,772 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4938\n",
            "2025-03-24 01:24:00,160 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6256\n",
            "2025-03-24 01:24:07,879 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6697\n",
            "2025-03-24 01:24:15,582 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5850\n",
            "2025-03-24 01:24:23,364 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6071\n",
            "2025-03-24 01:24:31,106 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6844\n",
            "2025-03-24 01:24:38,946 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6049\n",
            "2025-03-24 01:24:46,759 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6312\n",
            "2025-03-24 01:24:54,437 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5902\n",
            "2025-03-24 01:25:02,063 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6064\n",
            "2025-03-24 01:25:09,734 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6176\n",
            "2025-03-24 01:25:17,350 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6109\n",
            "2025-03-24 01:25:25,161 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5771\n",
            "2025-03-24 01:25:32,825 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5821\n",
            "2025-03-24 01:25:40,542 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6435\n",
            "2025-03-24 01:25:48,273 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6906\n",
            "2025-03-24 01:25:56,137 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5705\n",
            "2025-03-24 01:26:03,934 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6607\n",
            "2025-03-24 01:26:11,732 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6123\n",
            "2025-03-24 01:26:19,457 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6484\n",
            "2025-03-24 01:26:27,052 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5985\n",
            "2025-03-24 01:26:34,717 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6429\n",
            "2025-03-24 01:26:42,315 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7289\n",
            "2025-03-24 01:26:50,058 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6229\n",
            "2025-03-24 01:26:57,657 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6334\n",
            "2025-03-24 01:27:05,404 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6660\n",
            "2025-03-24 01:27:13,129 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5338\n",
            "2025-03-24 01:27:20,572 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5839\n",
            "2025-03-24 01:27:28,383 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5846\n",
            "2025-03-24 01:27:36,012 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6345\n",
            "2025-03-24 01:27:43,948 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5908\n",
            "2025-03-24 01:27:51,471 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6551\n",
            "2025-03-24 01:27:58,843 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6649\n",
            "2025-03-24 01:28:04,453 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4634\n",
            "2025-03-24 01:28:05,028 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2124\n",
            "2025-03-24 01:28:05,028 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:29:00,981 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.6260, Val Loss: 0.4579, Val Acc: 0.8292\n",
            "2025-03-24 01:29:01,348 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 01:29:01,349 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 01:29:07,180 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4383\n",
            "2025-03-24 01:29:14,963 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5558\n",
            "2025-03-24 01:29:22,500 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5331\n",
            "2025-03-24 01:29:30,301 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5500\n",
            "2025-03-24 01:29:40,778 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5054\n",
            "2025-03-24 01:29:49,666 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5132\n",
            "2025-03-24 01:29:58,163 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6163\n",
            "2025-03-24 01:30:06,857 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5686\n",
            "2025-03-24 01:30:15,361 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5460\n",
            "2025-03-24 01:30:24,081 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5133\n",
            "2025-03-24 01:30:32,652 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4742\n",
            "2025-03-24 01:30:41,049 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5370\n",
            "2025-03-24 01:30:49,623 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5990\n",
            "2025-03-24 01:30:58,001 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5884\n",
            "2025-03-24 01:31:06,419 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5741\n",
            "2025-03-24 01:31:15,017 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6637\n",
            "2025-03-24 01:31:23,429 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5451\n",
            "2025-03-24 01:31:32,021 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5077\n",
            "2025-03-24 01:31:40,433 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5620\n",
            "2025-03-24 01:31:48,828 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5002\n",
            "2025-03-24 01:31:57,220 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5089\n",
            "2025-03-24 01:32:05,812 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5420\n",
            "2025-03-24 01:32:14,219 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6229\n",
            "2025-03-24 01:32:22,615 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6357\n",
            "2025-03-24 01:32:31,056 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6063\n",
            "2025-03-24 01:32:39,460 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6108\n",
            "2025-03-24 01:32:48,120 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6341\n",
            "2025-03-24 01:32:56,604 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5842\n",
            "2025-03-24 01:33:05,196 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5925\n",
            "2025-03-24 01:33:13,594 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5010\n",
            "2025-03-24 01:33:22,037 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5227\n",
            "2025-03-24 01:33:30,387 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5198\n",
            "2025-03-24 01:33:38,880 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5257\n",
            "2025-03-24 01:33:45,181 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4247\n",
            "2025-03-24 01:33:45,821 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1174\n",
            "2025-03-24 01:33:45,823 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:34:46,377 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5582, Val Loss: 0.4197, Val Acc: 0.8478\n",
            "2025-03-24 01:34:46,749 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 01:34:46,749 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 01:34:53,187 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4106\n",
            "2025-03-24 01:35:01,592 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4918\n",
            "2025-03-24 01:35:09,998 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5356\n",
            "2025-03-24 01:35:18,383 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5020\n",
            "2025-03-24 01:35:26,812 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4685\n",
            "2025-03-24 01:35:35,380 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5161\n",
            "2025-03-24 01:35:43,782 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5332\n",
            "2025-03-24 01:35:52,204 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4950\n",
            "2025-03-24 01:36:00,751 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5103\n",
            "2025-03-24 01:36:09,287 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5771\n",
            "2025-03-24 01:36:17,723 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5456\n",
            "2025-03-24 01:36:26,116 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5753\n",
            "2025-03-24 01:36:34,497 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5937\n",
            "2025-03-24 01:36:42,911 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5334\n",
            "2025-03-24 01:36:51,372 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5079\n",
            "2025-03-24 01:36:59,796 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5554\n",
            "2025-03-24 01:37:08,529 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4984\n",
            "2025-03-24 01:37:16,926 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4865\n",
            "2025-03-24 01:37:25,557 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5459\n",
            "2025-03-24 01:37:33,993 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5233\n",
            "2025-03-24 01:37:42,418 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5342\n",
            "2025-03-24 01:37:50,854 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4792\n",
            "2025-03-24 01:37:59,305 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5264\n",
            "2025-03-24 01:38:07,834 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5467\n",
            "2025-03-24 01:38:16,497 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5497\n",
            "2025-03-24 01:38:24,942 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4364\n",
            "2025-03-24 01:38:33,352 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5086\n",
            "2025-03-24 01:38:42,099 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5219\n",
            "2025-03-24 01:38:50,490 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4730\n",
            "2025-03-24 01:38:58,899 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5303\n",
            "2025-03-24 01:39:07,482 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4938\n",
            "2025-03-24 01:39:15,871 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4908\n",
            "2025-03-24 01:39:24,384 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5214\n",
            "2025-03-24 01:39:30,885 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3488\n",
            "2025-03-24 01:39:31,502 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1526\n",
            "2025-03-24 01:39:31,502 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:40:33,267 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.5191, Val Loss: 0.4070, Val Acc: 0.8576\n",
            "2025-03-24 01:40:33,651 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 01:40:33,652 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 01:40:40,097 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3396\n",
            "2025-03-24 01:40:48,754 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4998\n",
            "2025-03-24 01:40:57,239 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4578\n",
            "2025-03-24 01:41:05,703 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4389\n",
            "2025-03-24 01:41:14,306 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5335\n",
            "2025-03-24 01:41:22,704 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4752\n",
            "2025-03-24 01:41:31,245 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4300\n",
            "2025-03-24 01:41:39,806 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5629\n",
            "2025-03-24 01:41:48,422 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5143\n",
            "2025-03-24 01:41:56,821 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4937\n",
            "2025-03-24 01:42:05,237 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5000\n",
            "2025-03-24 01:42:13,826 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5225\n",
            "2025-03-24 01:42:22,231 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4626\n",
            "2025-03-24 01:42:30,829 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4410\n",
            "2025-03-24 01:42:39,332 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4665\n",
            "2025-03-24 01:42:47,897 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4913\n",
            "2025-03-24 01:42:56,449 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4804\n",
            "2025-03-24 01:43:04,900 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4346\n",
            "2025-03-24 01:43:13,242 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5128\n",
            "2025-03-24 01:43:21,697 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4601\n",
            "2025-03-24 01:43:30,283 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4609\n",
            "2025-03-24 01:43:38,883 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4643\n",
            "2025-03-24 01:43:47,182 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4465\n",
            "2025-03-24 01:43:55,636 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4880\n",
            "2025-03-24 01:44:03,948 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5253\n",
            "2025-03-24 01:44:12,398 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4708\n",
            "2025-03-24 01:44:20,755 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5162\n",
            "2025-03-24 01:44:29,194 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4566\n",
            "2025-03-24 01:44:37,590 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5638\n",
            "2025-03-24 01:44:45,976 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5014\n",
            "2025-03-24 01:44:54,581 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4909\n",
            "2025-03-24 01:45:03,172 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5171\n",
            "2025-03-24 01:45:11,561 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5034\n",
            "2025-03-24 01:45:17,991 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4144\n",
            "2025-03-24 01:45:18,634 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0864\n",
            "2025-03-24 01:45:18,636 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:46:19,153 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4866, Val Loss: 0.3989, Val Acc: 0.8520\n",
            "2025-03-24 01:46:19,529 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 01:46:19,530 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 01:46:25,778 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-24 01:46:34,117 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4127\n",
            "2025-03-24 01:46:42,600 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4663\n",
            "2025-03-24 01:46:50,980 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4647\n",
            "2025-03-24 01:46:59,395 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4470\n",
            "2025-03-24 01:47:07,903 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4783\n",
            "2025-03-24 01:47:16,461 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4252\n",
            "2025-03-24 01:47:24,937 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4520\n",
            "2025-03-24 01:47:33,339 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4288\n",
            "2025-03-24 01:47:41,937 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4228\n",
            "2025-03-24 01:47:50,569 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4165\n",
            "2025-03-24 01:47:59,123 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4447\n",
            "2025-03-24 01:48:07,723 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4652\n",
            "2025-03-24 01:48:16,340 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5311\n",
            "2025-03-24 01:48:24,880 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4017\n",
            "2025-03-24 01:48:33,322 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4140\n",
            "2025-03-24 01:48:41,904 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4152\n",
            "2025-03-24 01:48:50,494 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3736\n",
            "2025-03-24 01:48:58,825 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4270\n",
            "2025-03-24 01:49:07,260 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4734\n",
            "2025-03-24 01:49:15,689 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4329\n",
            "2025-03-24 01:49:24,061 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4890\n",
            "2025-03-24 01:49:32,659 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4786\n",
            "2025-03-24 01:49:41,031 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4373\n",
            "2025-03-24 01:49:49,688 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4868\n",
            "2025-03-24 01:49:58,096 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4234\n",
            "2025-03-24 01:50:06,490 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4570\n",
            "2025-03-24 01:50:15,066 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4739\n",
            "2025-03-24 01:50:23,534 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4227\n",
            "2025-03-24 01:50:34,671 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4897\n",
            "2025-03-24 01:50:43,085 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4193\n",
            "2025-03-24 01:50:51,660 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4568\n",
            "2025-03-24 01:51:00,062 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4674\n",
            "2025-03-24 01:51:06,257 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3248\n",
            "2025-03-24 01:51:06,869 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1304\n",
            "2025-03-24 01:51:06,870 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:52:07,639 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4456, Val Loss: 0.3990, Val Acc: 0.8516\n",
            "2025-03-24 01:52:07,640 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 01:52:14,237 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3808\n",
            "2025-03-24 01:52:22,800 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4189\n",
            "2025-03-24 01:52:31,215 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4109\n",
            "2025-03-24 01:52:39,601 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3899\n",
            "2025-03-24 01:52:47,997 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4490\n",
            "2025-03-24 01:52:56,474 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3969\n",
            "2025-03-24 01:53:05,083 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3833\n",
            "2025-03-24 01:53:13,466 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3972\n",
            "2025-03-24 01:53:21,810 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5017\n",
            "2025-03-24 01:53:30,228 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4029\n",
            "2025-03-24 01:53:38,624 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4874\n",
            "2025-03-24 01:53:47,204 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4721\n",
            "2025-03-24 01:53:55,713 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4094\n",
            "2025-03-24 01:54:04,084 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3914\n",
            "2025-03-24 01:54:12,673 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4682\n",
            "2025-03-24 01:54:21,070 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4646\n",
            "2025-03-24 01:54:29,634 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4195\n",
            "2025-03-24 01:54:38,414 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3989\n",
            "2025-03-24 01:54:46,996 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4305\n",
            "2025-03-24 01:54:55,596 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3945\n",
            "2025-03-24 01:55:03,997 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4057\n",
            "2025-03-24 01:55:12,397 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4005\n",
            "2025-03-24 01:55:20,800 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4505\n",
            "2025-03-24 01:55:29,297 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4485\n",
            "2025-03-24 01:55:37,713 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3811\n",
            "2025-03-24 01:55:46,235 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4155\n",
            "2025-03-24 01:55:54,631 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4404\n",
            "2025-03-24 01:56:03,207 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3952\n",
            "2025-03-24 01:56:11,659 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4437\n",
            "2025-03-24 01:56:20,180 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4035\n",
            "2025-03-24 01:56:28,576 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5284\n",
            "2025-03-24 01:56:37,103 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4475\n",
            "2025-03-24 01:56:45,540 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4309\n",
            "2025-03-24 01:56:51,735 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3090\n",
            "2025-03-24 01:56:52,399 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1228\n",
            "2025-03-24 01:56:52,400 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 01:57:53,147 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4294, Val Loss: 0.3816, Val Acc: 0.8655\n",
            "2025-03-24 01:57:53,520 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 01:57:53,521 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 01:57:59,823 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3223\n",
            "2025-03-24 01:58:08,157 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4099\n",
            "2025-03-24 01:58:16,624 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4283\n",
            "2025-03-24 01:58:25,293 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3462\n",
            "2025-03-24 01:58:33,933 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4116\n",
            "2025-03-24 01:58:42,332 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4189\n",
            "2025-03-24 01:58:50,730 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3995\n",
            "2025-03-24 01:58:59,312 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4166\n",
            "2025-03-24 01:59:07,792 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3785\n",
            "2025-03-24 01:59:16,388 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3755\n",
            "2025-03-24 01:59:24,993 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4273\n",
            "2025-03-24 01:59:33,603 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3779\n",
            "2025-03-24 01:59:42,132 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3994\n",
            "2025-03-24 01:59:50,706 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4045\n",
            "2025-03-24 01:59:59,109 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3635\n",
            "2025-03-24 02:00:07,611 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4031\n",
            "2025-03-24 02:00:16,019 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4224\n",
            "2025-03-24 02:00:24,356 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3916\n",
            "2025-03-24 02:00:32,993 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4058\n",
            "2025-03-24 02:00:41,697 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5012\n",
            "2025-03-24 02:00:50,095 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3241\n",
            "2025-03-24 02:00:58,617 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4173\n",
            "2025-03-24 02:01:07,074 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4885\n",
            "2025-03-24 02:01:15,316 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4106\n",
            "2025-03-24 02:01:24,195 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3430\n",
            "2025-03-24 02:01:32,870 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3644\n",
            "2025-03-24 02:01:41,266 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3995\n",
            "2025-03-24 02:01:49,680 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4320\n",
            "2025-03-24 02:01:58,274 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4173\n",
            "2025-03-24 02:02:06,674 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4460\n",
            "2025-03-24 02:02:15,015 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4877\n",
            "2025-03-24 02:02:23,469 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3712\n",
            "2025-03-24 02:02:31,859 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3628\n",
            "2025-03-24 02:02:38,098 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-24 02:02:38,745 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1264\n",
            "2025-03-24 02:02:38,746 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:03:39,794 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.4048, Val Loss: 0.3947, Val Acc: 0.8646\n",
            "2025-03-24 02:03:39,795 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 02:03:46,315 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2833\n",
            "2025-03-24 02:03:54,836 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3497\n",
            "2025-03-24 02:04:03,226 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4061\n",
            "2025-03-24 02:04:11,789 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3372\n",
            "2025-03-24 02:04:20,281 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3327\n",
            "2025-03-24 02:04:28,861 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3739\n",
            "2025-03-24 02:04:37,398 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3676\n",
            "2025-03-24 02:04:45,826 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3645\n",
            "2025-03-24 02:04:54,299 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3873\n",
            "2025-03-24 02:05:02,608 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4509\n",
            "2025-03-24 02:05:11,154 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3671\n",
            "2025-03-24 02:05:20,007 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3849\n",
            "2025-03-24 02:05:28,597 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3605\n",
            "2025-03-24 02:05:37,006 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3507\n",
            "2025-03-24 02:05:45,396 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3560\n",
            "2025-03-24 02:05:53,802 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3563\n",
            "2025-03-24 02:06:02,197 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3867\n",
            "2025-03-24 02:06:10,595 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3454\n",
            "2025-03-24 02:06:19,189 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3775\n",
            "2025-03-24 02:06:27,708 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3901\n",
            "2025-03-24 02:06:36,224 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3708\n",
            "2025-03-24 02:06:44,988 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3752\n",
            "2025-03-24 02:06:53,386 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3971\n",
            "2025-03-24 02:07:01,779 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3941\n",
            "2025-03-24 02:07:10,372 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3504\n",
            "2025-03-24 02:07:18,772 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3624\n",
            "2025-03-24 02:07:27,353 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4073\n",
            "2025-03-24 02:07:36,079 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3439\n",
            "2025-03-24 02:07:44,564 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3804\n",
            "2025-03-24 02:07:52,954 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3313\n",
            "2025-03-24 02:08:01,360 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4151\n",
            "2025-03-24 02:08:09,753 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3315\n",
            "2025-03-24 02:08:18,158 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3643\n",
            "2025-03-24 02:08:24,427 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2904\n",
            "2025-03-24 02:08:25,061 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1489\n",
            "2025-03-24 02:08:25,062 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:09:25,768 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3731, Val Loss: 0.4401, Val Acc: 0.8534\n",
            "2025-03-24 02:09:25,768 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 02:09:32,153 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2306\n",
            "2025-03-24 02:09:40,821 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3386\n",
            "2025-03-24 02:09:49,517 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3354\n",
            "2025-03-24 02:09:57,925 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3654\n",
            "2025-03-24 02:10:06,311 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3405\n",
            "2025-03-24 02:10:14,714 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3839\n",
            "2025-03-24 02:10:23,174 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3674\n",
            "2025-03-24 02:10:31,536 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3259\n",
            "2025-03-24 02:10:39,992 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3261\n",
            "2025-03-24 02:10:48,624 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3464\n",
            "2025-03-24 02:10:57,296 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-24 02:11:05,479 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3396\n",
            "2025-03-24 02:11:13,894 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3830\n",
            "2025-03-24 02:11:22,387 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3264\n",
            "2025-03-24 02:11:31,165 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3513\n",
            "2025-03-24 02:11:39,696 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-24 02:11:48,176 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3297\n",
            "2025-03-24 02:11:56,532 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3211\n",
            "2025-03-24 02:12:05,265 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3723\n",
            "2025-03-24 02:12:13,870 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3864\n",
            "2025-03-24 02:12:22,390 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4096\n",
            "2025-03-24 02:12:30,874 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3309\n",
            "2025-03-24 02:12:39,357 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3954\n",
            "2025-03-24 02:12:47,909 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3472\n",
            "2025-03-24 02:12:56,471 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3481\n",
            "2025-03-24 02:13:05,045 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3642\n",
            "2025-03-24 02:13:13,450 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3776\n",
            "2025-03-24 02:13:22,003 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3678\n",
            "2025-03-24 02:13:30,645 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3225\n",
            "2025-03-24 02:13:39,259 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3795\n",
            "2025-03-24 02:13:47,789 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3650\n",
            "2025-03-24 02:13:56,296 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3402\n",
            "2025-03-24 02:14:04,596 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3420\n",
            "2025-03-24 02:14:10,929 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2749\n",
            "2025-03-24 02:14:11,554 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0804\n",
            "2025-03-24 02:14:11,555 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:15:12,705 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3505, Val Loss: 0.4192, Val Acc: 0.8590\n",
            "2025-03-24 02:15:12,705 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 02:15:19,245 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2824\n",
            "2025-03-24 02:15:27,730 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3946\n",
            "2025-03-24 02:15:36,193 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3405\n",
            "2025-03-24 02:15:44,573 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3568\n",
            "2025-03-24 02:15:52,847 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3287\n",
            "2025-03-24 02:16:01,483 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3363\n",
            "2025-03-24 02:16:10,220 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-24 02:16:18,790 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3664\n",
            "2025-03-24 02:16:27,175 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3089\n",
            "2025-03-24 02:16:35,531 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3020\n",
            "2025-03-24 02:16:43,877 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3593\n",
            "2025-03-24 02:16:52,455 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3429\n",
            "2025-03-24 02:17:01,199 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3365\n",
            "2025-03-24 02:17:09,603 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4030\n",
            "2025-03-24 02:17:17,994 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3636\n",
            "2025-03-24 02:17:26,394 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2814\n",
            "2025-03-24 02:17:34,951 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3668\n",
            "2025-03-24 02:17:43,365 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3583\n",
            "2025-03-24 02:17:51,865 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3213\n",
            "2025-03-24 02:18:00,573 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3532\n",
            "2025-03-24 02:18:08,983 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3420\n",
            "2025-03-24 02:18:17,565 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3516\n",
            "2025-03-24 02:18:25,947 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3779\n",
            "2025-03-24 02:18:34,396 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3656\n",
            "2025-03-24 02:18:42,823 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3499\n",
            "2025-03-24 02:18:51,586 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3502\n",
            "2025-03-24 02:19:00,082 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3112\n",
            "2025-03-24 02:19:08,704 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2995\n",
            "2025-03-24 02:19:17,355 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3489\n",
            "2025-03-24 02:19:25,980 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3706\n",
            "2025-03-24 02:19:34,549 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3311\n",
            "2025-03-24 02:19:42,893 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3135\n",
            "2025-03-24 02:19:51,347 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3589\n",
            "2025-03-24 02:19:57,939 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2458\n",
            "2025-03-24 02:19:58,601 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0912\n",
            "2025-03-24 02:19:58,601 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:20:59,324 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3450, Val Loss: 0.4120, Val Acc: 0.8623\n",
            "2025-03-24 02:20:59,325 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 02:21:05,720 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2506\n",
            "2025-03-24 02:21:14,121 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-24 02:21:22,516 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3151\n",
            "2025-03-24 02:21:30,916 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-24 02:21:39,557 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3263\n",
            "2025-03-24 02:21:47,941 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3086\n",
            "2025-03-24 02:21:56,363 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3436\n",
            "2025-03-24 02:22:04,826 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3135\n",
            "2025-03-24 02:22:13,327 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3492\n",
            "2025-03-24 02:22:21,929 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3277\n",
            "2025-03-24 02:22:30,294 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2930\n",
            "2025-03-24 02:22:38,732 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3170\n",
            "2025-03-24 02:22:47,096 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2939\n",
            "2025-03-24 02:22:55,692 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2816\n",
            "2025-03-24 02:23:04,113 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3080\n",
            "2025-03-24 02:23:12,781 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2795\n",
            "2025-03-24 02:23:21,249 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2874\n",
            "2025-03-24 02:23:29,674 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2997\n",
            "2025-03-24 02:23:38,264 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3266\n",
            "2025-03-24 02:23:46,674 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3490\n",
            "2025-03-24 02:23:55,250 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3432\n",
            "2025-03-24 02:24:03,629 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2845\n",
            "2025-03-24 02:24:12,045 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3439\n",
            "2025-03-24 02:24:20,542 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3074\n",
            "2025-03-24 02:24:29,056 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3124\n",
            "2025-03-24 02:24:37,651 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-24 02:24:46,051 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-24 02:24:54,641 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3101\n",
            "2025-03-24 02:25:02,996 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2885\n",
            "2025-03-24 02:25:11,438 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2868\n",
            "2025-03-24 02:25:19,833 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3226\n",
            "2025-03-24 02:25:28,339 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-24 02:25:36,830 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3201\n",
            "2025-03-24 02:25:43,085 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2389\n",
            "2025-03-24 02:25:43,707 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0877\n",
            "2025-03-24 02:25:43,708 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:26:44,179 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3113, Val Loss: 0.3911, Val Acc: 0.8702\n",
            "2025-03-24 02:26:44,180 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 02:26:50,589 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2358\n",
            "2025-03-24 02:26:59,204 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3216\n",
            "2025-03-24 02:27:07,607 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2773\n",
            "2025-03-24 02:27:16,004 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2769\n",
            "2025-03-24 02:27:24,404 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2853\n",
            "2025-03-24 02:27:32,975 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2922\n",
            "2025-03-24 02:27:41,345 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2783\n",
            "2025-03-24 02:27:49,773 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2888\n",
            "2025-03-24 02:27:58,309 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2791\n",
            "2025-03-24 02:28:06,974 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3001\n",
            "2025-03-24 02:28:15,547 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2802\n",
            "2025-03-24 02:28:23,955 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3013\n",
            "2025-03-24 02:28:32,367 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2533\n",
            "2025-03-24 02:28:40,756 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2810\n",
            "2025-03-24 02:28:49,045 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-24 02:28:57,460 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3050\n",
            "2025-03-24 02:29:06,003 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2776\n",
            "2025-03-24 02:29:14,402 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2847\n",
            "2025-03-24 02:29:22,899 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2853\n",
            "2025-03-24 02:29:31,554 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2972\n",
            "2025-03-24 02:29:40,233 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2802\n",
            "2025-03-24 02:29:48,961 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-24 02:29:57,360 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2985\n",
            "2025-03-24 02:30:05,988 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-24 02:30:14,646 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2952\n",
            "2025-03-24 02:30:23,334 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2857\n",
            "2025-03-24 02:30:31,680 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3120\n",
            "2025-03-24 02:30:40,577 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3035\n",
            "2025-03-24 02:30:49,342 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-24 02:30:57,743 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3224\n",
            "2025-03-24 02:31:06,240 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3061\n",
            "2025-03-24 02:31:14,931 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3027\n",
            "2025-03-24 02:31:23,416 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-24 02:31:29,874 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2346\n",
            "2025-03-24 02:31:30,483 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0716\n",
            "2025-03-24 02:31:30,484 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:32:31,326 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2903, Val Loss: 0.3942, Val Acc: 0.8692\n",
            "2025-03-24 02:32:31,326 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 02:32:37,675 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2090\n",
            "2025-03-24 02:32:46,258 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-24 02:32:54,686 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3001\n",
            "2025-03-24 02:33:03,094 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2858\n",
            "2025-03-24 02:33:11,505 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2792\n",
            "2025-03-24 02:33:19,952 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2817\n",
            "2025-03-24 02:33:28,334 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2923\n",
            "2025-03-24 02:33:37,076 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2703\n",
            "2025-03-24 02:33:45,483 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2723\n",
            "2025-03-24 02:33:54,075 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2714\n",
            "2025-03-24 02:34:02,536 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2538\n",
            "2025-03-24 02:34:11,077 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2856\n",
            "2025-03-24 02:34:19,481 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2558\n",
            "2025-03-24 02:34:27,871 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2568\n",
            "2025-03-24 02:34:36,461 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-24 02:34:44,860 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2850\n",
            "2025-03-24 02:34:53,272 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-24 02:35:01,866 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2696\n",
            "2025-03-24 02:35:10,261 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2704\n",
            "2025-03-24 02:35:18,659 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3045\n",
            "2025-03-24 02:35:27,057 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2924\n",
            "2025-03-24 02:35:35,449 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2777\n",
            "2025-03-24 02:35:43,839 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-24 02:35:52,236 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-24 02:36:00,640 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2780\n",
            "2025-03-24 02:36:09,036 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3059\n",
            "2025-03-24 02:36:17,632 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2644\n",
            "2025-03-24 02:36:26,218 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3369\n",
            "2025-03-24 02:36:34,610 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2911\n",
            "2025-03-24 02:36:43,006 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-24 02:36:51,407 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2799\n",
            "2025-03-24 02:36:59,802 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-24 02:37:08,206 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3122\n",
            "2025-03-24 02:37:14,472 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2254\n",
            "2025-03-24 02:37:15,112 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1451\n",
            "2025-03-24 02:37:15,113 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:38:15,733 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2844, Val Loss: 0.3891, Val Acc: 0.8683\n",
            "2025-03-24 02:38:15,734 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 02:38:22,024 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2148\n",
            "2025-03-24 02:38:30,645 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-24 02:38:38,959 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-24 02:38:47,581 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3001\n",
            "2025-03-24 02:38:55,986 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3052\n",
            "2025-03-24 02:39:04,399 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2701\n",
            "2025-03-24 02:39:12,791 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-24 02:39:21,184 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-24 02:39:29,767 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2841\n",
            "2025-03-24 02:39:38,183 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2872\n",
            "2025-03-24 02:39:46,564 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3059\n",
            "2025-03-24 02:39:55,023 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2621\n",
            "2025-03-24 02:40:03,360 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-24 02:40:11,793 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2657\n",
            "2025-03-24 02:40:20,446 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-24 02:40:28,965 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2652\n",
            "2025-03-24 02:40:37,359 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2750\n",
            "2025-03-24 02:40:45,756 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2595\n",
            "2025-03-24 02:40:54,313 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2594\n",
            "2025-03-24 02:41:02,732 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-24 02:41:11,103 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2518\n",
            "2025-03-24 02:41:19,528 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2845\n",
            "2025-03-24 02:41:28,014 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3016\n",
            "2025-03-24 02:41:36,537 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2559\n",
            "2025-03-24 02:41:44,937 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-24 02:41:53,389 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2487\n",
            "2025-03-24 02:42:01,752 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2590\n",
            "2025-03-24 02:42:10,178 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-24 02:42:18,641 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2734\n",
            "2025-03-24 02:42:27,290 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2983\n",
            "2025-03-24 02:42:35,797 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2794\n",
            "2025-03-24 02:42:44,158 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2568\n",
            "2025-03-24 02:42:52,545 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-24 02:42:58,934 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2451\n",
            "2025-03-24 02:42:59,592 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0803\n",
            "2025-03-24 02:42:59,592 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:43:59,942 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2763, Val Loss: 0.3946, Val Acc: 0.8734\n",
            "2025-03-24 02:43:59,942 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-24 02:44:06,483 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1960\n",
            "2025-03-24 02:44:14,894 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2588\n",
            "2025-03-24 02:44:23,275 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2760\n",
            "2025-03-24 02:44:31,693 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2426\n",
            "2025-03-24 02:44:40,089 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2748\n",
            "2025-03-24 02:44:48,484 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2500\n",
            "2025-03-24 02:44:56,880 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2610\n",
            "2025-03-24 02:45:05,279 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2660\n",
            "2025-03-24 02:45:13,672 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2933\n",
            "2025-03-24 02:45:22,083 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2733\n",
            "2025-03-24 02:45:30,554 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-24 02:45:38,961 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2744\n",
            "2025-03-24 02:45:47,658 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2836\n",
            "2025-03-24 02:45:56,063 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2470\n",
            "2025-03-24 02:46:04,528 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2985\n",
            "2025-03-24 02:46:12,944 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2633\n",
            "2025-03-24 02:46:21,661 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2706\n",
            "2025-03-24 02:46:30,232 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-24 02:46:38,541 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-24 02:46:46,878 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2563\n",
            "2025-03-24 02:46:55,199 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2455\n",
            "2025-03-24 02:47:03,743 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-24 02:47:12,186 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2500\n",
            "2025-03-24 02:47:20,561 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2593\n",
            "2025-03-24 02:47:28,989 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2433\n",
            "2025-03-24 02:47:37,406 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2662\n",
            "2025-03-24 02:47:45,792 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2824\n",
            "2025-03-24 02:47:54,212 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2714\n",
            "2025-03-24 02:48:02,602 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2785\n",
            "2025-03-24 02:48:10,952 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2686\n",
            "2025-03-24 02:48:19,574 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-24 02:48:28,690 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-24 02:48:37,182 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2587\n",
            "2025-03-24 02:48:43,511 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1930\n",
            "2025-03-24 02:48:44,122 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0645\n",
            "2025-03-24 02:48:44,122 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:49:44,610 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2660, Val Loss: 0.3999, Val Acc: 0.8683\n",
            "2025-03-24 02:49:44,611 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-24 02:49:51,177 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1817\n",
            "2025-03-24 02:49:59,633 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-24 02:50:08,149 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2780\n",
            "2025-03-24 02:50:16,577 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2735\n",
            "2025-03-24 02:50:24,858 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-24 02:50:33,184 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-24 02:50:41,649 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2744\n",
            "2025-03-24 02:50:50,108 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2436\n",
            "2025-03-24 02:50:58,541 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2508\n",
            "2025-03-24 02:51:06,987 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2580\n",
            "2025-03-24 02:51:15,563 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2649\n",
            "2025-03-24 02:51:24,141 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2823\n",
            "2025-03-24 02:51:32,536 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2635\n",
            "2025-03-24 02:51:40,899 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2659\n",
            "2025-03-24 02:51:49,511 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-24 02:51:57,928 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2639\n",
            "2025-03-24 02:52:06,346 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2477\n",
            "2025-03-24 02:52:14,815 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-24 02:52:23,147 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2507\n",
            "2025-03-24 02:52:31,542 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2665\n",
            "2025-03-24 02:52:40,143 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2464\n",
            "2025-03-24 02:52:48,543 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2650\n",
            "2025-03-24 02:52:57,119 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2579\n",
            "2025-03-24 02:53:05,513 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2953\n",
            "2025-03-24 02:53:13,930 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-24 02:53:22,304 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2346\n",
            "2025-03-24 02:53:30,749 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2516\n",
            "2025-03-24 02:53:39,185 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2460\n",
            "2025-03-24 02:53:47,579 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2484\n",
            "2025-03-24 02:53:56,143 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2792\n",
            "2025-03-24 02:54:04,715 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2945\n",
            "2025-03-24 02:54:13,268 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-24 02:54:21,689 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2860\n",
            "2025-03-24 02:54:28,137 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1920\n",
            "2025-03-24 02:54:28,749 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1157\n",
            "2025-03-24 02:54:28,749 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 02:55:29,263 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2657, Val Loss: 0.3914, Val Acc: 0.8776\n",
            "2025-03-24 02:55:29,264 - INFO - [TRAIN INFO] Early stopping at epoch 19 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 02:55:29,265 - INFO - [TRAIN INFO] Total Time: 6555.07s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▁▂▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇█████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇█████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇█████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇█████▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▆▆▅▅▄▄▄▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇▇██▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.26574</td></tr><tr><td>train_val_loss_diff</td><td>-0.1257</td></tr><tr><td>val_accuracy</td><td>0.87762</td></tr><tr><td>val_loss</td><td>0.39145</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_transformer_fusion_fold_2</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ew7pjung' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ew7pjung</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_010613-ew7pjung\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 02:55:31,415 - INFO - [TRAIN INFO] Fold 2 Training Complete at epoch 19. Total Time: 6557.22s\n",
            "2025-03-24 02:55:31,425 - INFO - [K-FOLD INFO] Fold 2 completed in 6560.46 seconds\n",
            "2025-03-24 02:55:31,426 - INFO - [K-FOLD INFO] ============================== Fold 3/5 ==============================\n",
            "2025-03-24 02:55:31,430 - INFO - [K-FOLD INFO] Fold 3:\n",
            "2025-03-24 02:55:31,431 - INFO -    Train Samples: 8594\n",
            "2025-03-24 02:55:31,431 - INFO -    Validation Samples: 2149\n",
            "2025-03-24 02:55:31,432 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 3\n",
            "2025-03-24 02:55:31,434 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 3:\n",
            "2025-03-24 02:55:31,434 - INFO -    Train batches: 135, Validation batches: 34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arkzs\\miniforge3\\envs\\enel645_torch_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 02:55:32,073 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 3\n",
            "2025-03-24 02:55:32,076 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 3:\n",
            "2025-03-24 02:55:32,076 - INFO - [K-FOLD INFO] Loss function initialized for Fold 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_transformer_fusion\\wandb\\run-20250324_025532-ffettvts</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ffettvts' target=\"_blank\">experiment_multimodal_transformer_fusion_fold_3</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ffettvts' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ffettvts</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 02:55:32,899 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 02:55:32,900 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 02:55:39,674 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.1368\n",
            "2025-03-24 02:55:48,086 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4705\n",
            "2025-03-24 02:55:56,482 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.4014\n",
            "2025-03-24 02:56:04,822 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.4238\n",
            "2025-03-24 02:56:13,238 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3335\n",
            "2025-03-24 02:56:21,486 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3071\n",
            "2025-03-24 02:56:30,080 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2888\n",
            "2025-03-24 02:56:38,563 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2620\n",
            "2025-03-24 02:56:46,859 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2590\n",
            "2025-03-24 02:56:55,263 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2575\n",
            "2025-03-24 02:57:03,692 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2072\n",
            "2025-03-24 02:57:12,222 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.2043\n",
            "2025-03-24 02:57:20,855 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.2494\n",
            "2025-03-24 02:57:29,251 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1645\n",
            "2025-03-24 02:57:37,650 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.2144\n",
            "2025-03-24 02:57:46,048 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1607\n",
            "2025-03-24 02:57:54,539 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.2129\n",
            "2025-03-24 02:58:02,872 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.1453\n",
            "2025-03-24 02:58:11,417 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.1337\n",
            "2025-03-24 02:58:19,723 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.1519\n",
            "2025-03-24 02:58:28,094 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.1733\n",
            "2025-03-24 02:58:36,482 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.1377\n",
            "2025-03-24 02:58:44,831 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0698\n",
            "2025-03-24 02:58:53,227 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0912\n",
            "2025-03-24 02:59:01,625 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 1.0750\n",
            "2025-03-24 02:59:10,009 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0202\n",
            "2025-03-24 02:59:18,604 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 1.1267\n",
            "2025-03-24 02:59:27,013 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 1.0414\n",
            "2025-03-24 02:59:35,467 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9870\n",
            "2025-03-24 02:59:43,893 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 1.0209\n",
            "2025-03-24 02:59:52,381 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9658\n",
            "2025-03-24 03:00:01,002 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9700\n",
            "2025-03-24 03:00:09,402 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9728\n",
            "2025-03-24 03:00:15,679 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7194\n",
            "2025-03-24 03:00:16,309 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2910\n",
            "2025-03-24 03:00:16,311 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:01:17,464 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1747, Val Loss: 0.8377, Val Acc: 0.6747\n",
            "2025-03-24 03:01:17,804 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 03:01:17,805 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 03:01:24,381 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.7302\n",
            "2025-03-24 03:01:32,967 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9623\n",
            "2025-03-24 03:01:41,844 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.9024\n",
            "2025-03-24 03:01:50,495 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9290\n",
            "2025-03-24 03:01:59,171 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8569\n",
            "2025-03-24 03:02:07,924 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.9063\n",
            "2025-03-24 03:02:16,362 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8749\n",
            "2025-03-24 03:02:25,160 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.9516\n",
            "2025-03-24 03:02:33,950 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8185\n",
            "2025-03-24 03:02:42,700 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.9732\n",
            "2025-03-24 03:02:51,344 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.9002\n",
            "2025-03-24 03:03:00,057 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8537\n",
            "2025-03-24 03:03:08,744 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8350\n",
            "2025-03-24 03:03:17,503 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8430\n",
            "2025-03-24 03:03:26,337 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.9040\n",
            "2025-03-24 03:03:34,947 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8874\n",
            "2025-03-24 03:03:43,729 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8016\n",
            "2025-03-24 03:03:52,531 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7471\n",
            "2025-03-24 03:04:01,122 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.8385\n",
            "2025-03-24 03:04:09,659 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8352\n",
            "2025-03-24 03:04:18,513 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.8498\n",
            "2025-03-24 03:04:27,210 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7696\n",
            "2025-03-24 03:04:35,906 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.8212\n",
            "2025-03-24 03:04:44,638 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8109\n",
            "2025-03-24 03:04:53,430 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.8047\n",
            "2025-03-24 03:05:02,110 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.8217\n",
            "2025-03-24 03:05:10,866 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9252\n",
            "2025-03-24 03:05:19,707 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7659\n",
            "2025-03-24 03:05:28,302 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7997\n",
            "2025-03-24 03:05:37,100 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.8248\n",
            "2025-03-24 03:05:45,541 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7868\n",
            "2025-03-24 03:05:54,181 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8513\n",
            "2025-03-24 03:06:02,911 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.8217\n",
            "2025-03-24 03:06:09,494 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5588\n",
            "2025-03-24 03:06:10,097 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2665\n",
            "2025-03-24 03:06:10,099 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:07:13,069 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8542, Val Loss: 0.6267, Val Acc: 0.7571\n",
            "2025-03-24 03:07:13,449 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 03:07:13,449 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 03:07:19,844 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5244\n",
            "2025-03-24 03:07:28,458 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.7954\n",
            "2025-03-24 03:07:36,863 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6939\n",
            "2025-03-24 03:07:45,266 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.7299\n",
            "2025-03-24 03:07:53,797 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7017\n",
            "2025-03-24 03:08:02,235 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7127\n",
            "2025-03-24 03:08:10,633 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7241\n",
            "2025-03-24 03:08:19,208 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7178\n",
            "2025-03-24 03:08:27,644 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8054\n",
            "2025-03-24 03:08:36,041 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7054\n",
            "2025-03-24 03:08:44,417 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6993\n",
            "2025-03-24 03:08:52,843 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7111\n",
            "2025-03-24 03:09:01,136 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6912\n",
            "2025-03-24 03:09:09,626 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7607\n",
            "2025-03-24 03:09:18,001 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7824\n",
            "2025-03-24 03:09:26,435 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6368\n",
            "2025-03-24 03:09:34,949 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7038\n",
            "2025-03-24 03:09:43,624 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6986\n",
            "2025-03-24 03:09:52,325 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6225\n",
            "2025-03-24 03:10:01,014 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6839\n",
            "2025-03-24 03:10:09,477 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7403\n",
            "2025-03-24 03:10:18,123 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7593\n",
            "2025-03-24 03:10:26,931 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6868\n",
            "2025-03-24 03:10:35,410 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7389\n",
            "2025-03-24 03:10:44,206 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6425\n",
            "2025-03-24 03:10:52,931 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6780\n",
            "2025-03-24 03:11:01,725 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7432\n",
            "2025-03-24 03:11:10,464 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7016\n",
            "2025-03-24 03:11:19,313 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6542\n",
            "2025-03-24 03:11:27,976 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6769\n",
            "2025-03-24 03:11:36,789 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6967\n",
            "2025-03-24 03:11:45,432 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5954\n",
            "2025-03-24 03:11:53,798 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6469\n",
            "2025-03-24 03:12:00,184 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5167\n",
            "2025-03-24 03:12:00,791 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2062\n",
            "2025-03-24 03:12:00,793 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:13:01,866 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.7047, Val Loss: 0.5030, Val Acc: 0.8092\n",
            "2025-03-24 03:13:02,251 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 03:13:02,251 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 03:13:08,670 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4453\n",
            "2025-03-24 03:13:17,166 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6803\n",
            "2025-03-24 03:13:25,579 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5922\n",
            "2025-03-24 03:13:34,030 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6039\n",
            "2025-03-24 03:13:42,538 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6318\n",
            "2025-03-24 03:13:50,952 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5976\n",
            "2025-03-24 03:13:59,340 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6134\n",
            "2025-03-24 03:14:07,747 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6276\n",
            "2025-03-24 03:14:16,324 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6558\n",
            "2025-03-24 03:14:24,765 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6053\n",
            "2025-03-24 03:14:33,136 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6019\n",
            "2025-03-24 03:14:41,537 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5860\n",
            "2025-03-24 03:14:50,109 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6042\n",
            "2025-03-24 03:14:58,511 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6645\n",
            "2025-03-24 03:15:06,828 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5593\n",
            "2025-03-24 03:15:15,380 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5987\n",
            "2025-03-24 03:15:23,779 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6466\n",
            "2025-03-24 03:15:32,069 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6018\n",
            "2025-03-24 03:15:40,540 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5365\n",
            "2025-03-24 03:15:48,985 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5218\n",
            "2025-03-24 03:15:57,405 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6781\n",
            "2025-03-24 03:16:05,909 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5962\n",
            "2025-03-24 03:16:14,699 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6610\n",
            "2025-03-24 03:16:23,380 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6305\n",
            "2025-03-24 03:16:32,073 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5696\n",
            "2025-03-24 03:16:40,675 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6436\n",
            "2025-03-24 03:16:49,298 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6644\n",
            "2025-03-24 03:16:57,696 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5521\n",
            "2025-03-24 03:17:06,352 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5917\n",
            "2025-03-24 03:17:14,926 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5824\n",
            "2025-03-24 03:17:23,442 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5519\n",
            "2025-03-24 03:17:32,267 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6186\n",
            "2025-03-24 03:17:40,768 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6018\n",
            "2025-03-24 03:17:47,364 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4570\n",
            "2025-03-24 03:17:48,027 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1401\n",
            "2025-03-24 03:17:48,028 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:18:49,016 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.6078, Val Loss: 0.4552, Val Acc: 0.8362\n",
            "2025-03-24 03:18:49,391 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 03:18:49,391 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 03:18:55,911 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3905\n",
            "2025-03-24 03:19:04,323 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5160\n",
            "2025-03-24 03:19:13,048 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5251\n",
            "2025-03-24 03:19:21,440 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5430\n",
            "2025-03-24 03:19:29,844 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6426\n",
            "2025-03-24 03:19:38,291 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6555\n",
            "2025-03-24 03:19:47,013 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5122\n",
            "2025-03-24 03:19:55,598 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5156\n",
            "2025-03-24 03:20:03,911 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6090\n",
            "2025-03-24 03:20:12,638 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5842\n",
            "2025-03-24 03:20:21,032 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5865\n",
            "2025-03-24 03:20:29,428 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4910\n",
            "2025-03-24 03:20:38,015 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5675\n",
            "2025-03-24 03:20:46,408 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5783\n",
            "2025-03-24 03:20:54,823 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5126\n",
            "2025-03-24 03:21:03,407 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5567\n",
            "2025-03-24 03:21:12,074 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5938\n",
            "2025-03-24 03:21:20,945 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5386\n",
            "2025-03-24 03:21:29,802 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6211\n",
            "2025-03-24 03:21:38,531 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6038\n",
            "2025-03-24 03:21:47,388 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6068\n",
            "2025-03-24 03:21:56,148 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5976\n",
            "2025-03-24 03:22:04,804 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5166\n",
            "2025-03-24 03:22:13,197 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4849\n",
            "2025-03-24 03:22:21,591 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5751\n",
            "2025-03-24 03:22:30,183 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5222\n",
            "2025-03-24 03:22:38,634 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4555\n",
            "2025-03-24 03:22:47,369 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5773\n",
            "2025-03-24 03:22:55,776 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6005\n",
            "2025-03-24 03:23:04,174 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5547\n",
            "2025-03-24 03:23:12,590 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5328\n",
            "2025-03-24 03:23:20,981 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5294\n",
            "2025-03-24 03:23:29,373 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4757\n",
            "2025-03-24 03:23:35,515 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3676\n",
            "2025-03-24 03:23:36,132 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1004\n",
            "2025-03-24 03:23:36,133 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:24:37,058 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5523, Val Loss: 0.4362, Val Acc: 0.8334\n",
            "2025-03-24 03:24:37,444 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 03:24:37,445 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 03:24:43,936 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4699\n",
            "2025-03-24 03:24:52,347 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5566\n",
            "2025-03-24 03:25:00,746 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5755\n",
            "2025-03-24 03:25:09,138 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5182\n",
            "2025-03-24 03:25:17,528 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4547\n",
            "2025-03-24 03:25:26,032 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5200\n",
            "2025-03-24 03:25:34,737 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4238\n",
            "2025-03-24 03:25:43,329 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4972\n",
            "2025-03-24 03:25:52,122 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4908\n",
            "2025-03-24 03:26:00,523 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5219\n",
            "2025-03-24 03:26:09,063 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4657\n",
            "2025-03-24 03:26:17,476 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4565\n",
            "2025-03-24 03:26:25,880 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4990\n",
            "2025-03-24 03:26:34,274 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4648\n",
            "2025-03-24 03:26:42,645 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5332\n",
            "2025-03-24 03:26:51,036 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5041\n",
            "2025-03-24 03:26:59,481 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4768\n",
            "2025-03-24 03:27:07,889 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5349\n",
            "2025-03-24 03:27:16,258 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5185\n",
            "2025-03-24 03:27:24,673 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5135\n",
            "2025-03-24 03:27:33,077 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4596\n",
            "2025-03-24 03:27:41,470 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5385\n",
            "2025-03-24 03:27:49,764 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4996\n",
            "2025-03-24 03:27:58,145 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5878\n",
            "2025-03-24 03:28:06,528 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5654\n",
            "2025-03-24 03:28:15,082 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5608\n",
            "2025-03-24 03:28:23,577 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5076\n",
            "2025-03-24 03:28:31,894 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4797\n",
            "2025-03-24 03:28:40,269 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5472\n",
            "2025-03-24 03:28:48,905 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4637\n",
            "2025-03-24 03:28:57,310 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5593\n",
            "2025-03-24 03:29:05,868 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5427\n",
            "2025-03-24 03:29:14,348 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4747\n",
            "2025-03-24 03:29:20,667 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3673\n",
            "2025-03-24 03:29:21,302 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1760\n",
            "2025-03-24 03:29:21,303 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:30:23,357 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.5133, Val Loss: 0.4282, Val Acc: 0.8413\n",
            "2025-03-24 03:30:23,733 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 03:30:23,733 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 03:30:30,276 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4133\n",
            "2025-03-24 03:30:38,837 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4474\n",
            "2025-03-24 03:30:47,451 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4679\n",
            "2025-03-24 03:30:56,079 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5026\n",
            "2025-03-24 03:31:04,423 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5207\n",
            "2025-03-24 03:31:12,834 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4356\n",
            "2025-03-24 03:31:21,229 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4804\n",
            "2025-03-24 03:31:29,611 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4722\n",
            "2025-03-24 03:31:38,028 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4647\n",
            "2025-03-24 03:31:46,635 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4985\n",
            "2025-03-24 03:31:55,058 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5036\n",
            "2025-03-24 03:32:03,555 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4777\n",
            "2025-03-24 03:32:12,007 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5105\n",
            "2025-03-24 03:32:20,606 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4881\n",
            "2025-03-24 03:32:28,839 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4642\n",
            "2025-03-24 03:32:37,331 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4977\n",
            "2025-03-24 03:32:45,785 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5070\n",
            "2025-03-24 03:32:54,198 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4936\n",
            "2025-03-24 03:33:02,593 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4482\n",
            "2025-03-24 03:33:10,987 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4640\n",
            "2025-03-24 03:33:19,385 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5267\n",
            "2025-03-24 03:33:27,971 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4397\n",
            "2025-03-24 03:33:36,380 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4829\n",
            "2025-03-24 03:33:44,782 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5327\n",
            "2025-03-24 03:33:53,182 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4364\n",
            "2025-03-24 03:34:01,581 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4487\n",
            "2025-03-24 03:34:09,975 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4869\n",
            "2025-03-24 03:34:18,376 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4526\n",
            "2025-03-24 03:34:26,776 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4448\n",
            "2025-03-24 03:34:35,166 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5385\n",
            "2025-03-24 03:34:43,675 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5014\n",
            "2025-03-24 03:34:52,100 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4848\n",
            "2025-03-24 03:35:00,473 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4966\n",
            "2025-03-24 03:35:06,789 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3849\n",
            "2025-03-24 03:35:07,459 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0986\n",
            "2025-03-24 03:35:07,459 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:36:08,846 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4834, Val Loss: 0.3992, Val Acc: 0.8599\n",
            "2025-03-24 03:36:09,216 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 03:36:09,217 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 03:36:15,531 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3381\n",
            "2025-03-24 03:36:24,000 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3838\n",
            "2025-03-24 03:36:32,587 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4783\n",
            "2025-03-24 03:36:40,911 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4476\n",
            "2025-03-24 03:36:49,355 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4367\n",
            "2025-03-24 03:36:57,768 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4674\n",
            "2025-03-24 03:37:06,044 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4084\n",
            "2025-03-24 03:37:14,515 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4803\n",
            "2025-03-24 03:37:22,910 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4747\n",
            "2025-03-24 03:37:31,276 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4449\n",
            "2025-03-24 03:37:39,716 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4420\n",
            "2025-03-24 03:37:48,089 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4617\n",
            "2025-03-24 03:37:56,512 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4425\n",
            "2025-03-24 03:38:05,253 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4627\n",
            "2025-03-24 03:38:13,565 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4596\n",
            "2025-03-24 03:38:22,277 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5143\n",
            "2025-03-24 03:38:30,698 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4364\n",
            "2025-03-24 03:38:39,294 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4228\n",
            "2025-03-24 03:38:47,714 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4283\n",
            "2025-03-24 03:38:56,157 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4850\n",
            "2025-03-24 03:39:04,568 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4989\n",
            "2025-03-24 03:39:13,141 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4222\n",
            "2025-03-24 03:39:21,552 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4347\n",
            "2025-03-24 03:39:30,075 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4050\n",
            "2025-03-24 03:39:38,472 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4421\n",
            "2025-03-24 03:39:46,875 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4349\n",
            "2025-03-24 03:39:55,272 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4401\n",
            "2025-03-24 03:40:03,668 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5132\n",
            "2025-03-24 03:40:12,252 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4723\n",
            "2025-03-24 03:40:20,815 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4818\n",
            "2025-03-24 03:40:29,270 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4540\n",
            "2025-03-24 03:40:37,933 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4996\n",
            "2025-03-24 03:40:46,450 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4654\n",
            "2025-03-24 03:40:52,602 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3562\n",
            "2025-03-24 03:40:53,211 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1115\n",
            "2025-03-24 03:40:53,212 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:41:54,539 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4547, Val Loss: 0.3892, Val Acc: 0.8534\n",
            "2025-03-24 03:41:54,910 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 03:41:54,911 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 03:42:01,424 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3286\n",
            "2025-03-24 03:42:09,830 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4538\n",
            "2025-03-24 03:42:18,291 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4260\n",
            "2025-03-24 03:42:26,660 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3900\n",
            "2025-03-24 03:42:35,141 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3833\n",
            "2025-03-24 03:42:43,661 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4341\n",
            "2025-03-24 03:42:52,079 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3963\n",
            "2025-03-24 03:43:00,520 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4108\n",
            "2025-03-24 03:43:08,970 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3961\n",
            "2025-03-24 03:43:17,256 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3924\n",
            "2025-03-24 03:43:25,804 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4203\n",
            "2025-03-24 03:43:34,197 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4682\n",
            "2025-03-24 03:43:42,445 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4387\n",
            "2025-03-24 03:43:50,989 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4404\n",
            "2025-03-24 03:43:59,370 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3912\n",
            "2025-03-24 03:44:07,672 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4325\n",
            "2025-03-24 03:44:16,197 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4072\n",
            "2025-03-24 03:44:24,591 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4544\n",
            "2025-03-24 03:44:33,374 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3979\n",
            "2025-03-24 03:44:41,742 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4450\n",
            "2025-03-24 03:44:50,182 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4090\n",
            "2025-03-24 03:44:58,567 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4555\n",
            "2025-03-24 03:45:06,942 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4476\n",
            "2025-03-24 03:45:15,227 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3613\n",
            "2025-03-24 03:45:23,816 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3855\n",
            "2025-03-24 03:45:32,183 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3763\n",
            "2025-03-24 03:45:40,753 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4575\n",
            "2025-03-24 03:45:49,158 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4555\n",
            "2025-03-24 03:45:57,563 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4839\n",
            "2025-03-24 03:46:05,955 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4658\n",
            "2025-03-24 03:46:14,537 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4772\n",
            "2025-03-24 03:46:22,950 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4141\n",
            "2025-03-24 03:46:31,348 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4632\n",
            "2025-03-24 03:46:37,566 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3286\n",
            "2025-03-24 03:46:38,205 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1173\n",
            "2025-03-24 03:46:38,206 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:47:39,618 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4268, Val Loss: 0.4235, Val Acc: 0.8516\n",
            "2025-03-24 03:47:39,619 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 03:47:45,904 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2969\n",
            "2025-03-24 03:47:54,336 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3952\n",
            "2025-03-24 03:48:02,738 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4160\n",
            "2025-03-24 03:48:11,176 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4540\n",
            "2025-03-24 03:48:19,552 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4022\n",
            "2025-03-24 03:48:27,960 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4549\n",
            "2025-03-24 03:48:36,557 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3553\n",
            "2025-03-24 03:48:44,903 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3871\n",
            "2025-03-24 03:48:53,220 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4062\n",
            "2025-03-24 03:49:01,681 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4493\n",
            "2025-03-24 03:49:10,096 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4230\n",
            "2025-03-24 03:49:18,572 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4331\n",
            "2025-03-24 03:49:26,984 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3734\n",
            "2025-03-24 03:49:35,684 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4355\n",
            "2025-03-24 03:49:44,479 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4220\n",
            "2025-03-24 03:49:53,080 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4008\n",
            "2025-03-24 03:50:01,878 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4219\n",
            "2025-03-24 03:50:10,474 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3865\n",
            "2025-03-24 03:50:19,190 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3914\n",
            "2025-03-24 03:50:27,947 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3869\n",
            "2025-03-24 03:50:36,681 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4047\n",
            "2025-03-24 03:50:45,255 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4748\n",
            "2025-03-24 03:50:54,008 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4103\n",
            "2025-03-24 03:51:02,663 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3920\n",
            "2025-03-24 03:51:11,267 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4108\n",
            "2025-03-24 03:51:20,060 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3886\n",
            "2025-03-24 03:51:28,654 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3495\n",
            "2025-03-24 03:51:37,369 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4433\n",
            "2025-03-24 03:51:46,102 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3805\n",
            "2025-03-24 03:51:54,637 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3756\n",
            "2025-03-24 03:52:03,605 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3544\n",
            "2025-03-24 03:52:12,105 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4073\n",
            "2025-03-24 03:52:20,854 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4010\n",
            "2025-03-24 03:52:27,325 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3185\n",
            "2025-03-24 03:52:27,969 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1278\n",
            "2025-03-24 03:52:27,970 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:53:29,134 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.4068, Val Loss: 0.3955, Val Acc: 0.8595\n",
            "2025-03-24 03:53:29,135 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 03:53:35,250 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3674\n",
            "2025-03-24 03:53:43,807 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3965\n",
            "2025-03-24 03:53:52,207 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3578\n",
            "2025-03-24 03:54:00,807 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3588\n",
            "2025-03-24 03:54:09,209 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3295\n",
            "2025-03-24 03:54:17,568 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3866\n",
            "2025-03-24 03:54:25,991 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3358\n",
            "2025-03-24 03:54:34,597 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3452\n",
            "2025-03-24 03:54:43,387 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3704\n",
            "2025-03-24 03:54:52,119 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4260\n",
            "2025-03-24 03:55:00,597 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3533\n",
            "2025-03-24 03:55:08,988 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3303\n",
            "2025-03-24 03:55:17,388 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4052\n",
            "2025-03-24 03:55:25,975 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3530\n",
            "2025-03-24 03:55:34,774 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3800\n",
            "2025-03-24 03:55:43,550 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3557\n",
            "2025-03-24 03:55:52,371 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3838\n",
            "2025-03-24 03:56:01,174 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3964\n",
            "2025-03-24 03:56:09,759 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3712\n",
            "2025-03-24 03:56:18,369 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3998\n",
            "2025-03-24 03:56:26,726 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4049\n",
            "2025-03-24 03:56:35,561 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3514\n",
            "2025-03-24 03:56:44,157 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4321\n",
            "2025-03-24 03:56:52,752 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3856\n",
            "2025-03-24 03:57:01,244 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3939\n",
            "2025-03-24 03:57:09,506 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4316\n",
            "2025-03-24 03:57:17,949 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3894\n",
            "2025-03-24 03:57:26,344 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3467\n",
            "2025-03-24 03:57:34,894 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3946\n",
            "2025-03-24 03:57:43,148 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3768\n",
            "2025-03-24 03:57:51,537 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3669\n",
            "2025-03-24 03:57:59,905 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3556\n",
            "2025-03-24 03:58:08,405 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3924\n",
            "2025-03-24 03:58:14,916 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2562\n",
            "2025-03-24 03:58:15,537 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0783\n",
            "2025-03-24 03:58:15,537 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 03:59:16,833 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3781, Val Loss: 0.4121, Val Acc: 0.8576\n",
            "2025-03-24 03:59:16,834 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 03:59:23,138 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2543\n",
            "2025-03-24 03:59:31,524 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3438\n",
            "2025-03-24 03:59:39,884 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3762\n",
            "2025-03-24 03:59:48,302 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3140\n",
            "2025-03-24 03:59:56,710 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3194\n",
            "2025-03-24 04:00:05,359 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3524\n",
            "2025-03-24 04:00:13,702 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3476\n",
            "2025-03-24 04:00:22,114 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3448\n",
            "2025-03-24 04:00:30,514 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3474\n",
            "2025-03-24 04:00:38,889 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3412\n",
            "2025-03-24 04:00:47,436 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3202\n",
            "2025-03-24 04:00:55,840 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3578\n",
            "2025-03-24 04:01:04,253 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3343\n",
            "2025-03-24 04:01:12,652 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3534\n",
            "2025-03-24 04:01:21,046 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4051\n",
            "2025-03-24 04:01:29,453 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3562\n",
            "2025-03-24 04:01:37,832 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3864\n",
            "2025-03-24 04:01:46,248 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3679\n",
            "2025-03-24 04:01:54,644 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3447\n",
            "2025-03-24 04:02:02,879 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3452\n",
            "2025-03-24 04:02:11,244 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3598\n",
            "2025-03-24 04:02:19,680 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3785\n",
            "2025-03-24 04:02:28,213 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4280\n",
            "2025-03-24 04:02:36,608 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3471\n",
            "2025-03-24 04:02:44,885 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3311\n",
            "2025-03-24 04:02:53,256 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3671\n",
            "2025-03-24 04:03:01,729 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4047\n",
            "2025-03-24 04:03:10,637 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3737\n",
            "2025-03-24 04:03:19,035 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4306\n",
            "2025-03-24 04:03:27,589 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3484\n",
            "2025-03-24 04:03:36,058 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3586\n",
            "2025-03-24 04:03:44,538 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3740\n",
            "2025-03-24 04:03:52,984 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3725\n",
            "2025-03-24 04:03:59,320 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3101\n",
            "2025-03-24 04:03:59,966 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1148\n",
            "2025-03-24 04:03:59,966 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:05:02,099 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3618, Val Loss: 0.3891, Val Acc: 0.8618\n",
            "2025-03-24 04:05:02,100 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 04:05:08,630 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-24 04:05:17,087 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3402\n",
            "2025-03-24 04:05:25,317 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3315\n",
            "2025-03-24 04:05:33,796 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3616\n",
            "2025-03-24 04:05:42,183 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3809\n",
            "2025-03-24 04:05:50,587 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3705\n",
            "2025-03-24 04:05:59,176 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3177\n",
            "2025-03-24 04:06:07,581 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2934\n",
            "2025-03-24 04:06:15,982 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3341\n",
            "2025-03-24 04:06:24,379 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3374\n",
            "2025-03-24 04:06:32,938 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3091\n",
            "2025-03-24 04:06:41,404 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-24 04:06:49,895 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3388\n",
            "2025-03-24 04:06:58,413 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-24 04:07:06,683 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3372\n",
            "2025-03-24 04:07:15,125 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3119\n",
            "2025-03-24 04:07:23,562 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3047\n",
            "2025-03-24 04:07:31,956 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2954\n",
            "2025-03-24 04:07:40,361 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3626\n",
            "2025-03-24 04:07:48,686 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3142\n",
            "2025-03-24 04:07:57,169 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3092\n",
            "2025-03-24 04:08:05,553 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3509\n",
            "2025-03-24 04:08:13,950 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3205\n",
            "2025-03-24 04:08:22,344 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3292\n",
            "2025-03-24 04:08:30,742 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3865\n",
            "2025-03-24 04:08:39,138 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3808\n",
            "2025-03-24 04:08:47,538 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3432\n",
            "2025-03-24 04:08:55,917 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3411\n",
            "2025-03-24 04:09:04,334 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3836\n",
            "2025-03-24 04:09:12,634 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4076\n",
            "2025-03-24 04:09:21,036 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3476\n",
            "2025-03-24 04:09:29,525 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3025\n",
            "2025-03-24 04:09:37,923 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3457\n",
            "2025-03-24 04:09:44,384 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-24 04:09:45,009 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1098\n",
            "2025-03-24 04:09:45,010 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:10:46,134 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3400, Val Loss: 0.3935, Val Acc: 0.8683\n",
            "2025-03-24 04:10:46,135 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 04:10:52,689 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2344\n",
            "2025-03-24 04:11:00,878 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3266\n",
            "2025-03-24 04:11:09,300 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 04:11:17,695 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3194\n",
            "2025-03-24 04:11:26,082 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3248\n",
            "2025-03-24 04:11:34,484 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3086\n",
            "2025-03-24 04:11:42,875 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3474\n",
            "2025-03-24 04:11:51,285 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3081\n",
            "2025-03-24 04:11:59,676 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3159\n",
            "2025-03-24 04:12:08,078 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3226\n",
            "2025-03-24 04:12:16,474 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3036\n",
            "2025-03-24 04:12:24,872 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2995\n",
            "2025-03-24 04:12:33,267 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2957\n",
            "2025-03-24 04:12:41,659 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3187\n",
            "2025-03-24 04:12:50,062 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3125\n",
            "2025-03-24 04:12:58,460 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3482\n",
            "2025-03-24 04:13:07,043 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3254\n",
            "2025-03-24 04:13:15,445 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3064\n",
            "2025-03-24 04:13:23,924 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3217\n",
            "2025-03-24 04:13:32,283 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3798\n",
            "2025-03-24 04:13:40,715 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3257\n",
            "2025-03-24 04:13:49,082 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3215\n",
            "2025-03-24 04:13:57,491 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3647\n",
            "2025-03-24 04:14:05,880 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3573\n",
            "2025-03-24 04:14:14,447 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3303\n",
            "2025-03-24 04:14:22,834 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3287\n",
            "2025-03-24 04:14:31,226 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2907\n",
            "2025-03-24 04:14:39,683 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3563\n",
            "2025-03-24 04:14:47,976 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3041\n",
            "2025-03-24 04:14:56,422 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3169\n",
            "2025-03-24 04:15:04,821 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3834\n",
            "2025-03-24 04:15:13,183 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2839\n",
            "2025-03-24 04:15:21,637 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3080\n",
            "2025-03-24 04:15:28,158 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2433\n",
            "2025-03-24 04:15:28,800 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1219\n",
            "2025-03-24 04:15:28,800 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:16:30,081 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3247, Val Loss: 0.4280, Val Acc: 0.8585\n",
            "2025-03-24 04:16:30,081 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 04:16:36,401 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2375\n",
            "2025-03-24 04:16:44,761 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3343\n",
            "2025-03-24 04:16:53,253 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3266\n",
            "2025-03-24 04:17:01,584 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3453\n",
            "2025-03-24 04:17:09,973 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3175\n",
            "2025-03-24 04:17:18,396 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2730\n",
            "2025-03-24 04:17:26,851 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-24 04:17:35,164 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-24 04:17:43,576 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3175\n",
            "2025-03-24 04:17:51,977 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3873\n",
            "2025-03-24 04:18:00,545 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2978\n",
            "2025-03-24 04:18:08,974 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3200\n",
            "2025-03-24 04:18:17,438 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3218\n",
            "2025-03-24 04:18:25,966 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-24 04:18:34,359 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3432\n",
            "2025-03-24 04:18:42,884 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3065\n",
            "2025-03-24 04:18:51,325 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3536\n",
            "2025-03-24 04:18:59,632 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3188\n",
            "2025-03-24 04:19:08,341 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3220\n",
            "2025-03-24 04:19:16,748 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3320\n",
            "2025-03-24 04:19:25,149 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-24 04:19:33,543 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-24 04:19:41,939 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3847\n",
            "2025-03-24 04:19:50,563 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3512\n",
            "2025-03-24 04:19:58,964 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2908\n",
            "2025-03-24 04:20:07,349 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-24 04:20:15,783 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-24 04:20:24,371 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3409\n",
            "2025-03-24 04:20:32,924 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-24 04:20:41,318 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3339\n",
            "2025-03-24 04:20:49,733 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3018\n",
            "2025-03-24 04:20:58,116 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3124\n",
            "2025-03-24 04:21:06,512 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3227\n",
            "2025-03-24 04:21:12,702 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2372\n",
            "2025-03-24 04:21:13,301 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1323\n",
            "2025-03-24 04:21:13,302 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:22:14,449 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.3180, Val Loss: 0.4150, Val Acc: 0.8637\n",
            "2025-03-24 04:22:14,450 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 04:22:20,876 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2511\n",
            "2025-03-24 04:22:29,046 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3217\n",
            "2025-03-24 04:22:37,417 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2780\n",
            "2025-03-24 04:22:45,771 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3205\n",
            "2025-03-24 04:22:54,247 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2765\n",
            "2025-03-24 04:23:02,688 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2841\n",
            "2025-03-24 04:23:11,304 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3129\n",
            "2025-03-24 04:23:19,706 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2762\n",
            "2025-03-24 04:23:28,124 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-24 04:23:36,533 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3252\n",
            "2025-03-24 04:23:45,067 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2974\n",
            "2025-03-24 04:23:53,466 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3213\n",
            "2025-03-24 04:24:01,862 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3042\n",
            "2025-03-24 04:24:10,257 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-24 04:24:18,840 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3254\n",
            "2025-03-24 04:24:27,226 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3212\n",
            "2025-03-24 04:24:35,604 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3237\n",
            "2025-03-24 04:24:43,879 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3069\n",
            "2025-03-24 04:24:52,275 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2857\n",
            "2025-03-24 04:25:00,687 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2874\n",
            "2025-03-24 04:25:09,060 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3857\n",
            "2025-03-24 04:25:17,427 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3241\n",
            "2025-03-24 04:25:26,011 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3079\n",
            "2025-03-24 04:25:34,375 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3383\n",
            "2025-03-24 04:25:42,653 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2971\n",
            "2025-03-24 04:25:51,222 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3108\n",
            "2025-03-24 04:25:59,423 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-24 04:26:07,922 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3449\n",
            "2025-03-24 04:26:16,294 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3098\n",
            "2025-03-24 04:26:24,640 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-24 04:26:33,405 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3168\n",
            "2025-03-24 04:26:41,892 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3278\n",
            "2025-03-24 04:26:50,410 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3275\n",
            "2025-03-24 04:26:56,871 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2397\n",
            "2025-03-24 04:26:57,505 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1629\n",
            "2025-03-24 04:26:57,506 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:27:58,769 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.3127, Val Loss: 0.4054, Val Acc: 0.8646\n",
            "2025-03-24 04:27:58,769 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 04:28:05,182 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1969\n",
            "2025-03-24 04:28:13,785 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-24 04:28:22,365 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2984\n",
            "2025-03-24 04:28:30,959 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2849\n",
            "2025-03-24 04:28:39,359 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2661\n",
            "2025-03-24 04:28:47,769 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2772\n",
            "2025-03-24 04:28:56,353 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2764\n",
            "2025-03-24 04:29:04,771 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2988\n",
            "2025-03-24 04:29:13,159 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-24 04:29:21,560 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-24 04:29:29,958 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3075\n",
            "2025-03-24 04:29:38,358 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2925\n",
            "2025-03-24 04:29:46,758 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2492\n",
            "2025-03-24 04:29:55,148 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2828\n",
            "2025-03-24 04:30:03,547 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2694\n",
            "2025-03-24 04:30:11,938 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2626\n",
            "2025-03-24 04:30:20,344 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-24 04:30:28,741 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2912\n",
            "2025-03-24 04:30:37,141 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2812\n",
            "2025-03-24 04:30:45,533 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2868\n",
            "2025-03-24 04:30:54,165 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2901\n",
            "2025-03-24 04:31:02,761 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2723\n",
            "2025-03-24 04:31:11,335 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2808\n",
            "2025-03-24 04:31:19,846 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2556\n",
            "2025-03-24 04:31:28,146 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2855\n",
            "2025-03-24 04:31:36,561 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-24 04:31:44,869 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2380\n",
            "2025-03-24 04:31:53,317 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2836\n",
            "2025-03-24 04:32:01,715 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2797\n",
            "2025-03-24 04:32:10,114 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-24 04:32:18,506 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2694\n",
            "2025-03-24 04:32:26,907 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2706\n",
            "2025-03-24 04:32:35,299 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2824\n",
            "2025-03-24 04:32:41,495 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2168\n",
            "2025-03-24 04:32:42,094 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0697\n",
            "2025-03-24 04:32:42,095 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:33:43,558 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2785, Val Loss: 0.4043, Val Acc: 0.8646\n",
            "2025-03-24 04:33:43,559 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-24 04:33:49,918 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2086\n",
            "2025-03-24 04:33:58,216 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2767\n",
            "2025-03-24 04:34:06,725 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2915\n",
            "2025-03-24 04:34:15,265 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2797\n",
            "2025-03-24 04:34:23,661 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2566\n",
            "2025-03-24 04:34:32,058 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2484\n",
            "2025-03-24 04:34:40,516 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-24 04:34:48,904 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2369\n",
            "2025-03-24 04:34:57,284 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3048\n",
            "2025-03-24 04:35:05,721 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2628\n",
            "2025-03-24 04:35:14,099 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-24 04:35:22,656 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2627\n",
            "2025-03-24 04:35:31,010 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 04:35:39,377 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2519\n",
            "2025-03-24 04:35:47,841 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2804\n",
            "2025-03-24 04:35:56,242 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2835\n",
            "2025-03-24 04:36:04,758 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2691\n",
            "2025-03-24 04:36:13,016 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2690\n",
            "2025-03-24 04:36:21,581 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2594\n",
            "2025-03-24 04:36:30,123 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-24 04:36:38,393 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2610\n",
            "2025-03-24 04:36:46,651 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2729\n",
            "2025-03-24 04:36:55,017 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2801\n",
            "2025-03-24 04:37:03,621 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2494\n",
            "2025-03-24 04:37:11,999 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2754\n",
            "2025-03-24 04:37:20,603 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-24 04:37:29,006 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2795\n",
            "2025-03-24 04:37:37,410 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2502\n",
            "2025-03-24 04:37:45,621 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2731\n",
            "2025-03-24 04:37:54,101 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-24 04:38:02,794 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-24 04:38:11,739 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3161\n",
            "2025-03-24 04:38:20,189 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-24 04:38:26,485 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1902\n",
            "2025-03-24 04:38:27,096 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1114\n",
            "2025-03-24 04:38:27,097 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:39:28,290 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2728, Val Loss: 0.3912, Val Acc: 0.8674\n",
            "2025-03-24 04:39:28,291 - INFO - [TRAIN INFO] Early stopping at epoch 18 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 04:39:28,291 - INFO - [TRAIN INFO] Total Time: 6235.39s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇████████▂▂▂</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇████████▂▂▂</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇████████▂▂▂</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇████████▂▂▂</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▆▅▅▄▄▄▃▃▂▂▂▁▁▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇█▇▇█████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>learning_rate_classifier</td><td>0.0015</td></tr><tr><td>learning_rate_fusion</td><td>0.0003</td></tr><tr><td>learning_rate_image</td><td>0.0003</td></tr><tr><td>learning_rate_text</td><td>1e-05</td></tr><tr><td>train_loss</td><td>0.2728</td></tr><tr><td>train_val_loss_diff</td><td>-0.11836</td></tr><tr><td>val_accuracy</td><td>0.86738</td></tr><tr><td>val_loss</td><td>0.39116</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_transformer_fusion_fold_3</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ffettvts' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ffettvts</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_025532-ffettvts\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 04:39:30,611 - INFO - [TRAIN INFO] Fold 3 Training Complete at epoch 18. Total Time: 6237.71s\n",
            "2025-03-24 04:39:30,635 - INFO - [K-FOLD INFO] Fold 3 completed in 6239.21 seconds\n",
            "2025-03-24 04:39:30,636 - INFO - [K-FOLD INFO] ============================== Fold 4/5 ==============================\n",
            "2025-03-24 04:39:30,639 - INFO - [K-FOLD INFO] Fold 4:\n",
            "2025-03-24 04:39:30,639 - INFO -    Train Samples: 8595\n",
            "2025-03-24 04:39:30,641 - INFO -    Validation Samples: 2148\n",
            "2025-03-24 04:39:30,642 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 4\n",
            "2025-03-24 04:39:30,643 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 4:\n",
            "2025-03-24 04:39:30,643 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 04:39:31,315 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 4\n",
            "2025-03-24 04:39:31,317 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 4:\n",
            "2025-03-24 04:39:31,317 - INFO - [K-FOLD INFO] Loss function initialized for Fold 4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_transformer_fusion\\wandb\\run-20250324_043931-zomhsau8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/zomhsau8' target=\"_blank\">experiment_multimodal_transformer_fusion_fold_4</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/zomhsau8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/zomhsau8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 04:39:32,113 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 04:39:32,113 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 04:39:38,540 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0471\n",
            "2025-03-24 04:39:47,175 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.3325\n",
            "2025-03-24 04:39:56,132 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3522\n",
            "2025-03-24 04:40:04,967 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3047\n",
            "2025-03-24 04:40:13,756 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.2783\n",
            "2025-03-24 04:40:22,421 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3083\n",
            "2025-03-24 04:40:31,171 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2941\n",
            "2025-03-24 04:40:39,860 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2667\n",
            "2025-03-24 04:40:48,626 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2054\n",
            "2025-03-24 04:40:57,375 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2805\n",
            "2025-03-24 04:41:06,055 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1868\n",
            "2025-03-24 04:41:14,747 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1892\n",
            "2025-03-24 04:41:23,464 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1854\n",
            "2025-03-24 04:41:32,011 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.2259\n",
            "2025-03-24 04:41:40,933 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1484\n",
            "2025-03-24 04:41:49,660 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1567\n",
            "2025-03-24 04:41:58,345 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1549\n",
            "2025-03-24 04:42:07,119 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0875\n",
            "2025-03-24 04:42:15,780 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.1399\n",
            "2025-03-24 04:42:24,534 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0423\n",
            "2025-03-24 04:42:33,412 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0656\n",
            "2025-03-24 04:42:42,166 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0761\n",
            "2025-03-24 04:42:51,101 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.1187\n",
            "2025-03-24 04:42:59,785 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0179\n",
            "2025-03-24 04:43:08,642 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9897\n",
            "2025-03-24 04:43:17,312 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0347\n",
            "2025-03-24 04:43:25,904 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 1.0236\n",
            "2025-03-24 04:43:34,688 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 1.0149\n",
            "2025-03-24 04:43:43,441 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9782\n",
            "2025-03-24 04:43:52,055 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 1.0234\n",
            "2025-03-24 04:44:00,890 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9493\n",
            "2025-03-24 04:44:09,649 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 1.0039\n",
            "2025-03-24 04:44:18,483 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9409\n",
            "2025-03-24 04:44:24,956 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7222\n",
            "2025-03-24 04:44:25,686 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2214\n",
            "2025-03-24 04:44:25,687 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:45:27,836 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1368, Val Loss: 0.8296, Val Acc: 0.6792\n",
            "2025-03-24 04:45:28,215 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 04:45:28,216 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 04:45:34,774 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.7559\n",
            "2025-03-24 04:45:43,473 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9195\n",
            "2025-03-24 04:45:52,230 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.9301\n",
            "2025-03-24 04:46:01,054 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9721\n",
            "2025-03-24 04:46:09,646 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8785\n",
            "2025-03-24 04:46:18,361 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8595\n",
            "2025-03-24 04:46:27,139 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.9309\n",
            "2025-03-24 04:46:36,044 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.9100\n",
            "2025-03-24 04:46:44,837 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.9090\n",
            "2025-03-24 04:46:53,630 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8600\n",
            "2025-03-24 04:47:02,298 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8900\n",
            "2025-03-24 04:47:10,951 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8294\n",
            "2025-03-24 04:47:19,750 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8520\n",
            "2025-03-24 04:47:28,623 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8332\n",
            "2025-03-24 04:47:37,300 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.9129\n",
            "2025-03-24 04:47:46,154 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8627\n",
            "2025-03-24 04:47:55,005 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8161\n",
            "2025-03-24 04:48:03,614 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.8290\n",
            "2025-03-24 04:48:12,412 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.8402\n",
            "2025-03-24 04:48:20,869 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8237\n",
            "2025-03-24 04:48:29,710 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.8045\n",
            "2025-03-24 04:48:38,379 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.8627\n",
            "2025-03-24 04:48:47,200 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.8276\n",
            "2025-03-24 04:48:55,799 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8554\n",
            "2025-03-24 04:49:04,595 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7535\n",
            "2025-03-24 04:49:13,392 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.8524\n",
            "2025-03-24 04:49:21,973 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.8604\n",
            "2025-03-24 04:49:30,618 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7530\n",
            "2025-03-24 04:49:39,191 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7684\n",
            "2025-03-24 04:49:47,940 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.8221\n",
            "2025-03-24 04:49:56,777 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7550\n",
            "2025-03-24 04:50:05,586 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7798\n",
            "2025-03-24 04:50:14,210 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7756\n",
            "2025-03-24 04:50:20,856 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5993\n",
            "2025-03-24 04:50:21,526 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2802\n",
            "2025-03-24 04:50:21,527 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:51:24,307 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8523, Val Loss: 0.6300, Val Acc: 0.7514\n",
            "2025-03-24 04:51:24,703 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 04:51:24,704 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 04:51:31,126 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5040\n",
            "2025-03-24 04:51:39,742 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.7489\n",
            "2025-03-24 04:51:48,379 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.7563\n",
            "2025-03-24 04:51:57,217 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.7332\n",
            "2025-03-24 04:52:05,848 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6993\n",
            "2025-03-24 04:52:14,731 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7690\n",
            "2025-03-24 04:52:23,336 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7509\n",
            "2025-03-24 04:52:31,937 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7487\n",
            "2025-03-24 04:52:40,700 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6889\n",
            "2025-03-24 04:52:49,521 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8256\n",
            "2025-03-24 04:52:58,318 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6553\n",
            "2025-03-24 04:53:07,067 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7534\n",
            "2025-03-24 04:53:15,916 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6445\n",
            "2025-03-24 04:53:24,715 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7226\n",
            "2025-03-24 04:53:33,421 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7739\n",
            "2025-03-24 04:53:42,304 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7053\n",
            "2025-03-24 04:53:50,913 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7500\n",
            "2025-03-24 04:53:59,639 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6926\n",
            "2025-03-24 04:54:08,411 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7657\n",
            "2025-03-24 04:54:17,102 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6969\n",
            "2025-03-24 04:54:25,698 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7308\n",
            "2025-03-24 04:54:34,371 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7619\n",
            "2025-03-24 04:54:43,277 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6811\n",
            "2025-03-24 04:54:51,813 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6642\n",
            "2025-03-24 04:55:00,558 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7160\n",
            "2025-03-24 04:55:09,383 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7120\n",
            "2025-03-24 04:55:18,148 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6803\n",
            "2025-03-24 04:55:26,750 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5915\n",
            "2025-03-24 04:55:35,577 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6146\n",
            "2025-03-24 04:55:44,434 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6757\n",
            "2025-03-24 04:55:53,272 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6860\n",
            "2025-03-24 04:56:02,066 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6123\n",
            "2025-03-24 04:56:10,843 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6995\n",
            "2025-03-24 04:56:17,427 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5309\n",
            "2025-03-24 04:56:18,155 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1954\n",
            "2025-03-24 04:56:18,156 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 04:57:19,196 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.7093, Val Loss: 0.5082, Val Acc: 0.8035\n",
            "2025-03-24 04:57:19,570 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 04:57:19,571 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 04:57:25,912 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4223\n",
            "2025-03-24 04:57:34,628 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6172\n",
            "2025-03-24 04:57:43,037 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6449\n",
            "2025-03-24 04:57:51,483 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6265\n",
            "2025-03-24 04:58:00,038 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6089\n",
            "2025-03-24 04:58:08,595 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5816\n",
            "2025-03-24 04:58:17,051 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6396\n",
            "2025-03-24 04:58:25,515 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6172\n",
            "2025-03-24 04:58:34,114 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6558\n",
            "2025-03-24 04:58:42,623 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6888\n",
            "2025-03-24 04:58:51,023 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5897\n",
            "2025-03-24 04:58:59,609 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6599\n",
            "2025-03-24 04:59:07,977 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5512\n",
            "2025-03-24 04:59:16,408 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6258\n",
            "2025-03-24 04:59:24,807 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5687\n",
            "2025-03-24 04:59:33,172 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6209\n",
            "2025-03-24 04:59:41,395 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6255\n",
            "2025-03-24 04:59:49,869 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5763\n",
            "2025-03-24 04:59:58,163 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5664\n",
            "2025-03-24 05:00:06,682 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6496\n",
            "2025-03-24 05:00:15,211 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6460\n",
            "2025-03-24 05:00:23,585 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5749\n",
            "2025-03-24 05:00:32,247 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5701\n",
            "2025-03-24 05:00:40,789 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6104\n",
            "2025-03-24 05:00:48,979 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6023\n",
            "2025-03-24 05:00:57,385 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5923\n",
            "2025-03-24 05:01:05,776 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6123\n",
            "2025-03-24 05:01:14,106 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6674\n",
            "2025-03-24 05:01:22,554 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6735\n",
            "2025-03-24 05:01:31,155 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6168\n",
            "2025-03-24 05:01:39,759 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5825\n",
            "2025-03-24 05:01:47,969 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5961\n",
            "2025-03-24 05:01:56,381 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6435\n",
            "2025-03-24 05:02:02,851 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4201\n",
            "2025-03-24 05:02:03,494 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1662\n",
            "2025-03-24 05:02:03,495 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:03:04,005 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.6137, Val Loss: 0.4534, Val Acc: 0.8254\n",
            "2025-03-24 05:03:04,373 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 05:03:04,373 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 05:03:10,805 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4721\n",
            "2025-03-24 05:03:19,149 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5033\n",
            "2025-03-24 05:03:27,590 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5529\n",
            "2025-03-24 05:03:36,163 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6427\n",
            "2025-03-24 05:03:44,626 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5836\n",
            "2025-03-24 05:03:53,225 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5887\n",
            "2025-03-24 05:04:01,726 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5148\n",
            "2025-03-24 05:04:10,307 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6228\n",
            "2025-03-24 05:04:18,694 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5231\n",
            "2025-03-24 05:04:27,222 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5369\n",
            "2025-03-24 05:04:35,594 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5405\n",
            "2025-03-24 05:04:44,306 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5222\n",
            "2025-03-24 05:04:52,706 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5785\n",
            "2025-03-24 05:05:01,099 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5283\n",
            "2025-03-24 05:05:09,619 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4751\n",
            "2025-03-24 05:05:18,061 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5498\n",
            "2025-03-24 05:05:26,476 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5188\n",
            "2025-03-24 05:05:34,793 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6014\n",
            "2025-03-24 05:05:43,268 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4918\n",
            "2025-03-24 05:05:51,623 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5666\n",
            "2025-03-24 05:05:59,968 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5266\n",
            "2025-03-24 05:06:08,328 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6092\n",
            "2025-03-24 05:06:16,684 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5474\n",
            "2025-03-24 05:06:25,141 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5986\n",
            "2025-03-24 05:06:33,510 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4605\n",
            "2025-03-24 05:06:41,881 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4770\n",
            "2025-03-24 05:06:50,432 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6233\n",
            "2025-03-24 05:06:58,838 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5217\n",
            "2025-03-24 05:07:07,197 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5358\n",
            "2025-03-24 05:07:15,641 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5109\n",
            "2025-03-24 05:07:24,003 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5396\n",
            "2025-03-24 05:07:32,417 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5825\n",
            "2025-03-24 05:07:40,813 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6021\n",
            "2025-03-24 05:07:47,153 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4461\n",
            "2025-03-24 05:07:47,864 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1653\n",
            "2025-03-24 05:07:47,865 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:08:48,392 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5529, Val Loss: 0.4091, Val Acc: 0.8459\n",
            "2025-03-24 05:08:48,753 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 05:08:48,754 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 05:08:55,079 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3836\n",
            "2025-03-24 05:09:03,536 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5073\n",
            "2025-03-24 05:09:12,077 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5219\n",
            "2025-03-24 05:09:20,629 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4809\n",
            "2025-03-24 05:09:29,017 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5122\n",
            "2025-03-24 05:09:37,427 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4724\n",
            "2025-03-24 05:09:46,009 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4965\n",
            "2025-03-24 05:09:54,493 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4887\n",
            "2025-03-24 05:10:03,057 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4929\n",
            "2025-03-24 05:10:11,490 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4983\n",
            "2025-03-24 05:10:19,829 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5731\n",
            "2025-03-24 05:10:28,349 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5116\n",
            "2025-03-24 05:10:37,003 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4645\n",
            "2025-03-24 05:10:45,574 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5086\n",
            "2025-03-24 05:10:53,885 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4760\n",
            "2025-03-24 05:11:02,373 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4801\n",
            "2025-03-24 05:11:11,359 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4716\n",
            "2025-03-24 05:11:20,087 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5589\n",
            "2025-03-24 05:11:28,896 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4763\n",
            "2025-03-24 05:11:37,387 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4740\n",
            "2025-03-24 05:11:45,993 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4919\n",
            "2025-03-24 05:11:54,432 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5286\n",
            "2025-03-24 05:12:02,764 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5512\n",
            "2025-03-24 05:12:11,216 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4548\n",
            "2025-03-24 05:12:19,754 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5616\n",
            "2025-03-24 05:12:28,131 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5488\n",
            "2025-03-24 05:12:36,666 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4915\n",
            "2025-03-24 05:12:45,196 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4903\n",
            "2025-03-24 05:12:53,537 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4831\n",
            "2025-03-24 05:13:02,158 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5422\n",
            "2025-03-24 05:13:10,735 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4701\n",
            "2025-03-24 05:13:19,146 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5047\n",
            "2025-03-24 05:13:27,535 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5693\n",
            "2025-03-24 05:13:33,940 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4031\n",
            "2025-03-24 05:13:34,569 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1602\n",
            "2025-03-24 05:13:34,570 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:14:35,308 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.5067, Val Loss: 0.4164, Val Acc: 0.8450\n",
            "2025-03-24 05:14:35,309 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 05:14:41,728 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3096\n",
            "2025-03-24 05:14:50,128 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4776\n",
            "2025-03-24 05:14:58,582 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4347\n",
            "2025-03-24 05:15:07,139 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4422\n",
            "2025-03-24 05:15:15,530 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4245\n",
            "2025-03-24 05:15:23,906 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4556\n",
            "2025-03-24 05:15:32,328 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4683\n",
            "2025-03-24 05:15:40,977 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4153\n",
            "2025-03-24 05:15:49,309 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4525\n",
            "2025-03-24 05:15:57,747 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4837\n",
            "2025-03-24 05:16:06,149 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4971\n",
            "2025-03-24 05:16:14,552 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4419\n",
            "2025-03-24 05:16:22,923 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4501\n",
            "2025-03-24 05:16:31,293 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3993\n",
            "2025-03-24 05:16:39,874 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5130\n",
            "2025-03-24 05:16:48,281 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4321\n",
            "2025-03-24 05:16:56,795 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4486\n",
            "2025-03-24 05:17:05,245 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4673\n",
            "2025-03-24 05:17:13,588 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4401\n",
            "2025-03-24 05:17:22,018 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4777\n",
            "2025-03-24 05:17:30,397 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4819\n",
            "2025-03-24 05:17:38,794 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4337\n",
            "2025-03-24 05:17:47,245 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5424\n",
            "2025-03-24 05:17:55,513 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4423\n",
            "2025-03-24 05:18:03,898 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5146\n",
            "2025-03-24 05:18:12,369 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4800\n",
            "2025-03-24 05:18:20,686 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5054\n",
            "2025-03-24 05:18:29,450 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5014\n",
            "2025-03-24 05:18:37,878 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5903\n",
            "2025-03-24 05:18:46,255 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4412\n",
            "2025-03-24 05:18:54,652 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4616\n",
            "2025-03-24 05:19:03,068 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4589\n",
            "2025-03-24 05:19:11,408 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4602\n",
            "2025-03-24 05:19:17,821 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4401\n",
            "2025-03-24 05:19:18,505 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1218\n",
            "2025-03-24 05:19:18,505 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:20:19,177 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4683, Val Loss: 0.4017, Val Acc: 0.8603\n",
            "2025-03-24 05:20:19,537 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-24 05:20:19,538 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 05:20:26,214 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-24 05:20:34,708 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4821\n",
            "2025-03-24 05:20:43,197 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4412\n",
            "2025-03-24 05:20:51,661 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4659\n",
            "2025-03-24 05:21:00,083 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4095\n",
            "2025-03-24 05:21:08,481 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3802\n",
            "2025-03-24 05:21:16,997 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4391\n",
            "2025-03-24 05:21:25,595 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4492\n",
            "2025-03-24 05:21:34,001 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4319\n",
            "2025-03-24 05:21:42,400 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4527\n",
            "2025-03-24 05:21:50,736 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4213\n",
            "2025-03-24 05:21:59,388 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4961\n",
            "2025-03-24 05:22:07,640 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4561\n",
            "2025-03-24 05:22:16,185 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4229\n",
            "2025-03-24 05:22:24,642 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4347\n",
            "2025-03-24 05:22:33,386 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4236\n",
            "2025-03-24 05:22:41,770 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4158\n",
            "2025-03-24 05:22:50,052 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4313\n",
            "2025-03-24 05:22:58,732 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4410\n",
            "2025-03-24 05:23:07,029 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4183\n",
            "2025-03-24 05:23:15,436 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4218\n",
            "2025-03-24 05:23:23,795 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5155\n",
            "2025-03-24 05:23:32,387 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5096\n",
            "2025-03-24 05:23:40,888 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4746\n",
            "2025-03-24 05:23:49,349 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4384\n",
            "2025-03-24 05:23:57,757 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4447\n",
            "2025-03-24 05:24:06,350 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4773\n",
            "2025-03-24 05:24:14,743 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4437\n",
            "2025-03-24 05:24:23,077 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4314\n",
            "2025-03-24 05:24:31,333 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4403\n",
            "2025-03-24 05:24:39,758 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4805\n",
            "2025-03-24 05:24:48,174 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5037\n",
            "2025-03-24 05:24:56,933 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4689\n",
            "2025-03-24 05:25:03,176 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3487\n",
            "2025-03-24 05:25:03,840 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1238\n",
            "2025-03-24 05:25:03,841 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:26:04,288 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4482, Val Loss: 0.4086, Val Acc: 0.8608\n",
            "2025-03-24 05:26:04,289 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 05:26:10,602 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3063\n",
            "2025-03-24 05:26:19,130 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4410\n",
            "2025-03-24 05:26:27,712 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4319\n",
            "2025-03-24 05:26:36,098 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3883\n",
            "2025-03-24 05:26:44,512 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4345\n",
            "2025-03-24 05:26:53,090 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4008\n",
            "2025-03-24 05:27:01,492 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3556\n",
            "2025-03-24 05:27:09,750 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4323\n",
            "2025-03-24 05:27:18,082 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4109\n",
            "2025-03-24 05:27:26,474 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4368\n",
            "2025-03-24 05:27:34,892 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4281\n",
            "2025-03-24 05:27:43,361 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4111\n",
            "2025-03-24 05:27:51,694 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4310\n",
            "2025-03-24 05:28:00,083 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3802\n",
            "2025-03-24 05:28:08,483 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4124\n",
            "2025-03-24 05:28:16,877 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3913\n",
            "2025-03-24 05:28:25,469 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4261\n",
            "2025-03-24 05:28:33,836 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4513\n",
            "2025-03-24 05:28:42,272 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4371\n",
            "2025-03-24 05:28:50,663 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4253\n",
            "2025-03-24 05:28:58,928 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3984\n",
            "2025-03-24 05:29:07,321 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4488\n",
            "2025-03-24 05:29:15,709 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4096\n",
            "2025-03-24 05:29:24,458 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4145\n",
            "2025-03-24 05:29:33,246 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4258\n",
            "2025-03-24 05:29:41,645 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4386\n",
            "2025-03-24 05:29:50,031 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3971\n",
            "2025-03-24 05:29:58,445 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3983\n",
            "2025-03-24 05:30:06,881 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4437\n",
            "2025-03-24 05:30:15,359 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3971\n",
            "2025-03-24 05:30:23,839 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4299\n",
            "2025-03-24 05:30:32,237 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4125\n",
            "2025-03-24 05:30:40,636 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4055\n",
            "2025-03-24 05:30:46,908 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3456\n",
            "2025-03-24 05:30:47,556 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1647\n",
            "2025-03-24 05:30:47,557 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:31:47,948 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4196, Val Loss: 0.4371, Val Acc: 0.8408\n",
            "2025-03-24 05:31:47,948 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 05:31:54,401 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2901\n",
            "2025-03-24 05:32:02,614 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4140\n",
            "2025-03-24 05:32:11,045 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4017\n",
            "2025-03-24 05:32:19,433 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3647\n",
            "2025-03-24 05:32:27,856 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3928\n",
            "2025-03-24 05:32:36,396 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3710\n",
            "2025-03-24 05:32:44,874 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4141\n",
            "2025-03-24 05:32:53,396 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3452\n",
            "2025-03-24 05:33:01,986 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3889\n",
            "2025-03-24 05:33:10,419 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3697\n",
            "2025-03-24 05:33:18,807 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4137\n",
            "2025-03-24 05:33:27,138 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4127\n",
            "2025-03-24 05:33:35,569 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3713\n",
            "2025-03-24 05:33:44,164 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4303\n",
            "2025-03-24 05:33:52,765 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4016\n",
            "2025-03-24 05:34:01,358 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4063\n",
            "2025-03-24 05:34:09,779 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3308\n",
            "2025-03-24 05:34:18,176 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4236\n",
            "2025-03-24 05:34:26,637 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3759\n",
            "2025-03-24 05:34:35,147 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3753\n",
            "2025-03-24 05:34:43,581 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3074\n",
            "2025-03-24 05:34:51,894 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4282\n",
            "2025-03-24 05:35:00,351 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4203\n",
            "2025-03-24 05:35:08,751 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3718\n",
            "2025-03-24 05:35:17,152 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4222\n",
            "2025-03-24 05:35:25,745 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3365\n",
            "2025-03-24 05:35:34,144 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3708\n",
            "2025-03-24 05:35:42,544 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3653\n",
            "2025-03-24 05:35:51,019 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4083\n",
            "2025-03-24 05:35:59,483 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4420\n",
            "2025-03-24 05:36:07,908 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4187\n",
            "2025-03-24 05:36:16,308 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4228\n",
            "2025-03-24 05:36:24,623 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4383\n",
            "2025-03-24 05:36:30,878 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3130\n",
            "2025-03-24 05:36:31,571 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1570\n",
            "2025-03-24 05:36:31,572 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:37:32,075 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3946, Val Loss: 0.4218, Val Acc: 0.8557\n",
            "2025-03-24 05:37:32,075 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 05:37:38,483 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2469\n",
            "2025-03-24 05:37:46,938 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3731\n",
            "2025-03-24 05:37:55,340 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3923\n",
            "2025-03-24 05:38:03,717 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3734\n",
            "2025-03-24 05:38:12,340 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3606\n",
            "2025-03-24 05:38:20,724 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3584\n",
            "2025-03-24 05:38:29,095 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3431\n",
            "2025-03-24 05:38:37,541 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3527\n",
            "2025-03-24 05:38:45,784 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3772\n",
            "2025-03-24 05:38:54,282 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3781\n",
            "2025-03-24 05:39:02,863 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3237\n",
            "2025-03-24 05:39:11,271 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3718\n",
            "2025-03-24 05:39:19,677 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4262\n",
            "2025-03-24 05:39:28,257 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3492\n",
            "2025-03-24 05:39:36,663 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3323\n",
            "2025-03-24 05:39:45,029 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3466\n",
            "2025-03-24 05:39:53,454 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3795\n",
            "2025-03-24 05:40:01,856 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4247\n",
            "2025-03-24 05:40:10,241 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3345\n",
            "2025-03-24 05:40:18,649 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4043\n",
            "2025-03-24 05:40:26,981 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3604\n",
            "2025-03-24 05:40:35,311 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3915\n",
            "2025-03-24 05:40:43,776 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3804\n",
            "2025-03-24 05:40:52,219 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3625\n",
            "2025-03-24 05:41:00,535 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3699\n",
            "2025-03-24 05:41:09,060 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3186\n",
            "2025-03-24 05:41:17,720 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3683\n",
            "2025-03-24 05:41:26,240 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4054\n",
            "2025-03-24 05:41:34,636 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3565\n",
            "2025-03-24 05:41:43,213 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3898\n",
            "2025-03-24 05:41:51,439 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3800\n",
            "2025-03-24 05:42:00,038 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4100\n",
            "2025-03-24 05:42:08,637 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3817\n",
            "2025-03-24 05:42:14,992 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2647\n",
            "2025-03-24 05:42:15,665 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0825\n",
            "2025-03-24 05:42:15,666 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:43:16,057 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3695, Val Loss: 0.4273, Val Acc: 0.8547\n",
            "2025-03-24 05:43:16,057 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 05:43:22,584 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2641\n",
            "2025-03-24 05:43:31,085 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3188\n",
            "2025-03-24 05:43:39,399 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3437\n",
            "2025-03-24 05:43:47,822 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3580\n",
            "2025-03-24 05:43:56,174 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3260\n",
            "2025-03-24 05:44:04,977 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3203\n",
            "2025-03-24 05:44:13,397 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3607\n",
            "2025-03-24 05:44:21,790 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3381\n",
            "2025-03-24 05:44:30,194 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3150\n",
            "2025-03-24 05:44:38,577 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3349\n",
            "2025-03-24 05:44:47,173 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3129\n",
            "2025-03-24 05:44:55,748 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3382\n",
            "2025-03-24 05:45:03,993 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-24 05:45:12,587 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-24 05:45:21,051 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3179\n",
            "2025-03-24 05:45:29,488 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3491\n",
            "2025-03-24 05:45:37,960 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3297\n",
            "2025-03-24 05:45:46,551 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3078\n",
            "2025-03-24 05:45:54,953 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3343\n",
            "2025-03-24 05:46:03,343 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2818\n",
            "2025-03-24 05:46:11,742 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3014\n",
            "2025-03-24 05:46:20,328 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3117\n",
            "2025-03-24 05:46:28,745 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3301\n",
            "2025-03-24 05:46:37,140 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3540\n",
            "2025-03-24 05:46:45,629 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3192\n",
            "2025-03-24 05:46:54,123 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3054\n",
            "2025-03-24 05:47:02,488 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3328\n",
            "2025-03-24 05:47:10,906 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3079\n",
            "2025-03-24 05:47:19,403 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2859\n",
            "2025-03-24 05:47:27,799 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3484\n",
            "2025-03-24 05:47:36,343 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3739\n",
            "2025-03-24 05:47:45,104 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3249\n",
            "2025-03-24 05:47:53,376 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3335\n",
            "2025-03-24 05:47:59,732 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2295\n",
            "2025-03-24 05:48:00,390 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0926\n",
            "2025-03-24 05:48:00,390 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:49:01,657 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3263, Val Loss: 0.4192, Val Acc: 0.8687\n",
            "2025-03-24 05:49:01,657 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 05:49:07,943 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2350\n",
            "2025-03-24 05:49:16,389 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3380\n",
            "2025-03-24 05:49:24,938 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3307\n",
            "2025-03-24 05:49:33,339 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3676\n",
            "2025-03-24 05:49:42,075 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2885\n",
            "2025-03-24 05:49:50,472 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3087\n",
            "2025-03-24 05:49:58,874 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3118\n",
            "2025-03-24 05:50:07,501 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3366\n",
            "2025-03-24 05:50:15,899 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2853\n",
            "2025-03-24 05:50:24,292 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3325\n",
            "2025-03-24 05:50:32,679 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2924\n",
            "2025-03-24 05:50:41,081 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3162\n",
            "2025-03-24 05:50:49,500 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2797\n",
            "2025-03-24 05:50:57,885 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3038\n",
            "2025-03-24 05:51:06,278 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2938\n",
            "2025-03-24 05:51:14,695 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3212\n",
            "2025-03-24 05:51:23,076 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3133\n",
            "2025-03-24 05:51:31,441 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3011\n",
            "2025-03-24 05:51:40,089 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2687\n",
            "2025-03-24 05:51:48,626 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3017\n",
            "2025-03-24 05:51:57,243 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2973\n",
            "2025-03-24 05:52:05,828 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3340\n",
            "2025-03-24 05:52:14,238 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2791\n",
            "2025-03-24 05:52:22,647 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2946\n",
            "2025-03-24 05:52:31,046 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3070\n",
            "2025-03-24 05:52:39,491 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-24 05:52:47,848 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3101\n",
            "2025-03-24 05:52:56,252 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3138\n",
            "2025-03-24 05:53:04,613 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2940\n",
            "2025-03-24 05:53:13,042 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3181\n",
            "2025-03-24 05:53:21,532 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2892\n",
            "2025-03-24 05:53:30,212 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3302\n",
            "2025-03-24 05:53:38,604 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3039\n",
            "2025-03-24 05:53:44,880 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2498\n",
            "2025-03-24 05:53:45,565 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0625\n",
            "2025-03-24 05:53:45,566 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 05:54:46,347 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3085, Val Loss: 0.4163, Val Acc: 0.8603\n",
            "2025-03-24 05:54:46,348 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 05:54:52,784 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2209\n",
            "2025-03-24 05:55:01,185 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3154\n",
            "2025-03-24 05:55:09,584 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3001\n",
            "2025-03-24 05:55:17,983 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2990\n",
            "2025-03-24 05:55:26,412 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2897\n",
            "2025-03-24 05:55:34,769 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3088\n",
            "2025-03-24 05:55:43,192 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2700\n",
            "2025-03-24 05:55:51,566 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2976\n",
            "2025-03-24 05:55:59,971 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3104\n",
            "2025-03-24 05:56:08,558 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2951\n",
            "2025-03-24 05:56:16,957 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-24 05:56:25,364 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3133\n",
            "2025-03-24 05:56:33,774 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-24 05:56:42,337 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-24 05:56:50,688 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2906\n",
            "2025-03-24 05:56:59,095 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-24 05:57:07,580 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3029\n",
            "2025-03-24 05:57:16,126 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2751\n",
            "2025-03-24 05:57:24,540 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3052\n",
            "2025-03-24 05:57:32,832 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2964\n",
            "2025-03-24 05:57:41,315 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2891\n",
            "2025-03-24 05:57:49,931 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2861\n",
            "2025-03-24 05:57:58,335 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-24 05:58:06,914 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2713\n",
            "2025-03-24 05:58:15,271 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-24 05:58:23,766 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3036\n",
            "2025-03-24 05:58:32,183 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3238\n",
            "2025-03-24 05:58:40,585 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3469\n",
            "2025-03-24 05:58:49,090 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2577\n",
            "2025-03-24 05:58:57,504 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2963\n",
            "2025-03-24 05:59:05,943 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3606\n",
            "2025-03-24 05:59:14,301 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3392\n",
            "2025-03-24 05:59:22,771 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2642\n",
            "2025-03-24 05:59:29,140 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2162\n",
            "2025-03-24 05:59:29,805 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1086\n",
            "2025-03-24 05:59:29,806 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:00:30,231 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.2988, Val Loss: 0.4269, Val Acc: 0.8552\n",
            "2025-03-24 06:00:30,232 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 06:00:36,537 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2488\n",
            "2025-03-24 06:00:44,960 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2869\n",
            "2025-03-24 06:00:53,672 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2999\n",
            "2025-03-24 06:01:02,140 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2880\n",
            "2025-03-24 06:01:10,571 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2983\n",
            "2025-03-24 06:01:19,102 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2784\n",
            "2025-03-24 06:01:27,663 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3165\n",
            "2025-03-24 06:01:36,061 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2957\n",
            "2025-03-24 06:01:44,455 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2837\n",
            "2025-03-24 06:01:52,863 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2763\n",
            "2025-03-24 06:02:01,450 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2573\n",
            "2025-03-24 06:02:09,850 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-24 06:02:18,261 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2972\n",
            "2025-03-24 06:02:26,645 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2804\n",
            "2025-03-24 06:02:35,040 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2769\n",
            "2025-03-24 06:02:43,425 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2989\n",
            "2025-03-24 06:02:51,824 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2947\n",
            "2025-03-24 06:03:00,237 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2819\n",
            "2025-03-24 06:03:08,602 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3438\n",
            "2025-03-24 06:03:17,030 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2906\n",
            "2025-03-24 06:03:25,425 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2865\n",
            "2025-03-24 06:03:33,813 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-24 06:03:42,231 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2601\n",
            "2025-03-24 06:03:50,618 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-24 06:03:58,922 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-24 06:04:07,435 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-24 06:04:15,930 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3086\n",
            "2025-03-24 06:04:24,564 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-24 06:04:32,941 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-24 06:04:41,601 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3084\n",
            "2025-03-24 06:04:50,208 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-24 06:04:58,594 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3114\n",
            "2025-03-24 06:05:06,996 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3091\n",
            "2025-03-24 06:05:13,268 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1910\n",
            "2025-03-24 06:05:13,893 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1347\n",
            "2025-03-24 06:05:13,894 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:06:14,454 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2917, Val Loss: 0.4288, Val Acc: 0.8608\n",
            "2025-03-24 06:06:14,455 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 06:06:20,781 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2239\n",
            "2025-03-24 06:06:29,080 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-24 06:06:37,576 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2937\n",
            "2025-03-24 06:06:46,167 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-24 06:06:54,621 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2801\n",
            "2025-03-24 06:07:02,981 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2754\n",
            "2025-03-24 06:07:11,462 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2818\n",
            "2025-03-24 06:07:19,806 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2670\n",
            "2025-03-24 06:07:28,403 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2696\n",
            "2025-03-24 06:07:36,843 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2675\n",
            "2025-03-24 06:07:45,163 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3068\n",
            "2025-03-24 06:07:53,532 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3007\n",
            "2025-03-24 06:08:02,001 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3153\n",
            "2025-03-24 06:08:10,445 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2642\n",
            "2025-03-24 06:08:18,771 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2683\n",
            "2025-03-24 06:08:27,379 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2620\n",
            "2025-03-24 06:08:36,123 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3001\n",
            "2025-03-24 06:08:44,563 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2705\n",
            "2025-03-24 06:08:53,086 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2790\n",
            "2025-03-24 06:09:01,528 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2897\n",
            "2025-03-24 06:09:09,920 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2871\n",
            "2025-03-24 06:09:18,317 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2705\n",
            "2025-03-24 06:09:26,721 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-24 06:09:35,305 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2773\n",
            "2025-03-24 06:09:43,711 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3207\n",
            "2025-03-24 06:09:52,112 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-24 06:10:00,514 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2962\n",
            "2025-03-24 06:10:09,097 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2831\n",
            "2025-03-24 06:10:17,475 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2795\n",
            "2025-03-24 06:10:25,853 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3032\n",
            "2025-03-24 06:10:34,191 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3077\n",
            "2025-03-24 06:10:42,659 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3154\n",
            "2025-03-24 06:10:51,097 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2621\n",
            "2025-03-24 06:10:57,496 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2070\n",
            "2025-03-24 06:10:58,159 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0615\n",
            "2025-03-24 06:10:58,160 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:12:00,547 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2833, Val Loss: 0.4271, Val Acc: 0.8594\n",
            "2025-03-24 06:12:00,548 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 06:12:07,063 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2031\n",
            "2025-03-24 06:12:15,474 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2845\n",
            "2025-03-24 06:12:23,863 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2620\n",
            "2025-03-24 06:12:32,490 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-24 06:12:40,919 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3108\n",
            "2025-03-24 06:12:49,281 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2440\n",
            "2025-03-24 06:12:57,679 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2927\n",
            "2025-03-24 06:13:06,128 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2797\n",
            "2025-03-24 06:13:14,670 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-24 06:13:22,987 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2894\n",
            "2025-03-24 06:13:31,481 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-24 06:13:39,840 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2719\n",
            "2025-03-24 06:13:48,120 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3010\n",
            "2025-03-24 06:13:56,842 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3099\n",
            "2025-03-24 06:14:05,234 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2657\n",
            "2025-03-24 06:14:13,634 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-24 06:14:22,188 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-24 06:14:30,487 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2994\n",
            "2025-03-24 06:14:38,867 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-24 06:14:47,324 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2496\n",
            "2025-03-24 06:14:55,887 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2857\n",
            "2025-03-24 06:15:04,249 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3021\n",
            "2025-03-24 06:15:12,686 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2464\n",
            "2025-03-24 06:15:21,021 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2551\n",
            "2025-03-24 06:15:29,621 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2553\n",
            "2025-03-24 06:15:38,005 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2466\n",
            "2025-03-24 06:15:46,391 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2768\n",
            "2025-03-24 06:15:54,684 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2728\n",
            "2025-03-24 06:16:03,185 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3167\n",
            "2025-03-24 06:16:11,638 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2655\n",
            "2025-03-24 06:16:20,040 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2513\n",
            "2025-03-24 06:16:28,425 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2515\n",
            "2025-03-24 06:16:37,013 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-24 06:16:43,699 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2104\n",
            "2025-03-24 06:16:44,352 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0702\n",
            "2025-03-24 06:16:44,353 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:17:44,854 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2748, Val Loss: 0.4168, Val Acc: 0.8655\n",
            "2025-03-24 06:17:44,854 - INFO - [TRAIN INFO] Early stopping at epoch 17 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 06:17:44,855 - INFO - [TRAIN INFO] Total Time: 5892.74s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▂▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇███▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇███▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇███▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇███▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇██▇█▇██████</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.27484</td></tr><tr><td>train_val_loss_diff</td><td>-0.14195</td></tr><tr><td>val_accuracy</td><td>0.86546</td></tr><tr><td>val_loss</td><td>0.41679</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_transformer_fusion_fold_4</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/zomhsau8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/zomhsau8</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_043931-zomhsau8\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 06:17:47,209 - INFO - [TRAIN INFO] Fold 4 Training Complete at epoch 17. Total Time: 5895.10s\n",
            "2025-03-24 06:17:47,229 - INFO - [K-FOLD INFO] Fold 4 completed in 5896.59 seconds\n",
            "2025-03-24 06:17:47,230 - INFO - [K-FOLD INFO] ============================== Fold 5/5 ==============================\n",
            "2025-03-24 06:17:47,232 - INFO - [K-FOLD INFO] Fold 5:\n",
            "2025-03-24 06:17:47,233 - INFO -    Train Samples: 8595\n",
            "2025-03-24 06:17:47,233 - INFO -    Validation Samples: 2148\n",
            "2025-03-24 06:17:47,234 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 5\n",
            "2025-03-24 06:17:47,236 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 5:\n",
            "2025-03-24 06:17:47,236 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 06:17:47,914 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 5\n",
            "2025-03-24 06:17:47,916 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 5:\n",
            "2025-03-24 06:17:47,917 - INFO - [K-FOLD INFO] Loss function initialized for Fold 5\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_transformer_fusion\\wandb\\run-20250324_061747-ypxz0dc8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ypxz0dc8' target=\"_blank\">experiment_multimodal_transformer_fusion_fold_5</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ypxz0dc8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ypxz0dc8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 06:17:48,608 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 06:17:48,608 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 06:17:55,169 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.1199\n",
            "2025-03-24 06:18:03,599 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4697\n",
            "2025-03-24 06:18:12,078 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.4235\n",
            "2025-03-24 06:18:20,503 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3917\n",
            "2025-03-24 06:18:29,148 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3400\n",
            "2025-03-24 06:18:37,562 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3419\n",
            "2025-03-24 06:18:46,147 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2828\n",
            "2025-03-24 06:18:54,512 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2860\n",
            "2025-03-24 06:19:02,999 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2812\n",
            "2025-03-24 06:19:11,546 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2337\n",
            "2025-03-24 06:19:19,901 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2311\n",
            "2025-03-24 06:19:28,332 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1982\n",
            "2025-03-24 06:19:36,734 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.2241\n",
            "2025-03-24 06:19:45,328 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1451\n",
            "2025-03-24 06:19:53,932 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1685\n",
            "2025-03-24 06:20:02,530 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1518\n",
            "2025-03-24 06:20:11,321 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1678\n",
            "2025-03-24 06:20:20,048 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.1879\n",
            "2025-03-24 06:20:28,841 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0976\n",
            "2025-03-24 06:20:37,519 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0621\n",
            "2025-03-24 06:20:46,271 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0870\n",
            "2025-03-24 06:20:55,115 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0674\n",
            "2025-03-24 06:21:03,578 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0973\n",
            "2025-03-24 06:21:12,499 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0221\n",
            "2025-03-24 06:21:21,221 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 1.0372\n",
            "2025-03-24 06:21:30,061 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0777\n",
            "2025-03-24 06:21:38,884 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 1.0713\n",
            "2025-03-24 06:21:47,500 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9882\n",
            "2025-03-24 06:21:56,286 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9699\n",
            "2025-03-24 06:22:05,087 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9938\n",
            "2025-03-24 06:22:13,793 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9466\n",
            "2025-03-24 06:22:22,476 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9863\n",
            "2025-03-24 06:22:31,093 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9440\n",
            "2025-03-24 06:22:37,671 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.8155\n",
            "2025-03-24 06:22:38,322 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2263\n",
            "2025-03-24 06:22:38,322 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:23:40,838 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1596, Val Loss: 0.8281, Val Acc: 0.6834\n",
            "2025-03-24 06:23:41,184 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 06:23:41,185 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 06:23:47,871 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.7314\n",
            "2025-03-24 06:23:56,508 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9263\n",
            "2025-03-24 06:24:05,343 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8975\n",
            "2025-03-24 06:24:14,243 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9075\n",
            "2025-03-24 06:24:23,039 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8798\n",
            "2025-03-24 06:24:31,800 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.9170\n",
            "2025-03-24 06:24:40,591 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8942\n",
            "2025-03-24 06:24:49,396 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8161\n",
            "2025-03-24 06:24:58,115 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8612\n",
            "2025-03-24 06:25:06,910 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8371\n",
            "2025-03-24 06:25:15,772 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.9196\n",
            "2025-03-24 06:25:24,523 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8540\n",
            "2025-03-24 06:25:33,229 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8614\n",
            "2025-03-24 06:25:42,002 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8416\n",
            "2025-03-24 06:25:50,806 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7921\n",
            "2025-03-24 06:25:59,414 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7912\n",
            "2025-03-24 06:26:08,206 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8104\n",
            "2025-03-24 06:26:16,800 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.8130\n",
            "2025-03-24 06:26:25,503 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.8702\n",
            "2025-03-24 06:26:34,188 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8600\n",
            "2025-03-24 06:26:42,998 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.8227\n",
            "2025-03-24 06:26:51,544 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.8485\n",
            "2025-03-24 06:27:00,393 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.8216\n",
            "2025-03-24 06:27:09,193 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8408\n",
            "2025-03-24 06:27:17,971 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9141\n",
            "2025-03-24 06:27:26,790 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.8281\n",
            "2025-03-24 06:27:35,503 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.8597\n",
            "2025-03-24 06:27:44,341 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7999\n",
            "2025-03-24 06:27:52,971 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7710\n",
            "2025-03-24 06:28:01,630 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7325\n",
            "2025-03-24 06:28:10,464 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8252\n",
            "2025-03-24 06:28:19,278 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8434\n",
            "2025-03-24 06:28:28,124 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7949\n",
            "2025-03-24 06:28:34,510 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5469\n",
            "2025-03-24 06:28:35,196 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1888\n",
            "2025-03-24 06:28:35,197 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:29:38,003 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8450, Val Loss: 0.6148, Val Acc: 0.7649\n",
            "2025-03-24 06:29:38,429 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 06:29:38,430 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 06:29:44,959 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5526\n",
            "2025-03-24 06:29:53,542 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.7400\n",
            "2025-03-24 06:30:02,134 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6942\n",
            "2025-03-24 06:30:10,935 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.7116\n",
            "2025-03-24 06:30:19,589 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7245\n",
            "2025-03-24 06:30:28,253 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7518\n",
            "2025-03-24 06:30:36,828 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7274\n",
            "2025-03-24 06:30:45,688 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8075\n",
            "2025-03-24 06:30:54,487 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7452\n",
            "2025-03-24 06:31:03,232 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6952\n",
            "2025-03-24 06:31:12,020 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7578\n",
            "2025-03-24 06:31:20,874 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6995\n",
            "2025-03-24 06:31:29,708 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6590\n",
            "2025-03-24 06:31:38,236 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7216\n",
            "2025-03-24 06:31:47,103 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7613\n",
            "2025-03-24 06:31:55,891 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6995\n",
            "2025-03-24 06:32:04,570 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7532\n",
            "2025-03-24 06:32:13,343 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6979\n",
            "2025-03-24 06:32:22,297 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6394\n",
            "2025-03-24 06:32:30,851 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7469\n",
            "2025-03-24 06:32:39,706 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7087\n",
            "2025-03-24 06:32:48,381 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7050\n",
            "2025-03-24 06:32:57,074 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6119\n",
            "2025-03-24 06:33:05,852 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7122\n",
            "2025-03-24 06:33:14,674 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6634\n",
            "2025-03-24 06:33:23,477 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6631\n",
            "2025-03-24 06:33:32,147 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6959\n",
            "2025-03-24 06:33:40,747 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7660\n",
            "2025-03-24 06:33:49,627 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7007\n",
            "2025-03-24 06:33:58,272 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6487\n",
            "2025-03-24 06:34:07,006 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7247\n",
            "2025-03-24 06:34:15,808 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6665\n",
            "2025-03-24 06:34:24,628 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6525\n",
            "2025-03-24 06:34:31,235 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5033\n",
            "2025-03-24 06:34:31,914 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2526\n",
            "2025-03-24 06:34:31,915 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:35:33,538 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.7100, Val Loss: 0.5094, Val Acc: 0.8096\n",
            "2025-03-24 06:35:33,940 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 06:35:33,941 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 06:35:40,296 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5116\n",
            "2025-03-24 06:35:48,666 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5648\n",
            "2025-03-24 06:35:57,027 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6275\n",
            "2025-03-24 06:36:05,816 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6115\n",
            "2025-03-24 06:36:14,285 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5791\n",
            "2025-03-24 06:36:23,002 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6750\n",
            "2025-03-24 06:36:31,403 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6740\n",
            "2025-03-24 06:36:39,859 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6165\n",
            "2025-03-24 06:36:48,245 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6321\n",
            "2025-03-24 06:36:56,841 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6175\n",
            "2025-03-24 06:37:05,298 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6346\n",
            "2025-03-24 06:37:14,178 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5986\n",
            "2025-03-24 06:37:22,790 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5920\n",
            "2025-03-24 06:37:31,422 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6713\n",
            "2025-03-24 06:37:40,080 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7009\n",
            "2025-03-24 06:37:48,950 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6568\n",
            "2025-03-24 06:37:57,402 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6756\n",
            "2025-03-24 06:38:05,996 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6018\n",
            "2025-03-24 06:38:14,729 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5880\n",
            "2025-03-24 06:38:23,385 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6119\n",
            "2025-03-24 06:38:32,022 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5979\n",
            "2025-03-24 06:38:40,784 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6294\n",
            "2025-03-24 06:38:49,507 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6103\n",
            "2025-03-24 06:38:58,174 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5691\n",
            "2025-03-24 06:39:06,845 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5814\n",
            "2025-03-24 06:39:15,634 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6201\n",
            "2025-03-24 06:39:24,448 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6157\n",
            "2025-03-24 06:39:33,341 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6338\n",
            "2025-03-24 06:39:42,165 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5753\n",
            "2025-03-24 06:39:50,765 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5406\n",
            "2025-03-24 06:39:59,542 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6664\n",
            "2025-03-24 06:40:08,289 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5830\n",
            "2025-03-24 06:40:16,848 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6505\n",
            "2025-03-24 06:40:23,548 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4575\n",
            "2025-03-24 06:40:24,216 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1310\n",
            "2025-03-24 06:40:24,217 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:41:25,647 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.6194, Val Loss: 0.4547, Val Acc: 0.8273\n",
            "2025-03-24 06:41:26,011 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 06:41:26,012 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 06:41:32,352 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4193\n",
            "2025-03-24 06:41:40,832 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5761\n",
            "2025-03-24 06:41:49,273 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5974\n",
            "2025-03-24 06:41:57,702 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5757\n",
            "2025-03-24 06:42:06,011 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5761\n",
            "2025-03-24 06:42:14,497 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6099\n",
            "2025-03-24 06:42:22,870 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5807\n",
            "2025-03-24 06:42:31,363 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4895\n",
            "2025-03-24 06:42:39,731 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6323\n",
            "2025-03-24 06:42:48,164 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5603\n",
            "2025-03-24 06:42:56,573 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5988\n",
            "2025-03-24 06:43:05,494 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5292\n",
            "2025-03-24 06:43:13,880 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5576\n",
            "2025-03-24 06:43:22,298 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5519\n",
            "2025-03-24 06:43:30,693 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6164\n",
            "2025-03-24 06:43:39,094 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5210\n",
            "2025-03-24 06:43:47,680 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5279\n",
            "2025-03-24 06:43:56,103 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5429\n",
            "2025-03-24 06:44:04,459 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5188\n",
            "2025-03-24 06:44:12,874 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6740\n",
            "2025-03-24 06:44:21,274 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5779\n",
            "2025-03-24 06:44:29,673 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5517\n",
            "2025-03-24 06:44:38,278 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5487\n",
            "2025-03-24 06:44:46,802 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5939\n",
            "2025-03-24 06:44:55,240 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5652\n",
            "2025-03-24 06:45:03,782 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6302\n",
            "2025-03-24 06:45:12,247 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5128\n",
            "2025-03-24 06:45:20,664 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5818\n",
            "2025-03-24 06:45:29,075 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5519\n",
            "2025-03-24 06:45:37,458 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5664\n",
            "2025-03-24 06:45:45,853 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5845\n",
            "2025-03-24 06:45:54,254 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5299\n",
            "2025-03-24 06:46:02,911 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5597\n",
            "2025-03-24 06:46:09,245 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4524\n",
            "2025-03-24 06:46:09,896 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1518\n",
            "2025-03-24 06:46:09,897 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:47:10,498 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5693, Val Loss: 0.4171, Val Acc: 0.8408\n",
            "2025-03-24 06:47:10,867 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 06:47:10,867 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 06:47:17,340 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3590\n",
            "2025-03-24 06:47:25,798 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4748\n",
            "2025-03-24 06:47:34,109 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5251\n",
            "2025-03-24 06:47:42,473 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4964\n",
            "2025-03-24 06:47:51,019 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5104\n",
            "2025-03-24 06:47:59,341 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5420\n",
            "2025-03-24 06:48:07,644 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4912\n",
            "2025-03-24 06:48:15,987 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4950\n",
            "2025-03-24 06:48:24,447 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5052\n",
            "2025-03-24 06:48:32,799 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4720\n",
            "2025-03-24 06:48:41,233 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5307\n",
            "2025-03-24 06:48:49,799 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4720\n",
            "2025-03-24 06:48:58,198 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5156\n",
            "2025-03-24 06:49:06,668 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5432\n",
            "2025-03-24 06:49:15,189 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5115\n",
            "2025-03-24 06:49:23,547 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5248\n",
            "2025-03-24 06:49:32,372 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6018\n",
            "2025-03-24 06:49:40,905 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5254\n",
            "2025-03-24 06:49:49,424 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5607\n",
            "2025-03-24 06:49:57,866 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4272\n",
            "2025-03-24 06:50:06,421 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4896\n",
            "2025-03-24 06:50:14,902 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5555\n",
            "2025-03-24 06:50:23,428 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5773\n",
            "2025-03-24 06:50:31,820 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4812\n",
            "2025-03-24 06:50:40,188 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5099\n",
            "2025-03-24 06:50:48,600 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5454\n",
            "2025-03-24 06:50:57,011 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4623\n",
            "2025-03-24 06:51:05,576 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5216\n",
            "2025-03-24 06:51:13,930 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5389\n",
            "2025-03-24 06:51:22,381 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5306\n",
            "2025-03-24 06:51:30,775 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5243\n",
            "2025-03-24 06:51:39,123 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5394\n",
            "2025-03-24 06:51:47,724 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5580\n",
            "2025-03-24 06:51:53,959 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3813\n",
            "2025-03-24 06:51:54,620 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1207\n",
            "2025-03-24 06:51:54,620 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:52:55,867 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.5161, Val Loss: 0.4039, Val Acc: 0.8482\n",
            "2025-03-24 06:52:56,236 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 06:52:56,237 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 06:53:02,910 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3656\n",
            "2025-03-24 06:53:11,323 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4494\n",
            "2025-03-24 06:53:19,745 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4855\n",
            "2025-03-24 06:53:28,132 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4422\n",
            "2025-03-24 06:53:36,562 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5670\n",
            "2025-03-24 06:53:45,191 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4436\n",
            "2025-03-24 06:53:53,696 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4738\n",
            "2025-03-24 06:54:02,100 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4669\n",
            "2025-03-24 06:54:10,679 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4371\n",
            "2025-03-24 06:54:19,097 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5037\n",
            "2025-03-24 06:54:27,569 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4034\n",
            "2025-03-24 06:54:36,176 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4663\n",
            "2025-03-24 06:54:44,715 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4894\n",
            "2025-03-24 06:54:53,158 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5037\n",
            "2025-03-24 06:55:01,783 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5743\n",
            "2025-03-24 06:55:10,305 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5347\n",
            "2025-03-24 06:55:18,678 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4871\n",
            "2025-03-24 06:55:27,116 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5013\n",
            "2025-03-24 06:55:35,533 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4862\n",
            "2025-03-24 06:55:44,053 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5827\n",
            "2025-03-24 06:55:52,465 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5719\n",
            "2025-03-24 06:56:01,058 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4852\n",
            "2025-03-24 06:56:09,262 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5013\n",
            "2025-03-24 06:56:17,728 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4977\n",
            "2025-03-24 06:56:26,241 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4949\n",
            "2025-03-24 06:56:34,952 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4472\n",
            "2025-03-24 06:56:43,251 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5142\n",
            "2025-03-24 06:56:51,655 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4606\n",
            "2025-03-24 06:57:00,029 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4705\n",
            "2025-03-24 06:57:08,440 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5150\n",
            "2025-03-24 06:57:16,803 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4416\n",
            "2025-03-24 06:57:25,241 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4944\n",
            "2025-03-24 06:57:33,611 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5378\n",
            "2025-03-24 06:57:39,868 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3603\n",
            "2025-03-24 06:57:40,519 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1043\n",
            "2025-03-24 06:57:40,520 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 06:58:41,529 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4907, Val Loss: 0.3902, Val Acc: 0.8547\n",
            "2025-03-24 06:58:41,903 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 06:58:41,904 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 06:58:48,214 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3188\n",
            "2025-03-24 06:58:56,761 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4280\n",
            "2025-03-24 06:59:05,160 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4363\n",
            "2025-03-24 06:59:13,531 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4310\n",
            "2025-03-24 06:59:22,038 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4977\n",
            "2025-03-24 06:59:30,437 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4263\n",
            "2025-03-24 06:59:38,844 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4307\n",
            "2025-03-24 06:59:47,420 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4156\n",
            "2025-03-24 06:59:55,791 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4770\n",
            "2025-03-24 07:00:04,194 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4506\n",
            "2025-03-24 07:00:12,785 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4307\n",
            "2025-03-24 07:00:21,327 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4542\n",
            "2025-03-24 07:00:29,663 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4236\n",
            "2025-03-24 07:00:38,205 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4903\n",
            "2025-03-24 07:00:46,581 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4490\n",
            "2025-03-24 07:00:55,028 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5201\n",
            "2025-03-24 07:01:03,577 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4256\n",
            "2025-03-24 07:01:11,971 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4103\n",
            "2025-03-24 07:01:20,369 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5591\n",
            "2025-03-24 07:01:29,154 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4536\n",
            "2025-03-24 07:01:37,559 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4147\n",
            "2025-03-24 07:01:45,953 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5066\n",
            "2025-03-24 07:01:54,144 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5035\n",
            "2025-03-24 07:02:02,635 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5189\n",
            "2025-03-24 07:02:11,294 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4396\n",
            "2025-03-24 07:02:19,932 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4595\n",
            "2025-03-24 07:02:28,345 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4051\n",
            "2025-03-24 07:02:36,925 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4673\n",
            "2025-03-24 07:02:45,306 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4645\n",
            "2025-03-24 07:02:53,781 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4928\n",
            "2025-03-24 07:03:02,380 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4928\n",
            "2025-03-24 07:03:10,788 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4401\n",
            "2025-03-24 07:03:19,389 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4867\n",
            "2025-03-24 07:03:25,720 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3460\n",
            "2025-03-24 07:03:26,373 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0967\n",
            "2025-03-24 07:03:26,374 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:04:26,945 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4582, Val Loss: 0.4127, Val Acc: 0.8552\n",
            "2025-03-24 07:04:26,945 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 07:04:33,286 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3330\n",
            "2025-03-24 07:04:41,680 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4280\n",
            "2025-03-24 07:04:50,077 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4609\n",
            "2025-03-24 07:04:58,471 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4519\n",
            "2025-03-24 07:05:06,854 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3911\n",
            "2025-03-24 07:05:15,454 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4335\n",
            "2025-03-24 07:05:24,078 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4693\n",
            "2025-03-24 07:05:32,675 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4488\n",
            "2025-03-24 07:05:41,073 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4011\n",
            "2025-03-24 07:05:49,302 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4403\n",
            "2025-03-24 07:05:58,079 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4126\n",
            "2025-03-24 07:06:06,456 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3572\n",
            "2025-03-24 07:06:14,868 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4502\n",
            "2025-03-24 07:06:23,272 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4664\n",
            "2025-03-24 07:06:31,670 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3920\n",
            "2025-03-24 07:06:40,069 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4837\n",
            "2025-03-24 07:06:48,686 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4562\n",
            "2025-03-24 07:06:57,065 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3835\n",
            "2025-03-24 07:07:05,533 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4265\n",
            "2025-03-24 07:07:14,259 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4042\n",
            "2025-03-24 07:07:22,653 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4557\n",
            "2025-03-24 07:07:31,026 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4649\n",
            "2025-03-24 07:07:39,448 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4378\n",
            "2025-03-24 07:07:47,791 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4626\n",
            "2025-03-24 07:07:56,251 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4609\n",
            "2025-03-24 07:08:04,842 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4011\n",
            "2025-03-24 07:08:13,434 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4620\n",
            "2025-03-24 07:08:21,844 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4094\n",
            "2025-03-24 07:08:30,249 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4168\n",
            "2025-03-24 07:08:38,664 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4039\n",
            "2025-03-24 07:08:47,050 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4358\n",
            "2025-03-24 07:08:55,405 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4322\n",
            "2025-03-24 07:09:03,872 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4400\n",
            "2025-03-24 07:09:10,229 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3654\n",
            "2025-03-24 07:09:10,939 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0737\n",
            "2025-03-24 07:09:10,939 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:10:11,743 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4330, Val Loss: 0.3940, Val Acc: 0.8557\n",
            "2025-03-24 07:10:11,744 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 07:10:18,184 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2731\n",
            "2025-03-24 07:10:26,724 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4173\n",
            "2025-03-24 07:10:35,202 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3621\n",
            "2025-03-24 07:10:43,874 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3909\n",
            "2025-03-24 07:10:52,370 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3726\n",
            "2025-03-24 07:11:00,747 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4230\n",
            "2025-03-24 07:11:09,015 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3600\n",
            "2025-03-24 07:11:17,478 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3920\n",
            "2025-03-24 07:11:26,035 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4075\n",
            "2025-03-24 07:11:34,465 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3947\n",
            "2025-03-24 07:11:42,826 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3705\n",
            "2025-03-24 07:11:51,394 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4104\n",
            "2025-03-24 07:11:59,727 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4307\n",
            "2025-03-24 07:12:08,214 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3911\n",
            "2025-03-24 07:12:16,484 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3846\n",
            "2025-03-24 07:12:24,962 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4290\n",
            "2025-03-24 07:12:33,357 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4277\n",
            "2025-03-24 07:12:41,758 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3773\n",
            "2025-03-24 07:12:50,385 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3998\n",
            "2025-03-24 07:12:58,800 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4009\n",
            "2025-03-24 07:13:07,194 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3965\n",
            "2025-03-24 07:13:15,578 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4158\n",
            "2025-03-24 07:13:23,954 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4041\n",
            "2025-03-24 07:13:32,236 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4204\n",
            "2025-03-24 07:13:40,740 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4852\n",
            "2025-03-24 07:13:49,137 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4001\n",
            "2025-03-24 07:13:57,534 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3927\n",
            "2025-03-24 07:14:05,932 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3441\n",
            "2025-03-24 07:14:14,331 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4085\n",
            "2025-03-24 07:14:22,919 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4059\n",
            "2025-03-24 07:14:31,328 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3961\n",
            "2025-03-24 07:14:39,884 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3744\n",
            "2025-03-24 07:14:48,314 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4001\n",
            "2025-03-24 07:14:54,535 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-24 07:14:55,169 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0876\n",
            "2025-03-24 07:14:55,170 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:15:56,007 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3981, Val Loss: 0.3979, Val Acc: 0.8631\n",
            "2025-03-24 07:15:56,007 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 07:16:02,455 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-24 07:16:10,850 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4010\n",
            "2025-03-24 07:16:19,247 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4288\n",
            "2025-03-24 07:16:27,647 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3536\n",
            "2025-03-24 07:16:36,325 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3521\n",
            "2025-03-24 07:16:44,709 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3690\n",
            "2025-03-24 07:16:53,079 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3643\n",
            "2025-03-24 07:17:01,501 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3749\n",
            "2025-03-24 07:17:09,683 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3597\n",
            "2025-03-24 07:17:18,074 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3759\n",
            "2025-03-24 07:17:26,326 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4010\n",
            "2025-03-24 07:17:34,916 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3540\n",
            "2025-03-24 07:17:43,472 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4250\n",
            "2025-03-24 07:17:51,872 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4286\n",
            "2025-03-24 07:18:00,354 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4044\n",
            "2025-03-24 07:18:08,860 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4018\n",
            "2025-03-24 07:18:17,253 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3759\n",
            "2025-03-24 07:18:25,648 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4299\n",
            "2025-03-24 07:18:34,046 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3753\n",
            "2025-03-24 07:18:42,632 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3654\n",
            "2025-03-24 07:18:51,027 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3710\n",
            "2025-03-24 07:18:59,447 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3662\n",
            "2025-03-24 07:19:07,816 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3940\n",
            "2025-03-24 07:19:16,229 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3942\n",
            "2025-03-24 07:19:24,623 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4208\n",
            "2025-03-24 07:19:33,126 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4076\n",
            "2025-03-24 07:19:41,444 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4022\n",
            "2025-03-24 07:19:50,230 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3808\n",
            "2025-03-24 07:19:58,824 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4206\n",
            "2025-03-24 07:20:07,607 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3780\n",
            "2025-03-24 07:20:16,324 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4404\n",
            "2025-03-24 07:20:24,725 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4092\n",
            "2025-03-24 07:20:33,414 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3714\n",
            "2025-03-24 07:20:39,881 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3150\n",
            "2025-03-24 07:20:40,562 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1042\n",
            "2025-03-24 07:20:40,563 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:21:42,071 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3912, Val Loss: 0.4023, Val Acc: 0.8603\n",
            "2025-03-24 07:21:42,072 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 07:21:48,391 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2680\n",
            "2025-03-24 07:21:56,791 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3377\n",
            "2025-03-24 07:22:05,189 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3371\n",
            "2025-03-24 07:22:13,777 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3470\n",
            "2025-03-24 07:22:22,388 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3742\n",
            "2025-03-24 07:22:31,083 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3487\n",
            "2025-03-24 07:22:39,657 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3041\n",
            "2025-03-24 07:22:47,990 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3310\n",
            "2025-03-24 07:22:56,656 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3540\n",
            "2025-03-24 07:23:05,342 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3281\n",
            "2025-03-24 07:23:13,751 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3558\n",
            "2025-03-24 07:23:22,329 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3475\n",
            "2025-03-24 07:23:30,758 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3628\n",
            "2025-03-24 07:23:39,539 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3612\n",
            "2025-03-24 07:23:48,059 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3417\n",
            "2025-03-24 07:23:56,602 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3117\n",
            "2025-03-24 07:24:04,970 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3156\n",
            "2025-03-24 07:24:13,547 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3493\n",
            "2025-03-24 07:24:22,263 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3748\n",
            "2025-03-24 07:24:30,928 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3348\n",
            "2025-03-24 07:24:39,509 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2879\n",
            "2025-03-24 07:24:47,963 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3280\n",
            "2025-03-24 07:24:56,616 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3288\n",
            "2025-03-24 07:25:05,324 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3282\n",
            "2025-03-24 07:25:13,798 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3232\n",
            "2025-03-24 07:25:22,313 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3392\n",
            "2025-03-24 07:25:30,800 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3301\n",
            "2025-03-24 07:25:39,326 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3073\n",
            "2025-03-24 07:25:47,715 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3112\n",
            "2025-03-24 07:25:56,289 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3180\n",
            "2025-03-24 07:26:04,757 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3326\n",
            "2025-03-24 07:26:13,512 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3250\n",
            "2025-03-24 07:26:21,911 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3333\n",
            "2025-03-24 07:26:28,180 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2793\n",
            "2025-03-24 07:26:28,864 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1213\n",
            "2025-03-24 07:26:28,864 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:27:30,096 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3371, Val Loss: 0.3934, Val Acc: 0.8678\n",
            "2025-03-24 07:27:30,097 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 07:27:36,523 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2066\n",
            "2025-03-24 07:27:44,944 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3238\n",
            "2025-03-24 07:27:53,482 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3305\n",
            "2025-03-24 07:28:01,923 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3078\n",
            "2025-03-24 07:28:10,226 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3598\n",
            "2025-03-24 07:28:18,912 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2912\n",
            "2025-03-24 07:28:27,314 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3208\n",
            "2025-03-24 07:28:35,705 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3290\n",
            "2025-03-24 07:28:44,086 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3336\n",
            "2025-03-24 07:28:52,467 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3108\n",
            "2025-03-24 07:29:00,938 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-24 07:29:09,267 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3206\n",
            "2025-03-24 07:29:17,845 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3485\n",
            "2025-03-24 07:29:26,246 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3129\n",
            "2025-03-24 07:29:34,468 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3097\n",
            "2025-03-24 07:29:42,900 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2909\n",
            "2025-03-24 07:29:51,252 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-24 07:29:59,731 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3288\n",
            "2025-03-24 07:30:08,417 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3104\n",
            "2025-03-24 07:30:17,021 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3115\n",
            "2025-03-24 07:30:25,618 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3119\n",
            "2025-03-24 07:30:33,806 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3709\n",
            "2025-03-24 07:30:42,233 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3472\n",
            "2025-03-24 07:30:50,620 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3311\n",
            "2025-03-24 07:30:59,200 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3187\n",
            "2025-03-24 07:31:07,588 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3162\n",
            "2025-03-24 07:31:16,189 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3127\n",
            "2025-03-24 07:31:24,588 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3232\n",
            "2025-03-24 07:31:33,008 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-24 07:31:41,406 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3004\n",
            "2025-03-24 07:31:49,605 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3422\n",
            "2025-03-24 07:31:58,064 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2970\n",
            "2025-03-24 07:32:06,463 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3295\n",
            "2025-03-24 07:32:12,991 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2305\n",
            "2025-03-24 07:32:13,656 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0597\n",
            "2025-03-24 07:32:13,657 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:33:14,121 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3180, Val Loss: 0.3820, Val Acc: 0.8682\n",
            "2025-03-24 07:33:14,507 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-24 07:33:14,507 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 07:33:20,961 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2050\n",
            "2025-03-24 07:33:29,663 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3316\n",
            "2025-03-24 07:33:38,154 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-24 07:33:46,389 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3225\n",
            "2025-03-24 07:33:54,820 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3027\n",
            "2025-03-24 07:34:03,415 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3089\n",
            "2025-03-24 07:34:11,750 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-24 07:34:20,277 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2883\n",
            "2025-03-24 07:34:28,762 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2962\n",
            "2025-03-24 07:34:37,358 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3268\n",
            "2025-03-24 07:34:45,757 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2860\n",
            "2025-03-24 07:34:54,398 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2962\n",
            "2025-03-24 07:35:02,989 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2803\n",
            "2025-03-24 07:35:11,546 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2840\n",
            "2025-03-24 07:35:20,016 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3010\n",
            "2025-03-24 07:35:28,734 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-24 07:35:36,980 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3385\n",
            "2025-03-24 07:35:45,405 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2758\n",
            "2025-03-24 07:35:53,829 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3541\n",
            "2025-03-24 07:36:02,322 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2996\n",
            "2025-03-24 07:36:10,926 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2661\n",
            "2025-03-24 07:36:19,466 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-24 07:36:27,899 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3347\n",
            "2025-03-24 07:36:36,294 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2841\n",
            "2025-03-24 07:36:44,498 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3191\n",
            "2025-03-24 07:36:52,849 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3095\n",
            "2025-03-24 07:37:01,465 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-24 07:37:09,864 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3448\n",
            "2025-03-24 07:37:18,279 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3461\n",
            "2025-03-24 07:37:26,681 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3019\n",
            "2025-03-24 07:37:35,314 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3236\n",
            "2025-03-24 07:37:43,670 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3282\n",
            "2025-03-24 07:37:52,058 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3053\n",
            "2025-03-24 07:37:58,686 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2329\n",
            "2025-03-24 07:37:59,329 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0984\n",
            "2025-03-24 07:37:59,330 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:39:00,214 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3069, Val Loss: 0.4028, Val Acc: 0.8589\n",
            "2025-03-24 07:39:00,214 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 07:39:06,495 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2124\n",
            "2025-03-24 07:39:14,889 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3153\n",
            "2025-03-24 07:39:23,316 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2865\n",
            "2025-03-24 07:39:31,683 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 07:39:40,359 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2775\n",
            "2025-03-24 07:39:48,815 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2726\n",
            "2025-03-24 07:39:57,173 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2868\n",
            "2025-03-24 07:40:05,566 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2852\n",
            "2025-03-24 07:40:13,916 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-24 07:40:22,347 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-24 07:40:30,744 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2909\n",
            "2025-03-24 07:40:39,306 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3288\n",
            "2025-03-24 07:40:47,944 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2731\n",
            "2025-03-24 07:40:56,439 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3118\n",
            "2025-03-24 07:41:04,830 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3116\n",
            "2025-03-24 07:41:13,410 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3169\n",
            "2025-03-24 07:41:21,821 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2723\n",
            "2025-03-24 07:41:30,408 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2947\n",
            "2025-03-24 07:41:39,025 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2765\n",
            "2025-03-24 07:41:47,536 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3041\n",
            "2025-03-24 07:41:55,802 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3038\n",
            "2025-03-24 07:42:04,213 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2795\n",
            "2025-03-24 07:42:12,813 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2894\n",
            "2025-03-24 07:42:21,218 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-24 07:42:29,565 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2657\n",
            "2025-03-24 07:42:38,006 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 07:42:46,399 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-24 07:42:55,168 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3033\n",
            "2025-03-24 07:43:03,605 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2986\n",
            "2025-03-24 07:43:12,039 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2878\n",
            "2025-03-24 07:43:20,442 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2841\n",
            "2025-03-24 07:43:28,842 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3107\n",
            "2025-03-24 07:43:37,365 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3080\n",
            "2025-03-24 07:43:43,575 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2148\n",
            "2025-03-24 07:43:44,203 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0634\n",
            "2025-03-24 07:43:44,204 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:44:44,719 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2933, Val Loss: 0.4136, Val Acc: 0.8645\n",
            "2025-03-24 07:44:44,719 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 07:44:51,126 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2179\n",
            "2025-03-24 07:44:59,518 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2991\n",
            "2025-03-24 07:45:07,877 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2870\n",
            "2025-03-24 07:45:16,465 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2822\n",
            "2025-03-24 07:45:24,954 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2876\n",
            "2025-03-24 07:45:33,357 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2589\n",
            "2025-03-24 07:45:41,750 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-24 07:45:50,380 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-24 07:45:58,718 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2770\n",
            "2025-03-24 07:46:07,205 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2642\n",
            "2025-03-24 07:46:15,577 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2905\n",
            "2025-03-24 07:46:23,921 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2750\n",
            "2025-03-24 07:46:32,450 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3002\n",
            "2025-03-24 07:46:41,134 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2703\n",
            "2025-03-24 07:46:49,526 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-24 07:46:57,937 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2920\n",
            "2025-03-24 07:47:06,525 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2819\n",
            "2025-03-24 07:47:15,106 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3007\n",
            "2025-03-24 07:47:23,520 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2750\n",
            "2025-03-24 07:47:31,916 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2983\n",
            "2025-03-24 07:47:40,416 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2963\n",
            "2025-03-24 07:47:48,888 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2794\n",
            "2025-03-24 07:47:57,314 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2785\n",
            "2025-03-24 07:48:05,709 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-24 07:48:14,064 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-24 07:48:22,423 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3028\n",
            "2025-03-24 07:48:30,898 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-24 07:48:39,300 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3031\n",
            "2025-03-24 07:48:47,883 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2955\n",
            "2025-03-24 07:48:56,287 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2876\n",
            "2025-03-24 07:49:04,689 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3002\n",
            "2025-03-24 07:49:13,084 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2942\n",
            "2025-03-24 07:49:21,480 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2995\n",
            "2025-03-24 07:49:27,645 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2097\n",
            "2025-03-24 07:49:28,298 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1109\n",
            "2025-03-24 07:49:28,299 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:50:29,044 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2859, Val Loss: 0.4257, Val Acc: 0.8617\n",
            "2025-03-24 07:50:29,044 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 07:50:35,446 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2193\n",
            "2025-03-24 07:50:44,042 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2916\n",
            "2025-03-24 07:50:52,457 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-24 07:51:01,039 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2883\n",
            "2025-03-24 07:51:09,452 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-24 07:51:17,847 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2975\n",
            "2025-03-24 07:51:26,252 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2874\n",
            "2025-03-24 07:51:34,828 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-24 07:51:43,206 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2950\n",
            "2025-03-24 07:51:51,435 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2864\n",
            "2025-03-24 07:51:59,879 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2487\n",
            "2025-03-24 07:52:08,410 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2879\n",
            "2025-03-24 07:52:16,807 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2698\n",
            "2025-03-24 07:52:25,099 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3048\n",
            "2025-03-24 07:52:33,707 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-24 07:52:42,223 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-24 07:52:50,618 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2783\n",
            "2025-03-24 07:52:59,399 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3018\n",
            "2025-03-24 07:53:07,996 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2703\n",
            "2025-03-24 07:53:16,426 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-24 07:53:24,804 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3038\n",
            "2025-03-24 07:53:33,247 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2753\n",
            "2025-03-24 07:53:41,683 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2905\n",
            "2025-03-24 07:53:50,053 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2989\n",
            "2025-03-24 07:53:58,452 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2786\n",
            "2025-03-24 07:54:06,862 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-24 07:54:15,386 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3108\n",
            "2025-03-24 07:54:23,794 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-24 07:54:32,187 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-24 07:54:40,579 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2865\n",
            "2025-03-24 07:54:48,995 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2698\n",
            "2025-03-24 07:54:57,381 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2642\n",
            "2025-03-24 07:55:05,996 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2856\n",
            "2025-03-24 07:55:12,371 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2001\n",
            "2025-03-24 07:55:13,028 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0713\n",
            "2025-03-24 07:55:13,028 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 07:56:14,125 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2824, Val Loss: 0.4151, Val Acc: 0.8673\n",
            "2025-03-24 07:56:14,126 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-24 07:56:20,531 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1964\n",
            "2025-03-24 07:56:28,949 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2739\n",
            "2025-03-24 07:56:37,361 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2700\n",
            "2025-03-24 07:56:46,079 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3097\n",
            "2025-03-24 07:56:54,703 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-24 07:57:02,974 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2635\n",
            "2025-03-24 07:57:11,429 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-24 07:57:19,774 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2861\n",
            "2025-03-24 07:57:28,354 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2858\n",
            "2025-03-24 07:57:36,730 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2876\n",
            "2025-03-24 07:57:45,126 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-24 07:57:53,526 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2891\n",
            "2025-03-24 07:58:02,109 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-24 07:58:10,630 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2671\n",
            "2025-03-24 07:58:18,895 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2827\n",
            "2025-03-24 07:58:27,277 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2757\n",
            "2025-03-24 07:58:35,692 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-24 07:58:44,040 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2517\n",
            "2025-03-24 07:58:52,700 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-24 07:59:01,087 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3264\n",
            "2025-03-24 07:59:09,470 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-24 07:59:17,887 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2580\n",
            "2025-03-24 07:59:26,488 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2607\n",
            "2025-03-24 07:59:34,933 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2902\n",
            "2025-03-24 07:59:43,345 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2984\n",
            "2025-03-24 07:59:51,889 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2850\n",
            "2025-03-24 08:00:00,479 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2735\n",
            "2025-03-24 08:00:08,925 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-24 08:00:17,338 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2739\n",
            "2025-03-24 08:00:25,711 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2690\n",
            "2025-03-24 08:00:34,154 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2704\n",
            "2025-03-24 08:00:43,152 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2632\n",
            "2025-03-24 08:00:51,735 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2989\n",
            "2025-03-24 08:00:58,107 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2181\n",
            "2025-03-24 08:00:58,777 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1024\n",
            "2025-03-24 08:00:58,778 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 08:01:59,558 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2783, Val Loss: 0.4158, Val Acc: 0.8687\n",
            "2025-03-24 08:01:59,559 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-24 08:02:06,041 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1959\n",
            "2025-03-24 08:02:14,381 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2904\n",
            "2025-03-24 08:02:22,697 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2557\n",
            "2025-03-24 08:02:31,029 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-24 08:02:39,536 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2719\n",
            "2025-03-24 08:02:47,858 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2652\n",
            "2025-03-24 08:02:56,261 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2688\n",
            "2025-03-24 08:03:04,681 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2677\n",
            "2025-03-24 08:03:13,059 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-24 08:03:21,589 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 08:03:30,413 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2808\n",
            "2025-03-24 08:03:39,011 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-24 08:03:47,447 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2869\n",
            "2025-03-24 08:03:55,815 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2810\n",
            "2025-03-24 08:04:04,228 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2608\n",
            "2025-03-24 08:04:12,570 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3011\n",
            "2025-03-24 08:04:21,007 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-24 08:04:29,607 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2836\n",
            "2025-03-24 08:04:38,005 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2661\n",
            "2025-03-24 08:04:46,591 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2650\n",
            "2025-03-24 08:04:54,979 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2892\n",
            "2025-03-24 08:05:03,387 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2631\n",
            "2025-03-24 08:05:11,677 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2881\n",
            "2025-03-24 08:05:20,229 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2690\n",
            "2025-03-24 08:05:28,773 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2453\n",
            "2025-03-24 08:05:37,221 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2631\n",
            "2025-03-24 08:05:45,581 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3012\n",
            "2025-03-24 08:05:53,980 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2588\n",
            "2025-03-24 08:06:02,377 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2837\n",
            "2025-03-24 08:06:11,032 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2595\n",
            "2025-03-24 08:06:19,407 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2696\n",
            "2025-03-24 08:06:30,372 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2686\n",
            "2025-03-24 08:06:39,360 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2729\n",
            "2025-03-24 08:06:45,863 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2006\n",
            "2025-03-24 08:06:46,529 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0627\n",
            "2025-03-24 08:06:46,530 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 08:07:46,458 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2733, Val Loss: 0.4066, Val Acc: 0.8692\n",
            "2025-03-24 08:07:46,458 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-24 08:07:52,540 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1942\n",
            "2025-03-24 08:08:01,074 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2704\n",
            "2025-03-24 08:08:09,794 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2652\n",
            "2025-03-24 08:08:18,699 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2669\n",
            "2025-03-24 08:08:27,118 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2580\n",
            "2025-03-24 08:08:35,697 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2476\n",
            "2025-03-24 08:08:44,305 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2847\n",
            "2025-03-24 08:08:52,704 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 08:09:01,137 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-24 08:09:09,558 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2582\n",
            "2025-03-24 08:09:17,909 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2832\n",
            "2025-03-24 08:09:26,372 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2629\n",
            "2025-03-24 08:09:35,089 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2404\n",
            "2025-03-24 08:09:43,599 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2777\n",
            "2025-03-24 08:09:52,300 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2630\n",
            "2025-03-24 08:10:00,700 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2642\n",
            "2025-03-24 08:10:09,459 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2901\n",
            "2025-03-24 08:10:18,250 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2642\n",
            "2025-03-24 08:10:27,165 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-24 08:10:36,000 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-24 08:10:44,803 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-24 08:10:53,415 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3097\n",
            "2025-03-24 08:11:02,178 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2895\n",
            "2025-03-24 08:11:10,881 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2499\n",
            "2025-03-24 08:11:19,571 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2638\n",
            "2025-03-24 08:11:28,051 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-24 08:11:36,778 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-24 08:11:45,270 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2915\n",
            "2025-03-24 08:11:53,779 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2710\n",
            "2025-03-24 08:12:02,194 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2456\n",
            "2025-03-24 08:12:10,626 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-24 08:12:19,384 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2627\n",
            "2025-03-24 08:12:28,067 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2422\n",
            "2025-03-24 08:12:34,643 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1966\n",
            "2025-03-24 08:12:35,293 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0568\n",
            "2025-03-24 08:12:35,294 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 08:13:37,025 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2686, Val Loss: 0.4055, Val Acc: 0.8664\n",
            "2025-03-24 08:13:37,025 - INFO - [TRAIN INFO] ============================== Epoch 21/50 ==============================\n",
            "2025-03-24 08:13:43,593 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1882\n",
            "2025-03-24 08:13:52,374 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2618\n",
            "2025-03-24 08:14:00,799 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-24 08:14:09,184 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2776\n",
            "2025-03-24 08:14:17,622 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2785\n",
            "2025-03-24 08:14:26,110 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2558\n",
            "2025-03-24 08:14:35,000 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2747\n",
            "2025-03-24 08:14:43,614 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2622\n",
            "2025-03-24 08:14:52,365 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-24 08:15:01,125 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-24 08:15:09,811 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3019\n",
            "2025-03-24 08:15:18,587 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2704\n",
            "2025-03-24 08:15:27,364 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2427\n",
            "2025-03-24 08:15:36,131 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2489\n",
            "2025-03-24 08:15:44,961 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2700\n",
            "2025-03-24 08:15:53,557 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2639\n",
            "2025-03-24 08:16:01,969 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2350\n",
            "2025-03-24 08:16:10,355 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2574\n",
            "2025-03-24 08:16:18,983 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-24 08:16:27,697 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2767\n",
            "2025-03-24 08:16:36,584 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2702\n",
            "2025-03-24 08:16:45,220 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2694\n",
            "2025-03-24 08:16:53,974 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2585\n",
            "2025-03-24 08:17:02,677 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2443\n",
            "2025-03-24 08:17:11,420 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2734\n",
            "2025-03-24 08:17:19,974 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2566\n",
            "2025-03-24 08:17:28,761 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2575\n",
            "2025-03-24 08:17:37,348 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2630\n",
            "2025-03-24 08:17:45,918 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2520\n",
            "2025-03-24 08:17:54,523 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-24 08:18:03,233 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2499\n",
            "2025-03-24 08:18:12,149 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2779\n",
            "2025-03-24 08:18:20,786 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2735\n",
            "2025-03-24 08:18:27,461 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1892\n",
            "2025-03-24 08:18:28,158 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0891\n",
            "2025-03-24 08:18:28,159 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 08:19:29,431 - INFO - [TRAIN INFO] Epoch 21/50, Train Loss: 0.2655, Val Loss: 0.4139, Val Acc: 0.8669\n",
            "2025-03-24 08:19:29,432 - INFO - [TRAIN INFO] ============================== Epoch 22/50 ==============================\n",
            "2025-03-24 08:19:35,994 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1895\n",
            "2025-03-24 08:19:44,666 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2860\n",
            "2025-03-24 08:19:53,386 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2826\n",
            "2025-03-24 08:20:02,080 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-24 08:20:10,859 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2468\n",
            "2025-03-24 08:20:19,494 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2339\n",
            "2025-03-24 08:20:28,271 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2696\n",
            "2025-03-24 08:20:36,920 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2442\n",
            "2025-03-24 08:20:46,301 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2916\n",
            "2025-03-24 08:20:54,692 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2514\n",
            "2025-03-24 08:21:03,087 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2607\n",
            "2025-03-24 08:21:11,469 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2683\n",
            "2025-03-24 08:21:19,859 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2544\n",
            "2025-03-24 08:21:28,470 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2445\n",
            "2025-03-24 08:21:36,885 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-24 08:21:45,269 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2519\n",
            "2025-03-24 08:21:53,673 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-24 08:22:02,062 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2421\n",
            "2025-03-24 08:22:10,471 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-24 08:22:18,874 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-24 08:22:27,440 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-24 08:22:35,856 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2474\n",
            "2025-03-24 08:22:44,282 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2518\n",
            "2025-03-24 08:22:52,703 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-24 08:23:01,104 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2769\n",
            "2025-03-24 08:23:09,480 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-24 08:23:17,887 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-24 08:23:26,641 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2410\n",
            "2025-03-24 08:23:35,037 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-24 08:23:43,434 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2547\n",
            "2025-03-24 08:23:51,636 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-24 08:24:00,083 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2647\n",
            "2025-03-24 08:24:08,639 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2522\n",
            "2025-03-24 08:24:14,799 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2087\n",
            "2025-03-24 08:24:15,441 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0924\n",
            "2025-03-24 08:24:15,442 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 08:25:15,767 - INFO - [TRAIN INFO] Epoch 22/50, Train Loss: 0.2635, Val Loss: 0.4119, Val Acc: 0.8729\n",
            "2025-03-24 08:25:15,768 - INFO - [TRAIN INFO] ============================== Epoch 23/50 ==============================\n",
            "2025-03-24 08:25:22,112 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1866\n",
            "2025-03-24 08:25:30,684 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-24 08:25:39,211 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2728\n",
            "2025-03-24 08:25:47,732 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-24 08:25:55,306 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2486\n",
            "2025-03-24 08:26:03,004 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2486\n",
            "2025-03-24 08:26:10,600 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2587\n",
            "2025-03-24 08:26:18,609 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2762\n",
            "2025-03-24 08:26:26,202 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2676\n",
            "2025-03-24 08:26:33,996 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 08:26:42,591 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2354\n",
            "2025-03-24 08:26:51,001 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2347\n",
            "2025-03-24 08:26:59,449 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2676\n",
            "2025-03-24 08:27:07,849 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-24 08:27:16,231 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2481\n",
            "2025-03-24 08:27:24,386 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2541\n",
            "2025-03-24 08:27:32,127 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2817\n",
            "2025-03-24 08:27:40,381 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2626\n",
            "2025-03-24 08:27:48,487 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2623\n",
            "2025-03-24 08:27:56,515 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2733\n",
            "2025-03-24 08:28:05,179 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2722\n",
            "2025-03-24 08:28:13,810 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2516\n",
            "2025-03-24 08:28:22,395 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2753\n",
            "2025-03-24 08:28:30,971 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2618\n",
            "2025-03-24 08:28:38,783 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2552\n",
            "2025-03-24 08:28:46,346 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2527\n",
            "2025-03-24 08:28:54,398 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2750\n",
            "2025-03-24 08:29:02,725 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2497\n",
            "2025-03-24 08:29:11,345 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2659\n",
            "2025-03-24 08:29:19,892 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2653\n",
            "2025-03-24 08:29:28,529 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2536\n",
            "2025-03-24 08:29:37,130 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-24 08:29:45,300 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2789\n",
            "2025-03-24 08:29:51,540 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2091\n",
            "2025-03-24 08:29:52,185 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0629\n",
            "2025-03-24 08:29:52,185 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 08:30:53,014 - INFO - [TRAIN INFO] Epoch 23/50, Train Loss: 0.2623, Val Loss: 0.4150, Val Acc: 0.8692\n",
            "2025-03-24 08:30:53,015 - INFO - [TRAIN INFO] Early stopping at epoch 23 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 08:30:53,015 - INFO - [TRAIN INFO] Total Time: 7984.41s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▁▂▃▃▄▅▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▂▃▄▄▅▆▇███▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▂▃▄▄▅▆▇███▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▂▃▄▄▅▆▇███▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▂▃▄▄▅▆▇███▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▇▆▆▅▅▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇▇████▇█████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>23</td></tr><tr><td>learning_rate_classifier</td><td>0.00014</td></tr><tr><td>learning_rate_fusion</td><td>3e-05</td></tr><tr><td>learning_rate_image</td><td>3e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.26234</td></tr><tr><td>train_val_loss_diff</td><td>-0.15265</td></tr><tr><td>val_accuracy</td><td>0.86918</td></tr><tr><td>val_loss</td><td>0.41499</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_transformer_fusion_fold_5</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ypxz0dc8' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/ypxz0dc8</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_061747-ypxz0dc8\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 08:30:55,350 - INFO - [TRAIN INFO] Fold 5 Training Complete at epoch 23. Total Time: 7986.74s\n",
            "2025-03-24 08:30:55,368 - INFO - [K-FOLD INFO] Fold 5 completed in 7988.14 seconds\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "logging.info(\"[K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "\n",
        "    if fold < 1:\n",
        "        continue\n",
        "\n",
        "    fold_start_time = time.time()  # Start timing for this fold\n",
        "    logging.info(f\"[K-FOLD INFO] ============================== Fold {fold+1}/{K_FOLDS} ==============================\")\n",
        "\n",
        "    # Get train and validation subsets\n",
        "    train_texts_fold = train_texts[train_idx]\n",
        "    val_texts_fold = train_texts[val_idx]\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "    train_image_paths_fold = train_image_paths[train_idx]\n",
        "    val_image_paths_fold = train_image_paths[val_idx]\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train Samples: {len(train_texts_fold)}\")\n",
        "    logging.info(f\"   Validation Samples: {len(val_texts_fold)}\")\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_image_dataset = ImageDataset(train_image_paths_fold, train_labels_fold, transform[\"train\"])\n",
        "    val_image_dataset = ImageDataset(val_image_paths_fold, val_labels_fold, transform[\"val\"])\n",
        "    \n",
        "    train_text_dataset = CustomTextDataset(train_texts_fold, train_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "    val_text_dataset = CustomTextDataset(val_texts_fold, val_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # Create multimodal datasets\n",
        "    train_multimodal_dataset = MultimodalDataset(train_image_dataset, train_text_dataset)\n",
        "    val_multimodal_dataset = MultimodalDataset(val_image_dataset, val_text_dataset)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Created multimodal datasets for Fold {fold+1}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    dataloaders = {\n",
        "        \"train_loader\": DataLoader(train_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=True),\n",
        "        \"val_loader\": DataLoader(val_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    }\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] DataLoaders initialized for Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train batches: {len(dataloaders['train_loader'])}, Validation batches: {len(dataloaders['val_loader'])}\")\n",
        "\n",
        "    # Initialize model, optimizer, and criterion\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Model initialized on {device} for Fold {fold+1}\")\n",
        "\n",
        "    # Define Optimizer using AdamW\n",
        "    optimizer = optim.AdamW([\n",
        "        {\"params\": model.image_model.features[-3:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_IMAGE, \"weight_decay\": WEIGHT_DECAY_IMAGE},  # Unfrozen EfficientNet layer\n",
        "        {\"params\": model.text_model.transformer.layer[-2:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_TEXT, \"weight_decay\": WEIGHT_DECAY_TEXT},  # Unfrozen DistilBERT layer\n",
        "        {\"params\": model.image_fc.parameters(), \"lr\": LEARNING_RATE_IMAGE, \"weight_decay\": 0}, \n",
        "        {\"params\": model.text_fc.parameters(), \"lr\": LEARNING_RATE_TEXT, \"weight_decay\": 0},\n",
        "        {\"params\": model.fusion_fc.parameters(), \"lr\": LEARNING_RATE_FUSION, \"weight_decay\": WEIGHT_DECAY_FUSION},  \n",
        "        {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER, \"weight_decay\": WEIGHT_DECAY_CLASSIFIER}  \n",
        "    ], betas=(0.9, 0.999), eps=1e-8)  # Default AdamW betas and eps\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Optimizer initialized for Fold {fold+1}:\")\n",
        "    \n",
        "    # Define Loss Function\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING_PREDICTION) \n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Loss function initialized for Fold {fold+1}\")\n",
        "\n",
        "    # Train model for this fold\n",
        "    train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True)\n",
        "\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Measure Fold Time\n",
        "    fold_time = time.time() - fold_start_time\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1} completed in {fold_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for fold in range(K_FOLDS):\n",
        "#     logging.info(f\"\\n[TEST INFO] Evaluating Fold {fold + 1} on Test Set...\")\n",
        "\n",
        "#     # Load best model for the fold\n",
        "#     model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "#     model_path = f\"best_model_fold_{fold + 1}.pth\"\n",
        "    \n",
        "#     try:\n",
        "#         model.load_state_dict(torch.load(model_path))\n",
        "#         logging.info(f\"[TEST INFO] Loaded best model for Fold {fold + 1} from {model_path}\")\n",
        "#     except FileNotFoundError:\n",
        "#         logging.error(f\"[ERROR] Model file {model_path} not found! Skipping Fold {fold + 1} evaluation.\")\n",
        "#         continue  # Skip to the next fold if model file is missing\n",
        "\n",
        "#     model.eval()  # Set to evaluation mode\n",
        "\n",
        "#     # Evaluate model on test data\n",
        "#     test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "\n",
        "#     # Log test set performance for the fold\n",
        "#     logging.info(f\"[TEST INFO] Fold {fold + 1} Test Performance:\")\n",
        "#     logging.info(f\"   Test Loss: {test_loss:.4f}\")\n",
        "#     logging.info(f\"   Test Accuracy: {test_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "enel645_torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
