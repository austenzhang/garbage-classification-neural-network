{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLOOi0GQM5B"
      },
      "source": [
        "## Garbage Classification Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
        "from torchvision.models.efficientnet import EfficientNet_B0_Weights\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import wandb\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import time\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "NOTES = '''\n",
        "'''\n",
        "\n",
        "# ========================================= GLOBAL CONFIGURATION ================================================\n",
        "# Data Directories\n",
        "DATA_DIR = r\"C:\\NN Data\\garbage_data\\kfold_garbage_data\"\n",
        "CLASSES = [\"Black\", \"Blue\", \"Green\", \"TTR\"]\n",
        "\n",
        "# ========================================= Experiment Settings =========================================\n",
        "WANDB_RUN_NAME = \"experiment_multimodal_attention_only\"\n",
        "MODEL_NAME = \"experiment_multimodal_attention_only\"\n",
        "\n",
        "# ========================================= Data Settings =========================================\n",
        "IMAGE_SIZE = (224, 224)  # Input image size for EfficientNetV2-S\n",
        "NUM_CLASSES = 4  # Number of output classes for classification\n",
        "MAX_LEN = 40  # Maximum token length for DistilBERT tokenizer\n",
        "TEST_SIZE = 0.2  # Test dataset size split\n",
        "K_FOLDS = 5  # Number of folds for stratified k-fold cross-validation\n",
        "\n",
        "# ========================================= Training Hyperparameters =========================================\n",
        "BATCH_SIZE = 64  # Number of samples per batch\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "EPOCHS = 50  # Maximum number of training epochs\n",
        "DROPOUT_IMAGE = 0.2 # Reduce from 0.3\n",
        "DROPOUT_TEXT = 0.1 # Reduce from 0.2\n",
        "DROPOUT_FUSION = 0.2 \n",
        "DROPOUT_CLASSIFIER = 0.1\n",
        "PATIENCE = 10  # Number of epochs to wait before early stopping\n",
        "CONVERGENCE_THRESHOLD = 0.001  # Minimum improvement in validation loss to continue training\n",
        "\n",
        "# ========================================= Optimization Settings =========================================\n",
        "OPTIMIZER = \"AdamW\"\n",
        "LR_SCHEDULING_FACTOR = 0.3\n",
        "LEARNING_RATE_UNFREEZE_IMAGE = 1e-5\n",
        "LEARNING_RATE_UNFREEZE_TEXT = 1e-5\n",
        "LEARNING_RATE_FUSION = 1e-3\n",
        "LEARNING_RATE_CLASSIFIER = 5e-3\n",
        "LEARNING_RATE_IMAGE = 0.001 # # EfficientNetB0\n",
        "LEARNING_RATE_TEXT = 0.00002 # DistilBERT Uncased\n",
        "WEIGHT_DECAY_TEXT = 1e-3  # Reduce from 1e-2\n",
        "WEIGHT_DECAY_IMAGE = 1e-4  # Reduce from 1e-3\n",
        "WEIGHT_DECAY_FUSION = 4e-4 \n",
        "WEIGHT_DECAY_CLASSIFIER = 1e-3  # Reduce from 1e-4\n",
        "LABEL_SMOOTHING_PREDICTION = 0.05 # Reduce from 0.1\n",
        "\n",
        "# ========================================= System Settings =========================================\n",
        "NUM_WORKERS = 4  # Dataloader parallelization\n",
        "\n",
        "# Wandb Configuration\n",
        "WANDB_CONFIG = {\n",
        "    \"entity\": \"shcau-university-of-calgary-in-alberta\",\n",
        "    \"project\": \"transfer_learning_garbage\",\n",
        "    \"name\": WANDB_RUN_NAME,\n",
        "    \"tags\": [\"distilBERT\", \"efficientnet\", \"CVPR_2024_dataset\"],\n",
        "    \"notes\": NOTES,\n",
        "    \"config\": {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"dataset\": \"CVPR_2024_dataset\",\n",
        "        \"image_size\": IMAGE_SIZE,\n",
        "        \"num_workers\": NUM_WORKERS,\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"max_len\": MAX_LEN,\n",
        "        \"learning_rate_image\": LEARNING_RATE_IMAGE,\n",
        "        \"learning_rate_text\": LEARNING_RATE_TEXT,\n",
        "        \"learning_rate_fusion\": LEARNING_RATE_FUSION,\n",
        "        \"learning_rate_classifier\": LEARNING_RATE_CLASSIFIER,\n",
        "        \"learning_rate_unfreeze_image\": LEARNING_RATE_UNFREEZE_IMAGE, # learning rate for unfrozen EfficientNet layers\n",
        "        \"learning_rate_unfreeze_text\": LEARNING_RATE_UNFREEZE_TEXT, # learning rate for unfrozen DistilBERT layers\n",
        "        \"dropout_image\": DROPOUT_IMAGE,\n",
        "        \"dropout_text\": DROPOUT_TEXT,\n",
        "        \"dropout_classifier\": DROPOUT_CLASSIFIER,\n",
        "        \"convergence_threshold\": CONVERGENCE_THRESHOLD,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"weight_decay_text\": WEIGHT_DECAY_TEXT,\n",
        "        \"weight_decay_image\": WEIGHT_DECAY_IMAGE,\n",
        "        \"weight_decay_classifier\": WEIGHT_DECAY_CLASSIFIER,\n",
        "        \"label_smoothing_prediction\": LABEL_SMOOTHING_PREDICTION,\n",
        "        \"optimizer\": OPTIMIZER \n",
        "    },\n",
        "    \"job_type\": \"train\",\n",
        "    \"resume\": \"allow\",\n",
        "}\n",
        "\n",
        "# Normalization Stats\n",
        "NORMALIZATION_STATS = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOG_FILE = \"experiment_multimodal_attention_only.txt\"  # Log file name\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Log everything (INFO and above)\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(LOG_FILE, mode='w'),  # Overwrite log file on each run\n",
        "        logging.StreamHandler(sys.stdout)  # Print log messages to console too\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:13:33,598 - INFO - [CONFIG] ============================== Experiment Configuration ==============================\n",
            "2025-03-25 09:13:33,599 - INFO - [CONFIG] Experiment Name: experiment_multimodal_attention_only\n",
            "2025-03-25 09:13:33,599 - INFO - [CONFIG] Entity: shcau-university-of-calgary-in-alberta\n",
            "2025-03-25 09:13:33,600 - INFO - [CONFIG] Project: transfer_learning_garbage\n",
            "2025-03-25 09:13:33,601 - INFO - [CONFIG] Tags: distilBERT, efficientnet, CVPR_2024_dataset\n",
            "2025-03-25 09:13:33,602 - INFO - [CONFIG] Notes: \n",
            "\n",
            "2025-03-25 09:13:33,602 - INFO - [CONFIG] Job Type: train\n",
            "2025-03-25 09:13:33,603 - INFO - [CONFIG] Resume: allow\n",
            "2025-03-25 09:13:33,604 - INFO - [CONFIG] ------------------------------ Hyperparameters ------------------------------\n",
            "2025-03-25 09:13:33,604 - INFO - [CONFIG] epochs: 50\n",
            "2025-03-25 09:13:33,605 - INFO - [CONFIG] batch_size: 64\n",
            "2025-03-25 09:13:33,605 - INFO - [CONFIG] dataset: CVPR_2024_dataset\n",
            "2025-03-25 09:13:33,606 - INFO - [CONFIG] image_size: (224, 224)\n",
            "2025-03-25 09:13:33,606 - INFO - [CONFIG] num_workers: 4\n",
            "2025-03-25 09:13:33,607 - INFO - [CONFIG] num_classes: 4\n",
            "2025-03-25 09:13:33,607 - INFO - [CONFIG] max_len: 40\n",
            "2025-03-25 09:13:33,608 - INFO - [CONFIG] learning_rate_image: 0.001\n",
            "2025-03-25 09:13:33,609 - INFO - [CONFIG] learning_rate_text: 2e-05\n",
            "2025-03-25 09:13:33,609 - INFO - [CONFIG] learning_rate_fusion: 0.001\n",
            "2025-03-25 09:13:33,610 - INFO - [CONFIG] learning_rate_classifier: 0.005\n",
            "2025-03-25 09:13:33,611 - INFO - [CONFIG] learning_rate_unfreeze_image: 1e-05\n",
            "2025-03-25 09:13:33,611 - INFO - [CONFIG] learning_rate_unfreeze_text: 1e-05\n",
            "2025-03-25 09:13:33,612 - INFO - [CONFIG] dropout_image: 0.2\n",
            "2025-03-25 09:13:33,612 - INFO - [CONFIG] dropout_text: 0.1\n",
            "2025-03-25 09:13:33,613 - INFO - [CONFIG] dropout_classifier: 0.1\n",
            "2025-03-25 09:13:33,614 - INFO - [CONFIG] convergence_threshold: 0.001\n",
            "2025-03-25 09:13:33,614 - INFO - [CONFIG] patience: 10\n",
            "2025-03-25 09:13:33,615 - INFO - [CONFIG] weight_decay_text: 0.001\n",
            "2025-03-25 09:13:33,615 - INFO - [CONFIG] weight_decay_image: 0.0001\n",
            "2025-03-25 09:13:33,616 - INFO - [CONFIG] weight_decay_classifier: 0.001\n",
            "2025-03-25 09:13:33,617 - INFO - [CONFIG] label_smoothing_prediction: 0.05\n",
            "2025-03-25 09:13:33,617 - INFO - [CONFIG] optimizer: AdamW\n",
            "2025-03-25 09:13:33,618 - INFO - [CONFIG] =============================================================================\n"
          ]
        }
      ],
      "source": [
        "# Log the configuration\n",
        "logging.info(\"[CONFIG] ============================== Experiment Configuration ==============================\")\n",
        "\n",
        "# Log top-level keys\n",
        "logging.info(f\"[CONFIG] Experiment Name: {WANDB_CONFIG['name']}\")\n",
        "logging.info(f\"[CONFIG] Entity: {WANDB_CONFIG['entity']}\")\n",
        "logging.info(f\"[CONFIG] Project: {WANDB_CONFIG['project']}\")\n",
        "logging.info(f\"[CONFIG] Tags: {', '.join(WANDB_CONFIG['tags'])}\")\n",
        "logging.info(f\"[CONFIG] Notes: {WANDB_CONFIG['notes']}\")\n",
        "logging.info(f\"[CONFIG] Job Type: {WANDB_CONFIG['job_type']}\")\n",
        "logging.info(f\"[CONFIG] Resume: {WANDB_CONFIG['resume']}\")\n",
        "\n",
        "# Log nested configuration (under 'config')\n",
        "logging.info(\"[CONFIG] ------------------------------ Hyperparameters ------------------------------\")\n",
        "for key, value in WANDB_CONFIG[\"config\"].items():\n",
        "    logging.info(f\"[CONFIG] {key}: {value}\")\n",
        "\n",
        "logging.info(\"[CONFIG] =============================================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_wandb(fold):\n",
        "    \"\"\"Initialize wandb for each fold with a unique run name.\"\"\"\n",
        "    wandb.init(\n",
        "        entity=WANDB_CONFIG[\"entity\"],\n",
        "        project=WANDB_CONFIG[\"project\"],\n",
        "        name=f\"{WANDB_RUN_NAME}_fold_{fold + 1}\",\n",
        "        tags=WANDB_CONFIG[\"tags\"],\n",
        "        notes=WANDB_CONFIG[\"notes\"],\n",
        "        config=WANDB_CONFIG[\"config\"],\n",
        "        job_type=WANDB_CONFIG[\"job_type\"],\n",
        "        resume=WANDB_CONFIG[\"resume\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SpaCy for lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load NLTK stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Standardize text, remove stopwords, and apply lemmatization.\"\"\"\n",
        "    # 1. Standardize text (lowercasing & trimming spaces)\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # 2. Remove stopwords\n",
        "    text_tokens = text.split()\n",
        "    text = \" \".join([word for word in text_tokens if word not in stop_words])\n",
        "\n",
        "    # 3. Lemmatization\n",
        "    doc = nlp(text)\n",
        "    text = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "    return text\n",
        "\n",
        "def read_text_files_with_labels_and_image_paths(path):\n",
        "    \"\"\"Extract text from file names, apply preprocessing, and return labels with image paths.\"\"\"\n",
        "    texts, labels, image_paths = [], [], []\n",
        "    class_folders = sorted(os.listdir(path))\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = sorted(os.listdir(class_path))  # Sort to ensure order consistency\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    # Extract filename without extension\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "\n",
        "                    # Replace underscores with spaces\n",
        "                    text = file_name_no_ext.replace(\"_\", \" \")\n",
        "\n",
        "                    # Remove numbers\n",
        "                    text_without_digits = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "                    # Apply preprocessing\n",
        "                    preprocessed_text = preprocess_text(text_without_digits)\n",
        "\n",
        "                    texts.append(preprocessed_text)\n",
        "                    labels.append(label_map[class_name])\n",
        "                    image_paths.append(file_path)\n",
        "\n",
        "    return np.array(texts), np.array(labels), np.array(image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    \"\"\"Dataset class for text data.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "    \n",
        "# Custom dataset class for images\n",
        "class ImageDataset(Dataset):\n",
        "    \"\"\"Dataset class for image data.\"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    \"\"\"Dataset class for multimodal data (image + text).\"\"\"\n",
        "    def __init__(self, image_dataset, text_dataset):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_dataset), len(self.text_dataset))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.image_dataset[idx]\n",
        "        text_data = self.text_dataset[idx]\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": text_data[\"input_ids\"],\n",
        "            \"attention_mask\": text_data[\"attention_mask\"],\n",
        "            \"label\": label\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================== Gated Fusion ========================\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(GatedFusion, self).__init__()\n",
        "        self.gate = nn.Linear(2 * feature_dim, feature_dim)  # Learnable gate\n",
        "        self.sigmoid = nn.Sigmoid()  # Activation\n",
        "\n",
        "    def forward(self, text_feat, image_feat):\n",
        "        combined_feat = torch.cat((text_feat, image_feat), dim=1)\n",
        "        gate_value = self.sigmoid(self.gate(combined_feat))  # Value between 0-1\n",
        "        fused_feat = (gate_value * text_feat) + ((1 - gate_value) * image_feat)  # Weighted fusion\n",
        "        return fused_feat\n",
        "\n",
        "class AttentionFusion(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(AttentionFusion, self).__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        # For cross-modal attention\n",
        "        self.Wq_text = nn.Linear(feature_dim, feature_dim)  # Query from text\n",
        "        self.Wk_image = nn.Linear(feature_dim, feature_dim)  # Key from image\n",
        "        self.Wv_image = nn.Linear(feature_dim, feature_dim)  # Value from image\n",
        "        \n",
        "        self.Wq_image = nn.Linear(feature_dim, feature_dim)  # Query from image\n",
        "        self.Wk_text = nn.Linear(feature_dim, feature_dim)  # Key from text\n",
        "        self.Wv_text = nn.Linear(feature_dim, feature_dim)  # Value from text\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.layer_norm = nn.LayerNorm(feature_dim)\n",
        "\n",
        "    def forward(self, text_feat, image_feat):\n",
        "        # Text-to-image attention\n",
        "        q_text = self.Wq_text(text_feat).unsqueeze(1)  # [batch, 1, dim]\n",
        "        k_image = self.Wk_image(image_feat).unsqueeze(2)  # [batch, dim, 1]\n",
        "        v_image = self.Wv_image(image_feat)  # [batch, dim]\n",
        "        \n",
        "        # Image-to-text attention\n",
        "        q_image = self.Wq_image(image_feat).unsqueeze(1)  # [batch, 1, dim]\n",
        "        k_text = self.Wk_text(text_feat).unsqueeze(2)  # [batch, dim, 1]\n",
        "        v_text = self.Wv_text(text_feat)  # [batch, dim]\n",
        "        \n",
        "        # Compute attention scores\n",
        "        text_attention = self.softmax(torch.bmm(q_text, k_image).squeeze(2))  # [batch, 1]\n",
        "        image_attention = self.softmax(torch.bmm(q_image, k_text).squeeze(2))  # [batch, 1]\n",
        "        \n",
        "        # Apply attention\n",
        "        text_out = text_attention * v_image + text_feat  # [batch, dim]\n",
        "        image_out = image_attention * v_text + image_feat  # [batch, dim]\n",
        "        \n",
        "        # Combine and normalize\n",
        "        fused_feat = self.layer_norm(text_out + image_out)\n",
        "        return fused_feat\n",
        "\n",
        "# ======================== Multimodal Classifier (Last Feature Extractor Layer Unfrozen) ========================\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    \"\"\"Multimodal model combining EfficientNetB0 and DistilBERT with partial fine-tuning.\"\"\"\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # ----------- Image Feature Extractor (EfficientNetB0) -----------\n",
        "        self.image_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        \n",
        "        # Freeze all layers except the last one\n",
        "        for param in self.image_model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.image_model.features[-3:].parameters():  # Unfreeze last feature layer\n",
        "            param.requires_grad = True\n",
        "\n",
        "        num_ftrs = self.image_model.classifier[1].in_features\n",
        "        self.image_model.classifier = nn.Identity()  # Remove classifier\n",
        "        self.image_fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_IMAGE)\n",
        "        )\n",
        "\n",
        "        # ----------- Text Feature Extractor (DistilBERT) -----------\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # Freeze all layers except the last transformer layer\n",
        "        for param in self.text_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.text_model.transformer.layer[-2:].parameters():  # Unfreeze last transformer layer\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(self.text_model.config.hidden_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_TEXT)\n",
        "        )\n",
        "\n",
        "        # ----------- Normalize Features -----------\n",
        "        self.text_norm = nn.LayerNorm(512)\n",
        "        self.image_norm = nn.LayerNorm(512)\n",
        "\n",
        "        # ----------- Attention Fusion -----------\n",
        "        self.attention_fusion = AttentionFusion(feature_dim=512)\n",
        "\n",
        "        # ----------- Fully Connected Fusion & Classification -----------\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512, 512),  # Increase dimension\n",
        "            nn.BatchNorm1d(512),  # Add batch normalization\n",
        "            nn.ReLU(),            # Use GELU activation\n",
        "\n",
        "            nn.Linear(512, 256),  # Intermediate layer\n",
        "            nn.BatchNorm1d(256),  # Batch normalization\n",
        "            nn.ReLU(),            # GELU activation\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT_CLASSIFIER)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, image_inputs):\n",
        "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(text_output.last_hidden_state[:, 0, :])\n",
        "        text_features = self.text_norm(text_features)\n",
        "        image_features = self.image_fc(self.image_model(image_inputs))\n",
        "        image_features = self.image_norm(image_features)\n",
        "        attention_feat = self.attention_fusion(text_features, image_features)\n",
        "        fused_features = self.fusion_fc(attention_feat)\n",
        "        output = self.classifier(self.dropout(fused_features))\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:14:00,499 - INFO - First 4 samples of dataset:\n",
            "\n",
            "2025-03-25 09:14:00,499 - INFO - Texts: ['aero bar wrapper' 'break glass' 'break rubber' 'butter paper']\n",
            "2025-03-25 09:14:00,500 - INFO - Labels: [0 0 0 0]\n",
            "2025-03-25 09:14:00,501 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Aero_bar_wrapper_1.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_Glass_5291.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_rubber_7263.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Butter_Paper_9976.png']\n",
            "2025-03-25 09:14:00,501 - INFO - \n",
            "Last 4 samples of dataset:\n",
            "\n",
            "2025-03-25 09:14:00,502 - INFO - Texts: ['wristwatch' 'xbox controller' 'xbox one controller' 'zipper file bag']\n",
            "2025-03-25 09:14:00,503 - INFO - Labels: [3 3 3 3]\n",
            "2025-03-25 09:14:00,503 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\wristwatch_3782.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_controller_2047.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_one_controller_2048.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\zipper_file_bag_2049.png']\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "texts, labels, image_paths = read_text_files_with_labels_and_image_paths(DATA_DIR)\n",
        "\n",
        "# Log first and last 4 samples\n",
        "logging.info(\"First 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[:4]}\")\n",
        "logging.info(f\"Labels: {labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[-4:]}\")\n",
        "logging.info(f\"Labels: {labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split into test set and development set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:14:00,531 - INFO - First 4 samples of test set:\n",
            "\n",
            "2025-03-25 09:14:00,531 - INFO - Texts: ['ballast light' 'old phone' 'milk jug lid tab' 'dirty dish sponge']\n",
            "2025-03-25 09:14:00,532 - INFO - Labels: [3 3 0 0]\n",
            "2025-03-25 09:14:00,533 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\ballast_light_286.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\Old_Phones_7828.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\milk_jug_lid_tab_1137.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\dirty_dish_sponge_437.png']\n",
            "2025-03-25 09:14:00,534 - INFO - \n",
            "Last 4 samples of test set:\n",
            "\n",
            "2025-03-25 09:14:00,534 - INFO - Texts: ['empty glass jar' 'non - stretchy plastic' 'backpack' 'piece break glass']\n",
            "2025-03-25 09:14:00,535 - INFO - Labels: [1 0 3 0]\n",
            "2025-03-25 09:14:00,536 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Blue\\\\empty_glass_jar_1609.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\non-stretchy plastic.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\backpack_216.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\piece_of_broken_glass_1315.png']\n"
          ]
        }
      ],
      "source": [
        "# Split into a test set and development set\n",
        "train_texts, test_texts, train_labels, test_labels, train_image_paths, test_image_paths = train_test_split(\n",
        "    texts, labels, image_paths, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Log first 4 samples of test set\n",
        "logging.info(\"First 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[:4]}\")\n",
        "logging.info(f\"Labels: {test_labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[-4:]}\")\n",
        "logging.info(f\"Labels: {test_labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE), \n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Apply correct normalization\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Only resize + normalize\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Only resize + normalize\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Tokenizer for DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoader for test set\n",
        "\n",
        "Create the dataloader for the test set and set aside for model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test dataset\n",
        "test_image_dataset = ImageDataset(test_image_paths, test_labels, transform[\"test\"])\n",
        "test_text_dataset = CustomTextDataset(test_texts, test_labels, tokenizer, max_len=MAX_LEN)  # Ensure tokenizer is defined\n",
        "test_multimodal_dataset = MultimodalDataset(test_image_dataset, test_text_dataset)\n",
        "\n",
        "# DataLoader for test set\n",
        "test_loader = DataLoader(test_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a peek at a batch in the test set to verify that data has been correctly organized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:14:02,330 - INFO - [INFO] One Batch Sample Inspection:\n",
            "2025-03-25 09:14:02,331 - INFO -    Images Shape: torch.Size([64, 3, 224, 224])\n",
            "2025-03-25 09:14:02,331 - INFO -    Input IDs Shape: torch.Size([64, 40])\n",
            "2025-03-25 09:14:02,332 - INFO -    Attention Mask Shape: torch.Size([64, 40])\n",
            "2025-03-25 09:14:02,333 - INFO -    Labels Shape: torch.Size([64])\n",
            "2025-03-25 09:14:02,333 - INFO - \n",
            "[INFO] First Sample:\n",
            "2025-03-25 09:14:02,338 - INFO -    Image Tensor: tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         ...,\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
            "\n",
            "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         ...,\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
            "\n",
            "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         ...,\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]])\n",
            "2025-03-25 09:14:02,339 - INFO -    Input IDs: tensor([  101, 28030,  2422,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "2025-03-25 09:14:02,340 - INFO -    Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "2025-03-25 09:14:02,341 - INFO -    Label: 3\n"
          ]
        }
      ],
      "source": [
        "# Get one batch\n",
        "for batch in test_loader:\n",
        "    images = batch[\"image\"]  # Image tensor\n",
        "    input_ids = batch[\"input_ids\"]  # Tokenized text tensor\n",
        "    attention_mask = batch[\"attention_mask\"]  # Attention mask\n",
        "    labels = batch[\"label\"]  # Labels tensor\n",
        "\n",
        "    # Log shapes of tensors\n",
        "    logging.info(\"[INFO] One Batch Sample Inspection:\")\n",
        "    logging.info(f\"   Images Shape: {images.shape}\")\n",
        "    logging.info(f\"   Input IDs Shape: {input_ids.shape}\")\n",
        "    logging.info(f\"   Attention Mask Shape: {attention_mask.shape}\")\n",
        "    logging.info(f\"   Labels Shape: {labels.shape}\")\n",
        "\n",
        "    # Log first sample details\n",
        "    logging.info(\"\\n[INFO] First Sample:\")\n",
        "    logging.info(f\"   Image Tensor: {images[0]}\")\n",
        "    logging.info(f\"   Input IDs: {input_ids[0]}\")\n",
        "    logging.info(f\"   Attention Mask: {attention_mask[0]}\")\n",
        "    logging.info(f\"   Label: {labels[0]}\")\n",
        "\n",
        "    break  # Stop after inspecting one batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Stratified K-Fold on the development set to split into train/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:14:02,424 - INFO - [INFO] Fold 1/5\n",
            "2025-03-25 09:14:02,425 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 09:14:02,427 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-25 09:14:02,428 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-25 09:14:02,429 - INFO - [INFO] Fold 2/5\n",
            "2025-03-25 09:14:02,430 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 09:14:02,432 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-25 09:14:02,433 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-25 09:14:02,434 - INFO - [INFO] Fold 3/5\n",
            "2025-03-25 09:14:02,435 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 09:14:02,436 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-25 09:14:02,437 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-25 09:14:02,438 - INFO - [INFO] Fold 4/5\n",
            "2025-03-25 09:14:02,438 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 09:14:02,440 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-25 09:14:02,441 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n",
            "2025-03-25 09:14:02,442 - INFO - [INFO] Fold 5/5\n",
            "2025-03-25 09:14:02,443 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 09:14:02,444 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-25 09:14:02,445 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    logging.info(f\"[INFO] Fold {fold + 1}/{K_FOLDS}\")\n",
        "\n",
        "    # Extract labels for current fold\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "\n",
        "    # Log class distributions\n",
        "    logging.info(\"[INFO] Class Distributions:\")\n",
        "    logging.info(f\"   Train Class Distribution: {Counter(train_labels_fold)}\")\n",
        "    logging.info(f\"   Validation Class Distribution: {Counter(val_labels_fold)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify k-fold was applied correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:14:02,468 - INFO - [INFO] No data leakage detected in Fold 1\n",
            "2025-03-25 09:14:02,470 - INFO - [INFO] No data leakage detected in Fold 2\n",
            "2025-03-25 09:14:02,471 - INFO - [INFO] No data leakage detected in Fold 3\n",
            "2025-03-25 09:14:02,472 - INFO - [INFO] No data leakage detected in Fold 4\n",
            "2025-03-25 09:14:02,474 - INFO - [INFO] No data leakage detected in Fold 5\n"
          ]
        }
      ],
      "source": [
        "# Ensure no data leakage in folds\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    train_set = set(train_idx)\n",
        "    val_set = set(val_idx)\n",
        "\n",
        "    # Check for intersection (should be empty)\n",
        "    intersection = train_set.intersection(val_set)\n",
        "    assert len(intersection) == 0, f\"Data leakage detected in Fold {fold + 1}\"\n",
        "\n",
        "    logging.info(f\"[INFO] No data leakage detected in Fold {fold + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "#     train_labels_fold = train_labels[train_idx]\n",
        "#     val_labels_fold = train_labels[val_idx]\n",
        "\n",
        "#     plt.figure(figsize=(10, 4))\n",
        "#     plt.hist(train_labels_fold, bins=len(set(train_labels)), alpha=0.6, label=\"Train\")\n",
        "#     plt.hist(val_labels_fold, bins=len(set(train_labels)), alpha=0.6, label=\"Validation\")\n",
        "#     plt.title(f\"Class Distribution in Fold {fold + 1}\")\n",
        "#     plt.legend()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Move data to the appropriate device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)  # Compute batch loss\n",
        "\n",
        "            # Aggregate loss for averaging\n",
        "            total_loss += loss.item() * labels.size(0)  # Multiply by batch size for proper averaging\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total  # Normalize loss over total samples\n",
        "    accuracy = correct / total  # Compute accuracy\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Weight Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adaptive_weight_decay(epoch, warmup_epochs=5, decay_factors=(0.1, 1.0)):\n",
        "    \"\"\"\n",
        "    Returns a scaled weight decay based on epoch number.\n",
        "    During warm-up, it applies a lower decay (decay_factors[0]).\n",
        "    After warm-up, it applies full weight decay (decay_factors[1]).\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return decay_factors[0]  # Use lower decay during warm-up\n",
        "    return decay_factors[1]  # Use normal decay afterward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_warmup_lr(epoch, warmup_epochs, base_lr):\n",
        "    \"\"\"\n",
        "    Linear warmup schedule for the learning rate.\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return base_lr * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        return base_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True):\n",
        "    initialize_wandb(fold)\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    best_val_loss = float(\"inf\")  # Track best validation loss\n",
        "    epochs_without_improvement = 0  # Track epochs without improvement until equals patience\n",
        "\n",
        "    # ================ ReduceLROnPlateau Scheduler ================\n",
        "    plateau_scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=LR_SCHEDULING_FACTOR, patience=3, verbose=True\n",
        "    )\n",
        "    \n",
        "    # AMP GradScaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    epoch_start_time = time.time()  # Start total training timer\n",
        "    logging.info(\"[TRAIN INFO] Starting Training...\")\n",
        "\n",
        "    # Warmup settings\n",
        "    WARMUP_EPOCHS = 8  # Number of epochs for warmup\n",
        "    base_lr_image = LEARNING_RATE_IMAGE  # Base learning rate for EfficientNet\n",
        "    base_lr_text = LEARNING_RATE_TEXT  # Base learning rate for DistilBERT\n",
        "    base_lr_fusion = LEARNING_RATE_FUSION  # Base learning rate for fusion layer\n",
        "    base_lr_classifier = LEARNING_RATE_CLASSIFIER  # Base learning rate for classifier\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        logging.info(f\"[TRAIN INFO] ============================== Epoch {epoch + 1}/{EPOCHS} ==============================\")\n",
        "        \n",
        "        # Apply learning rate warmup\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            warmup_lr_image = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_image)\n",
        "            warmup_lr_text = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_text)\n",
        "            warmup_lr_fusion = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_fusion)\n",
        "            warmup_lr_classifier = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_classifier)\n",
        "\n",
        "            # Update learning rates for each parameter group\n",
        "            optimizer.param_groups[0][\"lr\"] = warmup_lr_image  # Unfrozen EfficientNet layer\n",
        "            optimizer.param_groups[1][\"lr\"] = warmup_lr_text  # Unfrozen DistilBERT layer\n",
        "            optimizer.param_groups[2][\"lr\"] = warmup_lr_image  # Image FC layer\n",
        "            optimizer.param_groups[3][\"lr\"] = warmup_lr_text  # Text FC layer\n",
        "            optimizer.param_groups[4][\"lr\"] = warmup_lr_fusion  # Fusion layer\n",
        "            optimizer.param_groups[5][\"lr\"] = warmup_lr_classifier  # Classifier layer\n",
        "\n",
        "        model.train()  # Set model to training modes\n",
        "        total_train_loss = 0  # Track total training loss for the epoch\n",
        "        batch_train_loss = 0  # Track batch loss for gradient accumulation\n",
        "        step = 0  # Track the number of batches processed\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Training phase\n",
        "        for step, batch in enumerate(dataloaders[\"train_loader\"], 1):\n",
        "            # Move data to device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask, images)  # Send inputs to network and receive outputs\n",
        "                loss = criterion(outputs, labels) / GRAD_ACCUM_STEPS  # Compute loss (no normalization for gradient accumulation)\n",
        "\n",
        "            # Backward pass and optimizer step\n",
        "            scaler.scale(loss).backward()  # Scale loss and backpropagate\n",
        "\n",
        "            batch_train_loss += loss.item()\n",
        "            total_train_loss += loss.item() * GRAD_ACCUM_STEPS  # Undo normalization for total loss\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            # Perform optimizer step before learning rate scheduler step\n",
        "            if step % GRAD_ACCUM_STEPS == 0 or step == len(dataloaders[\"train_loader\"]):\n",
        "                # Gradient Clipping\n",
        "                scaler.unscale_(optimizer)  # Unscale gradients before clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to a max norm of 1.0\n",
        "\n",
        "                # Optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Log batch loss\n",
        "                logging.info(f\"[TRAIN INFO] Batch {step}/{len(dataloaders['train_loader'])}, Accumulated loss over {GRAD_ACCUM_STEPS} batches: {batch_train_loss:.4f}\")\n",
        "                batch_train_loss = 0  # Reset batch loss for the next accumulation\n",
        "\n",
        "        # Validation step to see how well model performs this epoch\n",
        "        logging.info(f\"[TRAIN INFO] Evaluating model...\")\n",
        "        val_loss, val_acc = evaluate_model(model, dataloaders[\"val_loader\"], device)\n",
        "        avg_train_loss = total_train_loss / len(dataloaders[\"train_loader\"])\n",
        "\n",
        "        # **Learning Rate Scheduler Handling**\n",
        "        plateau_scheduler.step(val_loss)  \n",
        "\n",
        "        # Log weight decay and learning rate updates\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_val_loss_diff\": avg_train_loss - val_loss,  # Track overfitting tendency\n",
        "            \"early_stopping_epochs\": epochs_without_improvement,  # Track early stopping\n",
        "            \"learning_rate_image\": optimizer.param_groups[0][\"lr\"],  # Log learning rates\n",
        "            \"learning_rate_text\": optimizer.param_groups[1][\"lr\"],\n",
        "            \"learning_rate_fusion\": optimizer.param_groups[4][\"lr\"],\n",
        "            \"learning_rate_classifier\": optimizer.param_groups[5][\"lr\"],\n",
        "        })\n",
        "\n",
        "        logging.info(f\"[TRAIN INFO] Epoch {epoch + 1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss - CONVERGENCE_THRESHOLD:  # If loss improves, save the model\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0  # Reset epochs without improvement counter for patience\n",
        "            torch.save(model.state_dict(), f\"{MODEL_NAME}_fold_{fold+1}.pth\")\n",
        "            logging.info(f\"[TRAIN INFO] Best Model Saved for Fold {fold + 1}\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1  # Increment until patience reached\n",
        "\n",
        "        # Early stopping if no improvement for epochs\n",
        "        if epochs_without_improvement >= PATIENCE:\n",
        "            total_training_time = time.time() - epoch_start_time\n",
        "            logging.info(f\"[TRAIN INFO] Early stopping at epoch {epoch + 1} as validation loss did not improve for {PATIENCE} epochs.\")\n",
        "            logging.info(f\"[TRAIN INFO] Total Time: {total_training_time:.2f}s\")\n",
        "            wandb.finish()\n",
        "            break\n",
        "\n",
        "    total_training_time = time.time() - epoch_start_time\n",
        "    logging.info(f\"[TRAIN INFO] Fold {fold + 1} Training Complete at epoch {epoch + 1}. Total Time: {total_training_time:.2f}s\")\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:14:02,555 - INFO - [K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\n",
            "2025-03-25 09:14:02,557 - INFO - [K-FOLD INFO] ============================== Fold 1/5 ==============================\n",
            "2025-03-25 09:14:02,559 - INFO - [K-FOLD INFO] Fold 1:\n",
            "2025-03-25 09:14:02,559 - INFO -    Train Samples: 8594\n",
            "2025-03-25 09:14:02,560 - INFO -    Validation Samples: 2149\n",
            "2025-03-25 09:14:02,560 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 1\n",
            "2025-03-25 09:14:02,561 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 1:\n",
            "2025-03-25 09:14:02,561 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 09:14:03,067 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 1\n",
            "2025-03-25 09:14:03,068 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 1:\n",
            "2025-03-25 09:14:03,069 - INFO - [K-FOLD INFO] Loss function initialized for Fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: shcau (shcau-university-of-calgary-in-alberta) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_only\\wandb\\run-20250325_091403-9kq846ql</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/9kq846ql' target=\"_blank\">experiment_multimodal_attention_only_fold_1</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/9kq846ql' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/9kq846ql</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arkzs\\miniforge3\\envs\\enel645_torch_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_34928\\836902376.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:14:04,622 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 09:14:04,622 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_34928\\836902376.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 09:14:11,111 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.2082\n",
            "2025-03-25 09:14:18,768 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4934\n",
            "2025-03-25 09:14:26,371 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.4689\n",
            "2025-03-25 09:14:34,175 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3933\n",
            "2025-03-25 09:14:41,793 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3881\n",
            "2025-03-25 09:14:49,293 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3409\n",
            "2025-03-25 09:14:57,207 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.3112\n",
            "2025-03-25 09:15:04,891 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2716\n",
            "2025-03-25 09:15:12,634 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2576\n",
            "2025-03-25 09:15:20,338 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2278\n",
            "2025-03-25 09:15:28,157 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1950\n",
            "2025-03-25 09:15:35,817 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.2153\n",
            "2025-03-25 09:15:43,650 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1581\n",
            "2025-03-25 09:15:51,532 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1083\n",
            "2025-03-25 09:15:59,339 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1439\n",
            "2025-03-25 09:16:07,169 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1381\n",
            "2025-03-25 09:16:14,776 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.0909\n",
            "2025-03-25 09:16:22,415 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0308\n",
            "2025-03-25 09:16:29,990 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0399\n",
            "2025-03-25 09:16:37,754 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0526\n",
            "2025-03-25 09:16:45,328 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0489\n",
            "2025-03-25 09:16:53,163 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.9598\n",
            "2025-03-25 09:17:00,845 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0157\n",
            "2025-03-25 09:17:08,762 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0190\n",
            "2025-03-25 09:17:16,588 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9833\n",
            "2025-03-25 09:17:24,426 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0173\n",
            "2025-03-25 09:17:32,043 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 1.0313\n",
            "2025-03-25 09:17:39,953 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 1.0240\n",
            "2025-03-25 09:17:47,766 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9063\n",
            "2025-03-25 09:17:55,440 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9574\n",
            "2025-03-25 09:18:03,338 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9209\n",
            "2025-03-25 09:18:11,131 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 1.0051\n",
            "2025-03-25 09:18:18,561 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.8676\n",
            "2025-03-25 09:18:24,236 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7500\n",
            "2025-03-25 09:18:24,875 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2519\n",
            "2025-03-25 09:18:24,876 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:19:23,076 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1346, Val Loss: 0.7808, Val Acc: 0.6985\n",
            "2025-03-25 09:19:23,348 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 09:19:23,348 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 09:19:29,177 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.7028\n",
            "2025-03-25 09:19:36,614 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9310\n",
            "2025-03-25 09:19:44,076 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8128\n",
            "2025-03-25 09:19:51,519 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9073\n",
            "2025-03-25 09:19:59,351 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8843\n",
            "2025-03-25 09:20:07,214 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8618\n",
            "2025-03-25 09:20:15,068 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8130\n",
            "2025-03-25 09:20:22,719 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8782\n",
            "2025-03-25 09:20:30,483 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8570\n",
            "2025-03-25 09:20:38,236 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8142\n",
            "2025-03-25 09:20:45,637 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7731\n",
            "2025-03-25 09:20:53,033 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8021\n",
            "2025-03-25 09:21:00,227 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8576\n",
            "2025-03-25 09:21:07,882 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7990\n",
            "2025-03-25 09:21:15,195 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.8026\n",
            "2025-03-25 09:21:22,466 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7530\n",
            "2025-03-25 09:21:29,785 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8421\n",
            "2025-03-25 09:21:37,402 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7546\n",
            "2025-03-25 09:21:44,955 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7384\n",
            "2025-03-25 09:21:52,271 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8311\n",
            "2025-03-25 09:21:59,472 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.8140\n",
            "2025-03-25 09:22:06,787 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7268\n",
            "2025-03-25 09:22:14,064 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7565\n",
            "2025-03-25 09:22:21,425 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7566\n",
            "2025-03-25 09:22:29,057 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.8055\n",
            "2025-03-25 09:22:36,198 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7831\n",
            "2025-03-25 09:22:43,698 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7967\n",
            "2025-03-25 09:22:51,238 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.8050\n",
            "2025-03-25 09:22:58,648 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7282\n",
            "2025-03-25 09:23:05,859 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7770\n",
            "2025-03-25 09:23:13,250 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8495\n",
            "2025-03-25 09:23:20,629 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6951\n",
            "2025-03-25 09:23:28,240 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7446\n",
            "2025-03-25 09:23:33,741 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5352\n",
            "2025-03-25 09:23:34,316 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.3323\n",
            "2025-03-25 09:23:34,316 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:24:28,773 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8096, Val Loss: 0.5493, Val Acc: 0.8018\n",
            "2025-03-25 09:24:29,083 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 09:24:29,084 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 09:24:34,586 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4721\n",
            "2025-03-25 09:24:42,249 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.7365\n",
            "2025-03-25 09:24:49,591 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6295\n",
            "2025-03-25 09:24:57,159 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6976\n",
            "2025-03-25 09:25:04,691 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5849\n",
            "2025-03-25 09:25:12,142 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7184\n",
            "2025-03-25 09:25:19,627 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7464\n",
            "2025-03-25 09:25:27,076 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6496\n",
            "2025-03-25 09:25:34,358 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6658\n",
            "2025-03-25 09:25:41,832 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7240\n",
            "2025-03-25 09:25:49,365 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6867\n",
            "2025-03-25 09:25:56,766 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6907\n",
            "2025-03-25 09:26:03,984 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6291\n",
            "2025-03-25 09:26:11,578 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6536\n",
            "2025-03-25 09:26:18,981 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6800\n",
            "2025-03-25 09:26:26,472 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6484\n",
            "2025-03-25 09:26:34,019 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6158\n",
            "2025-03-25 09:26:41,565 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6716\n",
            "2025-03-25 09:26:48,911 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6763\n",
            "2025-03-25 09:26:56,177 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6454\n",
            "2025-03-25 09:27:03,579 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5675\n",
            "2025-03-25 09:27:10,970 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5880\n",
            "2025-03-25 09:27:18,377 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6755\n",
            "2025-03-25 09:27:25,957 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5668\n",
            "2025-03-25 09:27:33,336 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7305\n",
            "2025-03-25 09:27:40,968 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6339\n",
            "2025-03-25 09:27:48,103 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6457\n",
            "2025-03-25 09:27:55,346 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6613\n",
            "2025-03-25 09:28:02,762 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5851\n",
            "2025-03-25 09:28:10,114 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6532\n",
            "2025-03-25 09:28:17,523 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6535\n",
            "2025-03-25 09:28:24,943 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6019\n",
            "2025-03-25 09:28:32,475 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5928\n",
            "2025-03-25 09:28:38,012 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5115\n",
            "2025-03-25 09:28:38,595 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.3160\n",
            "2025-03-25 09:28:38,596 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:29:33,398 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6579, Val Loss: 0.4599, Val Acc: 0.8297\n",
            "2025-03-25 09:29:33,695 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 09:29:33,695 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 09:29:39,303 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4451\n",
            "2025-03-25 09:29:49,764 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6021\n",
            "2025-03-25 09:29:57,756 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5618\n",
            "2025-03-25 09:30:05,687 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5747\n",
            "2025-03-25 09:30:13,168 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6143\n",
            "2025-03-25 09:30:20,913 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5805\n",
            "2025-03-25 09:30:28,673 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6380\n",
            "2025-03-25 09:30:36,310 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5451\n",
            "2025-03-25 09:30:43,664 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5329\n",
            "2025-03-25 09:30:51,209 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6254\n",
            "2025-03-25 09:30:59,098 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5493\n",
            "2025-03-25 09:31:06,753 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5997\n",
            "2025-03-25 09:31:14,287 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5452\n",
            "2025-03-25 09:31:21,880 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5591\n",
            "2025-03-25 09:31:29,249 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5729\n",
            "2025-03-25 09:31:36,662 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5704\n",
            "2025-03-25 09:31:44,214 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5756\n",
            "2025-03-25 09:31:51,787 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5198\n",
            "2025-03-25 09:31:59,132 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6610\n",
            "2025-03-25 09:32:06,881 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6174\n",
            "2025-03-25 09:32:14,478 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5539\n",
            "2025-03-25 09:32:21,862 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6428\n",
            "2025-03-25 09:32:29,269 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5245\n",
            "2025-03-25 09:32:36,491 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5630\n",
            "2025-03-25 09:32:43,745 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5901\n",
            "2025-03-25 09:32:51,069 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5652\n",
            "2025-03-25 09:32:58,238 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5774\n",
            "2025-03-25 09:33:05,850 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5220\n",
            "2025-03-25 09:33:13,063 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5539\n",
            "2025-03-25 09:33:20,444 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6424\n",
            "2025-03-25 09:33:27,609 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5950\n",
            "2025-03-25 09:33:34,739 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5972\n",
            "2025-03-25 09:33:42,053 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5395\n",
            "2025-03-25 09:33:47,447 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4176\n",
            "2025-03-25 09:33:47,992 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1406\n",
            "2025-03-25 09:33:47,992 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:34:43,002 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5782, Val Loss: 0.4286, Val Acc: 0.8404\n",
            "2025-03-25 09:34:43,310 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 09:34:43,310 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 09:34:49,231 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4062\n",
            "2025-03-25 09:34:56,629 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5255\n",
            "2025-03-25 09:35:03,992 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4851\n",
            "2025-03-25 09:35:11,617 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5632\n",
            "2025-03-25 09:35:18,975 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5550\n",
            "2025-03-25 09:35:26,624 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4996\n",
            "2025-03-25 09:35:34,091 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5283\n",
            "2025-03-25 09:35:41,814 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5961\n",
            "2025-03-25 09:35:49,385 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5774\n",
            "2025-03-25 09:35:56,808 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5395\n",
            "2025-03-25 09:36:04,180 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5147\n",
            "2025-03-25 09:36:11,812 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5462\n",
            "2025-03-25 09:36:19,485 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4967\n",
            "2025-03-25 09:36:27,119 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4940\n",
            "2025-03-25 09:36:34,801 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5843\n",
            "2025-03-25 09:36:42,091 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5165\n",
            "2025-03-25 09:36:49,296 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5206\n",
            "2025-03-25 09:36:56,722 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5063\n",
            "2025-03-25 09:37:04,165 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5044\n",
            "2025-03-25 09:37:11,589 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5237\n",
            "2025-03-25 09:37:18,664 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5171\n",
            "2025-03-25 09:37:25,915 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5285\n",
            "2025-03-25 09:37:33,367 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4963\n",
            "2025-03-25 09:37:40,765 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4543\n",
            "2025-03-25 09:37:48,035 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5380\n",
            "2025-03-25 09:37:55,371 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5501\n",
            "2025-03-25 09:38:02,767 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5065\n",
            "2025-03-25 09:38:10,202 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5655\n",
            "2025-03-25 09:38:17,293 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5660\n",
            "2025-03-25 09:38:24,687 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5523\n",
            "2025-03-25 09:38:31,900 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3889\n",
            "2025-03-25 09:38:39,221 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5365\n",
            "2025-03-25 09:38:46,661 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4832\n",
            "2025-03-25 09:38:51,982 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4449\n",
            "2025-03-25 09:38:52,546 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1248\n",
            "2025-03-25 09:38:52,547 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:39:46,808 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5255, Val Loss: 0.3970, Val Acc: 0.8534\n",
            "2025-03-25 09:39:47,104 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 09:39:47,104 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 09:39:52,851 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3166\n",
            "2025-03-25 09:40:00,329 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5450\n",
            "2025-03-25 09:40:07,721 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5126\n",
            "2025-03-25 09:40:15,153 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4723\n",
            "2025-03-25 09:40:22,477 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4754\n",
            "2025-03-25 09:40:29,827 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4644\n",
            "2025-03-25 09:40:37,321 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4640\n",
            "2025-03-25 09:40:44,419 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5434\n",
            "2025-03-25 09:40:51,668 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5639\n",
            "2025-03-25 09:40:59,115 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4755\n",
            "2025-03-25 09:41:06,701 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4368\n",
            "2025-03-25 09:41:13,998 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4465\n",
            "2025-03-25 09:41:21,257 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4824\n",
            "2025-03-25 09:41:28,643 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4763\n",
            "2025-03-25 09:41:36,056 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4589\n",
            "2025-03-25 09:41:43,388 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4632\n",
            "2025-03-25 09:41:50,816 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5259\n",
            "2025-03-25 09:41:58,053 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5102\n",
            "2025-03-25 09:42:05,424 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5095\n",
            "2025-03-25 09:42:12,616 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5296\n",
            "2025-03-25 09:42:19,894 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5146\n",
            "2025-03-25 09:42:27,181 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5150\n",
            "2025-03-25 09:42:34,672 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5104\n",
            "2025-03-25 09:42:42,014 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5245\n",
            "2025-03-25 09:42:49,485 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4496\n",
            "2025-03-25 09:42:56,883 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4713\n",
            "2025-03-25 09:43:04,280 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4857\n",
            "2025-03-25 09:43:11,650 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4746\n",
            "2025-03-25 09:43:19,098 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5448\n",
            "2025-03-25 09:43:27,468 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4666\n",
            "2025-03-25 09:43:34,867 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4985\n",
            "2025-03-25 09:43:42,192 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3981\n",
            "2025-03-25 09:43:49,470 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4088\n",
            "2025-03-25 09:43:54,796 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3363\n",
            "2025-03-25 09:43:55,368 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1345\n",
            "2025-03-25 09:43:55,369 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:44:50,075 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4861, Val Loss: 0.3699, Val Acc: 0.8618\n",
            "2025-03-25 09:44:50,395 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 09:44:50,396 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 09:44:56,238 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3398\n",
            "2025-03-25 09:45:03,562 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4424\n",
            "2025-03-25 09:45:11,060 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4581\n",
            "2025-03-25 09:45:18,344 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4605\n",
            "2025-03-25 09:45:25,588 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4460\n",
            "2025-03-25 09:45:32,974 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4245\n",
            "2025-03-25 09:45:40,232 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3795\n",
            "2025-03-25 09:45:47,592 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4359\n",
            "2025-03-25 09:45:55,198 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4388\n",
            "2025-03-25 09:46:02,399 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4940\n",
            "2025-03-25 09:46:09,842 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4096\n",
            "2025-03-25 09:46:17,231 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4649\n",
            "2025-03-25 09:46:24,592 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4495\n",
            "2025-03-25 09:46:32,203 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4123\n",
            "2025-03-25 09:46:39,546 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5144\n",
            "2025-03-25 09:46:46,811 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5024\n",
            "2025-03-25 09:46:54,379 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4867\n",
            "2025-03-25 09:47:01,547 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4666\n",
            "2025-03-25 09:47:08,894 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4837\n",
            "2025-03-25 09:47:16,092 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4283\n",
            "2025-03-25 09:47:23,413 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4198\n",
            "2025-03-25 09:47:30,803 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5176\n",
            "2025-03-25 09:47:38,141 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4758\n",
            "2025-03-25 09:47:45,517 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3861\n",
            "2025-03-25 09:47:53,182 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4737\n",
            "2025-03-25 09:48:00,410 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5248\n",
            "2025-03-25 09:48:07,973 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4455\n",
            "2025-03-25 09:48:15,277 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4569\n",
            "2025-03-25 09:48:22,435 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5007\n",
            "2025-03-25 09:48:29,773 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4968\n",
            "2025-03-25 09:48:36,992 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4765\n",
            "2025-03-25 09:48:44,116 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4348\n",
            "2025-03-25 09:48:51,373 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5171\n",
            "2025-03-25 09:48:56,968 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3644\n",
            "2025-03-25 09:48:57,504 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0819\n",
            "2025-03-25 09:48:57,504 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:49:52,030 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4596, Val Loss: 0.3820, Val Acc: 0.8585\n",
            "2025-03-25 09:49:52,031 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-25 09:49:57,670 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3077\n",
            "2025-03-25 09:50:04,951 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4353\n",
            "2025-03-25 09:50:12,237 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4081\n",
            "2025-03-25 09:50:19,747 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4525\n",
            "2025-03-25 09:50:27,089 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4245\n",
            "2025-03-25 09:50:34,528 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4189\n",
            "2025-03-25 09:50:41,883 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4729\n",
            "2025-03-25 09:50:49,142 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3823\n",
            "2025-03-25 09:50:56,459 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4512\n",
            "2025-03-25 09:51:03,733 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4374\n",
            "2025-03-25 09:51:10,991 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4198\n",
            "2025-03-25 09:51:18,315 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5239\n",
            "2025-03-25 09:51:25,726 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4067\n",
            "2025-03-25 09:51:33,194 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4463\n",
            "2025-03-25 09:51:40,534 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4828\n",
            "2025-03-25 09:51:47,798 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4724\n",
            "2025-03-25 09:51:55,138 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4555\n",
            "2025-03-25 09:52:02,520 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4652\n",
            "2025-03-25 09:52:09,738 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4023\n",
            "2025-03-25 09:52:17,128 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4883\n",
            "2025-03-25 09:52:24,407 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4740\n",
            "2025-03-25 09:52:31,704 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4230\n",
            "2025-03-25 09:52:39,039 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4812\n",
            "2025-03-25 09:52:46,282 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4427\n",
            "2025-03-25 09:52:53,579 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4853\n",
            "2025-03-25 09:53:00,837 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3745\n",
            "2025-03-25 09:53:08,000 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4909\n",
            "2025-03-25 09:53:15,339 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4748\n",
            "2025-03-25 09:53:22,871 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4336\n",
            "2025-03-25 09:53:30,270 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4467\n",
            "2025-03-25 09:53:37,463 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4310\n",
            "2025-03-25 09:53:44,704 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4726\n",
            "2025-03-25 09:53:52,265 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3845\n",
            "2025-03-25 09:53:57,650 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2927\n",
            "2025-03-25 09:53:58,187 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1033\n",
            "2025-03-25 09:53:58,187 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:54:52,410 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4434, Val Loss: 0.3771, Val Acc: 0.8651\n",
            "2025-03-25 09:54:52,411 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-25 09:54:58,000 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3495\n",
            "2025-03-25 09:55:05,339 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3738\n",
            "2025-03-25 09:55:12,846 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4219\n",
            "2025-03-25 09:55:20,237 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3923\n",
            "2025-03-25 09:55:27,653 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4354\n",
            "2025-03-25 09:55:34,912 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3913\n",
            "2025-03-25 09:55:42,131 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4380\n",
            "2025-03-25 09:55:49,325 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3789\n",
            "2025-03-25 09:55:56,538 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4312\n",
            "2025-03-25 09:56:03,774 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4151\n",
            "2025-03-25 09:56:11,143 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4063\n",
            "2025-03-25 09:56:18,395 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3704\n",
            "2025-03-25 09:56:25,815 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3973\n",
            "2025-03-25 09:56:33,244 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3992\n",
            "2025-03-25 09:56:40,364 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3919\n",
            "2025-03-25 09:56:47,580 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3685\n",
            "2025-03-25 09:56:55,020 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3628\n",
            "2025-03-25 09:57:02,187 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4353\n",
            "2025-03-25 09:57:09,807 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4263\n",
            "2025-03-25 09:57:16,968 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4078\n",
            "2025-03-25 09:57:24,407 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4303\n",
            "2025-03-25 09:57:31,517 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3681\n",
            "2025-03-25 09:57:38,759 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4138\n",
            "2025-03-25 09:57:46,066 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4225\n",
            "2025-03-25 09:57:53,426 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3904\n",
            "2025-03-25 09:58:00,777 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4370\n",
            "2025-03-25 09:58:08,227 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3547\n",
            "2025-03-25 09:58:15,616 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4428\n",
            "2025-03-25 09:58:22,908 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3862\n",
            "2025-03-25 09:58:30,049 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4073\n",
            "2025-03-25 09:58:37,406 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3587\n",
            "2025-03-25 09:58:44,716 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4125\n",
            "2025-03-25 09:58:52,001 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4619\n",
            "2025-03-25 09:58:57,292 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3137\n",
            "2025-03-25 09:58:57,830 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1391\n",
            "2025-03-25 09:58:57,831 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 09:59:51,787 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4069, Val Loss: 0.3782, Val Acc: 0.8623\n",
            "2025-03-25 09:59:51,787 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-25 09:59:57,352 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2691\n",
            "2025-03-25 10:00:04,854 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4062\n",
            "2025-03-25 10:00:12,143 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4188\n",
            "2025-03-25 10:00:19,748 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3913\n",
            "2025-03-25 10:00:27,144 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3850\n",
            "2025-03-25 10:00:34,479 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3857\n",
            "2025-03-25 10:00:41,756 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3516\n",
            "2025-03-25 10:00:48,899 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3817\n",
            "2025-03-25 10:00:56,535 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3870\n",
            "2025-03-25 10:01:03,711 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3396\n",
            "2025-03-25 10:01:11,141 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3558\n",
            "2025-03-25 10:01:18,471 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3740\n",
            "2025-03-25 10:01:25,926 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4019\n",
            "2025-03-25 10:01:32,936 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3731\n",
            "2025-03-25 10:01:40,198 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3931\n",
            "2025-03-25 10:01:47,442 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3841\n",
            "2025-03-25 10:01:54,932 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3929\n",
            "2025-03-25 10:02:02,057 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4116\n",
            "2025-03-25 10:02:09,207 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4379\n",
            "2025-03-25 10:02:16,724 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4225\n",
            "2025-03-25 10:02:24,042 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3550\n",
            "2025-03-25 10:02:31,328 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4079\n",
            "2025-03-25 10:02:38,428 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3436\n",
            "2025-03-25 10:02:45,793 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4098\n",
            "2025-03-25 10:02:53,129 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4327\n",
            "2025-03-25 10:03:00,527 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3907\n",
            "2025-03-25 10:03:07,885 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3533\n",
            "2025-03-25 10:03:15,000 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3947\n",
            "2025-03-25 10:03:22,154 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3756\n",
            "2025-03-25 10:03:29,387 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3603\n",
            "2025-03-25 10:03:36,503 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3693\n",
            "2025-03-25 10:03:43,911 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4168\n",
            "2025-03-25 10:03:51,690 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4027\n",
            "2025-03-25 10:03:56,926 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-25 10:03:57,473 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0628\n",
            "2025-03-25 10:03:57,474 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:04:51,734 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3856, Val Loss: 0.3730, Val Acc: 0.8720\n",
            "2025-03-25 10:04:51,735 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-25 10:04:57,160 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-25 10:05:04,475 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3242\n",
            "2025-03-25 10:05:11,624 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3129\n",
            "2025-03-25 10:05:19,071 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3591\n",
            "2025-03-25 10:05:26,284 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3452\n",
            "2025-03-25 10:05:33,484 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3375\n",
            "2025-03-25 10:05:40,781 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3154\n",
            "2025-03-25 10:05:48,063 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3130\n",
            "2025-03-25 10:05:55,457 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3357\n",
            "2025-03-25 10:06:02,855 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3530\n",
            "2025-03-25 10:06:10,237 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3234\n",
            "2025-03-25 10:06:17,657 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3105\n",
            "2025-03-25 10:06:25,022 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3390\n",
            "2025-03-25 10:06:32,218 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3356\n",
            "2025-03-25 10:06:39,646 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3697\n",
            "2025-03-25 10:06:47,041 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3494\n",
            "2025-03-25 10:06:54,440 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3289\n",
            "2025-03-25 10:07:01,640 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3652\n",
            "2025-03-25 10:07:08,912 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3317\n",
            "2025-03-25 10:07:16,149 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3495\n",
            "2025-03-25 10:07:23,443 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3247\n",
            "2025-03-25 10:07:30,745 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3143\n",
            "2025-03-25 10:07:38,028 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3761\n",
            "2025-03-25 10:07:45,326 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3373\n",
            "2025-03-25 10:07:52,502 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3216\n",
            "2025-03-25 10:07:59,821 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3111\n",
            "2025-03-25 10:08:07,019 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3757\n",
            "2025-03-25 10:08:14,259 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3298\n",
            "2025-03-25 10:08:21,799 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3123\n",
            "2025-03-25 10:08:29,032 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3498\n",
            "2025-03-25 10:08:36,427 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3343\n",
            "2025-03-25 10:08:43,689 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3344\n",
            "2025-03-25 10:08:51,005 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3611\n",
            "2025-03-25 10:08:56,396 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2452\n",
            "2025-03-25 10:08:56,929 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0793\n",
            "2025-03-25 10:08:56,930 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:09:50,768 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3377, Val Loss: 0.3797, Val Acc: 0.8753\n",
            "2025-03-25 10:09:50,768 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 10:09:56,334 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2633\n",
            "2025-03-25 10:10:03,779 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2987\n",
            "2025-03-25 10:10:11,132 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3003\n",
            "2025-03-25 10:10:18,498 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3123\n",
            "2025-03-25 10:10:25,714 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3270\n",
            "2025-03-25 10:10:33,073 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3230\n",
            "2025-03-25 10:10:40,389 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3117\n",
            "2025-03-25 10:10:47,771 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3244\n",
            "2025-03-25 10:10:55,153 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2931\n",
            "2025-03-25 10:11:02,279 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2967\n",
            "2025-03-25 10:11:09,476 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3085\n",
            "2025-03-25 10:11:16,913 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3286\n",
            "2025-03-25 10:11:24,129 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3044\n",
            "2025-03-25 10:11:31,647 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3151\n",
            "2025-03-25 10:11:38,844 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3313\n",
            "2025-03-25 10:11:46,286 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3080\n",
            "2025-03-25 10:11:53,468 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3053\n",
            "2025-03-25 10:12:00,749 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3460\n",
            "2025-03-25 10:12:07,875 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3076\n",
            "2025-03-25 10:12:15,227 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3089\n",
            "2025-03-25 10:12:22,538 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3327\n",
            "2025-03-25 10:12:29,734 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2906\n",
            "2025-03-25 10:12:37,104 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2859\n",
            "2025-03-25 10:12:44,245 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-25 10:12:51,516 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-25 10:12:58,651 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-25 10:13:05,864 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3045\n",
            "2025-03-25 10:13:13,007 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3081\n",
            "2025-03-25 10:13:20,506 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3159\n",
            "2025-03-25 10:13:27,831 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2855\n",
            "2025-03-25 10:13:35,303 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3044\n",
            "2025-03-25 10:13:42,638 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2955\n",
            "2025-03-25 10:13:49,763 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2986\n",
            "2025-03-25 10:13:55,189 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2249\n",
            "2025-03-25 10:13:55,777 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0681\n",
            "2025-03-25 10:13:55,778 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:14:50,284 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3088, Val Loss: 0.3782, Val Acc: 0.8730\n",
            "2025-03-25 10:14:50,285 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 10:14:55,746 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2224\n",
            "2025-03-25 10:15:03,095 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-25 10:15:10,373 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3099\n",
            "2025-03-25 10:15:17,858 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-25 10:15:25,032 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2911\n",
            "2025-03-25 10:15:32,387 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3108\n",
            "2025-03-25 10:15:39,808 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2798\n",
            "2025-03-25 10:15:47,227 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2797\n",
            "2025-03-25 10:15:54,575 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2970\n",
            "2025-03-25 10:16:02,058 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3032\n",
            "2025-03-25 10:16:09,371 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3450\n",
            "2025-03-25 10:16:16,771 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3142\n",
            "2025-03-25 10:16:24,134 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3204\n",
            "2025-03-25 10:16:31,546 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3069\n",
            "2025-03-25 10:16:38,664 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2905\n",
            "2025-03-25 10:16:46,057 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-25 10:16:53,225 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2726\n",
            "2025-03-25 10:17:00,650 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-25 10:17:07,803 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2828\n",
            "2025-03-25 10:17:14,993 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3090\n",
            "2025-03-25 10:17:22,591 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3283\n",
            "2025-03-25 10:17:29,844 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3019\n",
            "2025-03-25 10:17:37,231 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3138\n",
            "2025-03-25 10:17:44,404 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-25 10:17:51,624 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-25 10:17:59,188 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3100\n",
            "2025-03-25 10:18:06,610 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2920\n",
            "2025-03-25 10:18:13,961 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-25 10:18:21,229 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-25 10:18:28,612 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3300\n",
            "2025-03-25 10:18:35,952 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2536\n",
            "2025-03-25 10:18:43,422 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3150\n",
            "2025-03-25 10:18:50,770 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2895\n",
            "2025-03-25 10:18:56,101 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2197\n",
            "2025-03-25 10:18:56,650 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0622\n",
            "2025-03-25 10:18:56,651 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:19:50,867 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.2995, Val Loss: 0.3908, Val Acc: 0.8748\n",
            "2025-03-25 10:19:50,867 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 10:19:56,356 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2289\n",
            "2025-03-25 10:20:03,781 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2990\n",
            "2025-03-25 10:20:11,187 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2777\n",
            "2025-03-25 10:20:18,393 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2961\n",
            "2025-03-25 10:20:25,786 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2913\n",
            "2025-03-25 10:20:32,946 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-25 10:20:40,188 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3035\n",
            "2025-03-25 10:20:47,339 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2758\n",
            "2025-03-25 10:20:54,763 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2731\n",
            "2025-03-25 10:21:01,977 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3022\n",
            "2025-03-25 10:21:09,359 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3189\n",
            "2025-03-25 10:21:16,492 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-25 10:21:23,794 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2977\n",
            "2025-03-25 10:21:31,170 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3055\n",
            "2025-03-25 10:21:38,450 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3177\n",
            "2025-03-25 10:21:45,784 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3084\n",
            "2025-03-25 10:21:53,102 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2945\n",
            "2025-03-25 10:22:00,521 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2913\n",
            "2025-03-25 10:22:07,878 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3088\n",
            "2025-03-25 10:22:15,342 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2881\n",
            "2025-03-25 10:22:22,506 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2812\n",
            "2025-03-25 10:22:29,934 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3279\n",
            "2025-03-25 10:22:37,302 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2969\n",
            "2025-03-25 10:22:44,537 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2686\n",
            "2025-03-25 10:22:52,132 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-25 10:22:59,343 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2798\n",
            "2025-03-25 10:23:06,926 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2766\n",
            "2025-03-25 10:23:14,235 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-25 10:23:21,484 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2747\n",
            "2025-03-25 10:23:28,828 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-25 10:23:36,090 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3028\n",
            "2025-03-25 10:23:43,518 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-25 10:23:50,751 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2662\n",
            "2025-03-25 10:23:56,245 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2122\n",
            "2025-03-25 10:23:56,805 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1266\n",
            "2025-03-25 10:23:56,806 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:24:51,358 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.2921, Val Loss: 0.4009, Val Acc: 0.8674\n",
            "2025-03-25 10:24:51,358 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 10:24:56,812 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2064\n",
            "2025-03-25 10:25:04,023 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-25 10:25:11,489 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-25 10:25:18,888 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3178\n",
            "2025-03-25 10:25:26,186 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-25 10:25:33,512 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2731\n",
            "2025-03-25 10:25:40,616 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2853\n",
            "2025-03-25 10:25:48,039 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2688\n",
            "2025-03-25 10:25:55,464 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2723\n",
            "2025-03-25 10:26:02,677 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-25 10:26:10,034 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2857\n",
            "2025-03-25 10:26:17,300 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2911\n",
            "2025-03-25 10:26:24,874 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-25 10:26:32,177 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2822\n",
            "2025-03-25 10:26:39,543 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2804\n",
            "2025-03-25 10:26:47,008 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2996\n",
            "2025-03-25 10:26:54,198 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3114\n",
            "2025-03-25 10:27:01,354 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-25 10:27:08,639 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3087\n",
            "2025-03-25 10:27:16,066 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2936\n",
            "2025-03-25 10:27:23,203 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2532\n",
            "2025-03-25 10:27:30,583 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3223\n",
            "2025-03-25 10:27:37,874 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2641\n",
            "2025-03-25 10:27:45,136 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2818\n",
            "2025-03-25 10:27:52,451 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-25 10:27:59,759 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2730\n",
            "2025-03-25 10:28:07,052 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2780\n",
            "2025-03-25 10:28:14,320 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2902\n",
            "2025-03-25 10:28:21,722 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-25 10:28:29,014 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2631\n",
            "2025-03-25 10:28:36,112 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-25 10:28:43,378 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2889\n",
            "2025-03-25 10:28:50,701 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2823\n",
            "2025-03-25 10:28:56,089 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1903\n",
            "2025-03-25 10:28:56,632 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0795\n",
            "2025-03-25 10:28:56,632 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:29:50,943 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2816, Val Loss: 0.3889, Val Acc: 0.8725\n",
            "2025-03-25 10:29:50,943 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 10:29:56,348 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2050\n",
            "2025-03-25 10:30:03,618 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2611\n",
            "2025-03-25 10:30:11,012 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-25 10:30:18,346 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2537\n",
            "2025-03-25 10:30:25,627 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-25 10:30:32,924 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2730\n",
            "2025-03-25 10:30:40,202 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2619\n",
            "2025-03-25 10:30:47,419 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-25 10:30:54,718 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2796\n",
            "2025-03-25 10:31:02,102 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2832\n",
            "2025-03-25 10:31:09,384 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2722\n",
            "2025-03-25 10:31:16,729 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2637\n",
            "2025-03-25 10:31:24,131 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2524\n",
            "2025-03-25 10:31:31,332 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2694\n",
            "2025-03-25 10:31:38,625 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2872\n",
            "2025-03-25 10:31:45,984 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2741\n",
            "2025-03-25 10:31:53,343 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2819\n",
            "2025-03-25 10:32:00,791 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-25 10:32:08,066 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-25 10:32:15,559 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-25 10:32:22,957 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2719\n",
            "2025-03-25 10:32:30,160 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-25 10:32:37,567 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2878\n",
            "2025-03-25 10:32:44,870 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2872\n",
            "2025-03-25 10:32:52,344 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2634\n",
            "2025-03-25 10:32:59,363 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2760\n",
            "2025-03-25 10:33:06,651 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-25 10:33:13,970 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2744\n",
            "2025-03-25 10:33:21,261 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-25 10:33:29,546 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2702\n",
            "2025-03-25 10:33:36,683 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-25 10:33:43,863 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-25 10:33:51,333 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-25 10:33:56,813 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1986\n",
            "2025-03-25 10:33:57,366 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0720\n",
            "2025-03-25 10:33:57,367 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:34:51,481 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2723, Val Loss: 0.3926, Val Acc: 0.8758\n",
            "2025-03-25 10:34:51,482 - INFO - [TRAIN INFO] Early stopping at epoch 16 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 10:34:51,482 - INFO - [TRAIN INFO] Total Time: 4846.86s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▇▆▅▅▄▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇▇█▇███████</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.27235</td></tr><tr><td>train_val_loss_diff</td><td>-0.12028</td></tr><tr><td>val_accuracy</td><td>0.87576</td></tr><tr><td>val_loss</td><td>0.39262</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_only_fold_1</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/9kq846ql' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/9kq846ql</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250325_091403-9kq846ql\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 10:34:53,638 - INFO - [TRAIN INFO] Fold 1 Training Complete at epoch 16. Total Time: 4849.02s\n",
            "2025-03-25 10:34:53,655 - INFO - [K-FOLD INFO] Fold 1 completed in 4851.10 seconds\n",
            "2025-03-25 10:34:53,656 - INFO - [K-FOLD INFO] ============================== Fold 2/5 ==============================\n",
            "2025-03-25 10:34:53,659 - INFO - [K-FOLD INFO] Fold 2:\n",
            "2025-03-25 10:34:53,660 - INFO -    Train Samples: 8594\n",
            "2025-03-25 10:34:53,660 - INFO -    Validation Samples: 2149\n",
            "2025-03-25 10:34:53,661 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 2\n",
            "2025-03-25 10:34:53,662 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 2:\n",
            "2025-03-25 10:34:53,662 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 10:34:54,500 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 2\n",
            "2025-03-25 10:34:54,502 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 2:\n",
            "2025-03-25 10:34:54,503 - INFO - [K-FOLD INFO] Loss function initialized for Fold 2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_only\\wandb\\run-20250325_103454-7sd1wx0s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/7sd1wx0s' target=\"_blank\">experiment_multimodal_attention_only_fold_2</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/7sd1wx0s' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/7sd1wx0s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 10:34:55,247 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 10:34:55,248 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-25 10:35:00,932 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0572\n",
            "2025-03-25 10:35:08,708 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4016\n",
            "2025-03-25 10:35:15,889 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3698\n",
            "2025-03-25 10:35:23,278 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3560\n",
            "2025-03-25 10:35:30,912 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.2760\n",
            "2025-03-25 10:35:38,247 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3062\n",
            "2025-03-25 10:35:45,419 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2974\n",
            "2025-03-25 10:35:52,727 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2524\n",
            "2025-03-25 10:36:00,080 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2614\n",
            "2025-03-25 10:36:07,419 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.1797\n",
            "2025-03-25 10:36:14,871 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2046\n",
            "2025-03-25 10:36:22,314 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1438\n",
            "2025-03-25 10:36:29,494 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1444\n",
            "2025-03-25 10:36:37,078 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1901\n",
            "2025-03-25 10:36:44,476 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1141\n",
            "2025-03-25 10:36:51,874 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1289\n",
            "2025-03-25 10:36:59,045 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.0991\n",
            "2025-03-25 10:37:06,484 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.1091\n",
            "2025-03-25 10:37:13,852 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.9896\n",
            "2025-03-25 10:37:21,208 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.9971\n",
            "2025-03-25 10:37:28,475 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0318\n",
            "2025-03-25 10:37:35,769 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0556\n",
            "2025-03-25 10:37:43,075 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0435\n",
            "2025-03-25 10:37:50,400 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0032\n",
            "2025-03-25 10:37:58,059 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9871\n",
            "2025-03-25 10:38:05,497 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0371\n",
            "2025-03-25 10:38:12,924 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9312\n",
            "2025-03-25 10:38:20,200 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9769\n",
            "2025-03-25 10:38:27,458 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9602\n",
            "2025-03-25 10:38:34,748 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9420\n",
            "2025-03-25 10:38:41,944 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8806\n",
            "2025-03-25 10:38:49,098 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9253\n",
            "2025-03-25 10:38:56,445 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9554\n",
            "2025-03-25 10:39:01,865 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6983\n",
            "2025-03-25 10:39:02,431 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-25 10:39:02,432 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:39:56,373 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1135, Val Loss: 0.8007, Val Acc: 0.6794\n",
            "2025-03-25 10:39:56,662 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 10:39:56,663 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 10:40:02,426 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6503\n",
            "2025-03-25 10:40:09,750 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9539\n",
            "2025-03-25 10:40:17,025 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8886\n",
            "2025-03-25 10:40:24,556 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9240\n",
            "2025-03-25 10:40:32,016 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8556\n",
            "2025-03-25 10:40:39,367 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.9092\n",
            "2025-03-25 10:40:46,801 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8692\n",
            "2025-03-25 10:40:54,102 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7802\n",
            "2025-03-25 10:41:01,797 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8261\n",
            "2025-03-25 10:41:09,076 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8022\n",
            "2025-03-25 10:41:16,350 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8460\n",
            "2025-03-25 10:41:23,621 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7599\n",
            "2025-03-25 10:41:30,914 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8432\n",
            "2025-03-25 10:41:38,218 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8541\n",
            "2025-03-25 10:41:45,783 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7403\n",
            "2025-03-25 10:41:52,971 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8288\n",
            "2025-03-25 10:42:00,194 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8265\n",
            "2025-03-25 10:42:07,518 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7944\n",
            "2025-03-25 10:42:14,958 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7421\n",
            "2025-03-25 10:42:22,153 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7777\n",
            "2025-03-25 10:42:29,538 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7719\n",
            "2025-03-25 10:42:36,911 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7641\n",
            "2025-03-25 10:42:44,368 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.8093\n",
            "2025-03-25 10:42:51,574 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7755\n",
            "2025-03-25 10:42:59,161 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7897\n",
            "2025-03-25 10:43:06,374 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7910\n",
            "2025-03-25 10:43:13,652 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.8010\n",
            "2025-03-25 10:43:21,155 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7538\n",
            "2025-03-25 10:43:28,367 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7748\n",
            "2025-03-25 10:43:35,751 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7557\n",
            "2025-03-25 10:43:43,160 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7154\n",
            "2025-03-25 10:43:50,557 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8032\n",
            "2025-03-25 10:43:57,957 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7210\n",
            "2025-03-25 10:44:03,373 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6137\n",
            "2025-03-25 10:44:03,928 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2173\n",
            "2025-03-25 10:44:03,929 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:44:58,011 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8098, Val Loss: 0.5806, Val Acc: 0.7859\n",
            "2025-03-25 10:44:58,322 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 10:44:58,323 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 10:45:03,881 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5370\n",
            "2025-03-25 10:45:11,278 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6592\n",
            "2025-03-25 10:45:18,597 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6965\n",
            "2025-03-25 10:45:25,913 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6886\n",
            "2025-03-25 10:45:33,458 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6656\n",
            "2025-03-25 10:45:40,666 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7274\n",
            "2025-03-25 10:45:48,061 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7132\n",
            "2025-03-25 10:45:55,518 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6328\n",
            "2025-03-25 10:46:02,820 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7125\n",
            "2025-03-25 10:46:10,254 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6923\n",
            "2025-03-25 10:46:17,536 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6198\n",
            "2025-03-25 10:46:25,087 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6926\n",
            "2025-03-25 10:46:32,374 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6210\n",
            "2025-03-25 10:46:39,728 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5983\n",
            "2025-03-25 10:46:47,074 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7259\n",
            "2025-03-25 10:46:54,301 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6912\n",
            "2025-03-25 10:47:01,717 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6573\n",
            "2025-03-25 10:47:09,005 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6573\n",
            "2025-03-25 10:47:16,294 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6870\n",
            "2025-03-25 10:47:23,634 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6506\n",
            "2025-03-25 10:47:30,889 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6590\n",
            "2025-03-25 10:47:38,280 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7021\n",
            "2025-03-25 10:47:45,409 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6558\n",
            "2025-03-25 10:47:52,681 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6880\n",
            "2025-03-25 10:47:59,895 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6772\n",
            "2025-03-25 10:48:07,125 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7376\n",
            "2025-03-25 10:48:14,497 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5921\n",
            "2025-03-25 10:48:22,071 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6613\n",
            "2025-03-25 10:48:29,270 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6440\n",
            "2025-03-25 10:48:36,475 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6267\n",
            "2025-03-25 10:48:43,653 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6304\n",
            "2025-03-25 10:48:51,259 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7034\n",
            "2025-03-25 10:48:58,612 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5661\n",
            "2025-03-25 10:49:04,247 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4466\n",
            "2025-03-25 10:49:04,805 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2210\n",
            "2025-03-25 10:49:04,806 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:49:58,726 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6678, Val Loss: 0.4715, Val Acc: 0.8255\n",
            "2025-03-25 10:49:59,044 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 10:49:59,044 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 10:50:04,684 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4769\n",
            "2025-03-25 10:50:12,226 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6123\n",
            "2025-03-25 10:50:19,623 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5832\n",
            "2025-03-25 10:50:26,814 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5507\n",
            "2025-03-25 10:50:34,247 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5626\n",
            "2025-03-25 10:50:41,579 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4941\n",
            "2025-03-25 10:50:48,832 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6078\n",
            "2025-03-25 10:50:56,217 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5745\n",
            "2025-03-25 10:51:03,420 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5476\n",
            "2025-03-25 10:51:10,743 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5397\n",
            "2025-03-25 10:51:17,970 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6145\n",
            "2025-03-25 10:51:25,362 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5736\n",
            "2025-03-25 10:51:32,749 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5620\n",
            "2025-03-25 10:51:40,171 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5710\n",
            "2025-03-25 10:51:47,559 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5910\n",
            "2025-03-25 10:51:54,883 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6525\n",
            "2025-03-25 10:52:02,343 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6165\n",
            "2025-03-25 10:52:09,594 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6045\n",
            "2025-03-25 10:52:17,140 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5748\n",
            "2025-03-25 10:52:24,420 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5825\n",
            "2025-03-25 10:52:31,778 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5276\n",
            "2025-03-25 10:52:39,293 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5711\n",
            "2025-03-25 10:52:46,780 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6043\n",
            "2025-03-25 10:52:53,950 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5601\n",
            "2025-03-25 10:53:01,405 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6096\n",
            "2025-03-25 10:53:08,622 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5889\n",
            "2025-03-25 10:53:15,938 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6337\n",
            "2025-03-25 10:53:23,397 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5545\n",
            "2025-03-25 10:53:30,799 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6001\n",
            "2025-03-25 10:53:38,157 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5649\n",
            "2025-03-25 10:53:45,761 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5915\n",
            "2025-03-25 10:53:53,204 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5358\n",
            "2025-03-25 10:54:00,593 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5051\n",
            "2025-03-25 10:54:05,976 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4709\n",
            "2025-03-25 10:54:06,529 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1571\n",
            "2025-03-25 10:54:06,529 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 10:55:00,795 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5798, Val Loss: 0.4089, Val Acc: 0.8446\n",
            "2025-03-25 10:55:01,108 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 10:55:01,109 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 10:55:06,944 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4276\n",
            "2025-03-25 10:55:14,334 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5431\n",
            "2025-03-25 10:55:21,941 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5639\n",
            "2025-03-25 10:55:29,102 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5022\n",
            "2025-03-25 10:55:36,526 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5379\n",
            "2025-03-25 10:55:44,072 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5600\n",
            "2025-03-25 10:55:51,536 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5859\n",
            "2025-03-25 10:55:58,752 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5115\n",
            "2025-03-25 10:56:06,120 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4972\n",
            "2025-03-25 10:56:13,518 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5714\n",
            "2025-03-25 10:56:20,728 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6052\n",
            "2025-03-25 10:56:27,812 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5110\n",
            "2025-03-25 10:56:35,323 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5347\n",
            "2025-03-25 10:56:42,728 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4694\n",
            "2025-03-25 10:56:50,119 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4909\n",
            "2025-03-25 10:56:57,267 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4578\n",
            "2025-03-25 10:57:04,425 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4693\n",
            "2025-03-25 10:57:11,819 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5265\n",
            "2025-03-25 10:57:19,055 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5479\n",
            "2025-03-25 10:57:26,405 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5572\n",
            "2025-03-25 10:57:33,709 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4448\n",
            "2025-03-25 10:57:41,018 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4889\n",
            "2025-03-25 10:57:48,299 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5498\n",
            "2025-03-25 10:57:55,472 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5720\n",
            "2025-03-25 10:58:02,899 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5246\n",
            "2025-03-25 10:58:10,205 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5224\n",
            "2025-03-25 10:58:17,679 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4853\n",
            "2025-03-25 10:58:24,889 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5297\n",
            "2025-03-25 10:58:32,215 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5022\n",
            "2025-03-25 10:58:39,676 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5243\n",
            "2025-03-25 10:58:46,887 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5487\n",
            "2025-03-25 10:58:54,161 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5622\n",
            "2025-03-25 10:59:01,370 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5353\n",
            "2025-03-25 10:59:06,886 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4296\n",
            "2025-03-25 10:59:07,431 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1689\n",
            "2025-03-25 10:59:07,432 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:00:01,734 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5292, Val Loss: 0.3873, Val Acc: 0.8539\n",
            "2025-03-25 11:00:02,040 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 11:00:02,041 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 11:00:07,663 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3211\n",
            "2025-03-25 11:00:14,979 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4918\n",
            "2025-03-25 11:00:22,448 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4687\n",
            "2025-03-25 11:00:29,874 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4722\n",
            "2025-03-25 11:00:37,168 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4799\n",
            "2025-03-25 11:00:44,468 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4412\n",
            "2025-03-25 11:00:51,841 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4881\n",
            "2025-03-25 11:00:59,004 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5072\n",
            "2025-03-25 11:01:06,454 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4951\n",
            "2025-03-25 11:01:13,839 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5007\n",
            "2025-03-25 11:01:21,037 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4979\n",
            "2025-03-25 11:01:28,313 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4829\n",
            "2025-03-25 11:01:35,646 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5121\n",
            "2025-03-25 11:01:42,959 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5208\n",
            "2025-03-25 11:01:50,315 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5328\n",
            "2025-03-25 11:01:57,624 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4417\n",
            "2025-03-25 11:02:05,210 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4469\n",
            "2025-03-25 11:02:12,323 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5181\n",
            "2025-03-25 11:02:19,597 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4958\n",
            "2025-03-25 11:02:26,853 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5036\n",
            "2025-03-25 11:02:34,397 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4623\n",
            "2025-03-25 11:02:41,542 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5241\n",
            "2025-03-25 11:02:48,748 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5172\n",
            "2025-03-25 11:02:56,206 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4928\n",
            "2025-03-25 11:03:03,562 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5406\n",
            "2025-03-25 11:03:11,003 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4273\n",
            "2025-03-25 11:03:18,347 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5479\n",
            "2025-03-25 11:03:25,635 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4897\n",
            "2025-03-25 11:03:33,086 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4789\n",
            "2025-03-25 11:03:40,322 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4935\n",
            "2025-03-25 11:03:47,475 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5410\n",
            "2025-03-25 11:03:55,148 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4772\n",
            "2025-03-25 11:04:02,674 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4541\n",
            "2025-03-25 11:04:08,183 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3465\n",
            "2025-03-25 11:04:08,722 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1337\n",
            "2025-03-25 11:04:08,723 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:05:02,981 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4902, Val Loss: 0.3652, Val Acc: 0.8669\n",
            "2025-03-25 11:05:03,301 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 11:05:03,302 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 11:05:09,152 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3274\n",
            "2025-03-25 11:05:16,492 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4505\n",
            "2025-03-25 11:05:23,979 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4186\n",
            "2025-03-25 11:05:31,420 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4591\n",
            "2025-03-25 11:05:38,737 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4893\n",
            "2025-03-25 11:05:46,325 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4797\n",
            "2025-03-25 11:05:53,451 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4345\n",
            "2025-03-25 11:06:00,731 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4956\n",
            "2025-03-25 11:06:08,139 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3967\n",
            "2025-03-25 11:06:15,344 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4595\n",
            "2025-03-25 11:06:22,745 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5064\n",
            "2025-03-25 11:06:30,047 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4216\n",
            "2025-03-25 11:06:37,506 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4796\n",
            "2025-03-25 11:06:45,121 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4198\n",
            "2025-03-25 11:06:52,439 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4874\n",
            "2025-03-25 11:06:59,751 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5602\n",
            "2025-03-25 11:07:07,123 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4462\n",
            "2025-03-25 11:07:14,520 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4820\n",
            "2025-03-25 11:07:21,853 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4870\n",
            "2025-03-25 11:07:29,015 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4904\n",
            "2025-03-25 11:07:36,333 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4882\n",
            "2025-03-25 11:07:43,701 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4072\n",
            "2025-03-25 11:07:51,118 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4675\n",
            "2025-03-25 11:07:58,278 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4213\n",
            "2025-03-25 11:08:05,710 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4423\n",
            "2025-03-25 11:08:12,826 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4288\n",
            "2025-03-25 11:08:20,435 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4873\n",
            "2025-03-25 11:08:27,793 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4851\n",
            "2025-03-25 11:08:35,100 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4753\n",
            "2025-03-25 11:08:42,199 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5489\n",
            "2025-03-25 11:08:49,689 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4331\n",
            "2025-03-25 11:08:57,087 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4066\n",
            "2025-03-25 11:09:04,496 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3930\n",
            "2025-03-25 11:09:09,910 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3145\n",
            "2025-03-25 11:09:10,431 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0739\n",
            "2025-03-25 11:09:10,432 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:10:04,344 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4582, Val Loss: 0.3900, Val Acc: 0.8562\n",
            "2025-03-25 11:10:04,344 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-25 11:10:09,869 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3532\n",
            "2025-03-25 11:10:17,228 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4105\n",
            "2025-03-25 11:10:24,861 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4094\n",
            "2025-03-25 11:10:32,225 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4846\n",
            "2025-03-25 11:10:39,535 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4215\n",
            "2025-03-25 11:10:46,862 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4675\n",
            "2025-03-25 11:10:53,954 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4006\n",
            "2025-03-25 11:11:01,198 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4209\n",
            "2025-03-25 11:11:08,680 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4061\n",
            "2025-03-25 11:11:16,057 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3661\n",
            "2025-03-25 11:11:23,420 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4106\n",
            "2025-03-25 11:11:30,635 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4015\n",
            "2025-03-25 11:11:38,039 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4760\n",
            "2025-03-25 11:11:45,690 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4208\n",
            "2025-03-25 11:11:52,939 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4225\n",
            "2025-03-25 11:12:00,230 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4091\n",
            "2025-03-25 11:12:07,632 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4390\n",
            "2025-03-25 11:12:14,933 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4145\n",
            "2025-03-25 11:12:22,247 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4344\n",
            "2025-03-25 11:12:29,577 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4696\n",
            "2025-03-25 11:12:36,902 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3591\n",
            "2025-03-25 11:12:44,413 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4022\n",
            "2025-03-25 11:12:51,536 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4796\n",
            "2025-03-25 11:12:58,755 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3820\n",
            "2025-03-25 11:13:06,038 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3763\n",
            "2025-03-25 11:13:13,291 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4142\n",
            "2025-03-25 11:13:20,505 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4310\n",
            "2025-03-25 11:13:27,810 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4570\n",
            "2025-03-25 11:13:35,016 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4524\n",
            "2025-03-25 11:13:42,596 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3847\n",
            "2025-03-25 11:13:49,865 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4395\n",
            "2025-03-25 11:13:57,270 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4709\n",
            "2025-03-25 11:14:04,700 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4523\n",
            "2025-03-25 11:14:10,163 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3596\n",
            "2025-03-25 11:14:10,728 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0865\n",
            "2025-03-25 11:14:10,729 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:15:04,471 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4262, Val Loss: 0.3827, Val Acc: 0.8646\n",
            "2025-03-25 11:15:04,471 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-25 11:15:09,981 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2890\n",
            "2025-03-25 11:15:17,364 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3551\n",
            "2025-03-25 11:15:24,573 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3924\n",
            "2025-03-25 11:15:31,968 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4221\n",
            "2025-03-25 11:15:39,186 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3790\n",
            "2025-03-25 11:15:46,753 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4101\n",
            "2025-03-25 11:15:53,905 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3841\n",
            "2025-03-25 11:16:01,079 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3654\n",
            "2025-03-25 11:16:08,193 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4087\n",
            "2025-03-25 11:16:15,747 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4352\n",
            "2025-03-25 11:16:23,127 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3868\n",
            "2025-03-25 11:16:30,275 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3942\n",
            "2025-03-25 11:16:37,737 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3972\n",
            "2025-03-25 11:16:45,061 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4134\n",
            "2025-03-25 11:16:52,373 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3905\n",
            "2025-03-25 11:16:59,868 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4460\n",
            "2025-03-25 11:17:07,085 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3670\n",
            "2025-03-25 11:17:14,543 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4087\n",
            "2025-03-25 11:17:22,071 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4033\n",
            "2025-03-25 11:17:29,530 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4103\n",
            "2025-03-25 11:17:36,861 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4180\n",
            "2025-03-25 11:17:44,134 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3775\n",
            "2025-03-25 11:17:51,353 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3994\n",
            "2025-03-25 11:17:58,624 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3666\n",
            "2025-03-25 11:18:05,925 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4273\n",
            "2025-03-25 11:18:13,227 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4160\n",
            "2025-03-25 11:18:20,522 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4336\n",
            "2025-03-25 11:18:27,757 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4454\n",
            "2025-03-25 11:18:35,118 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3634\n",
            "2025-03-25 11:18:42,503 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4081\n",
            "2025-03-25 11:18:49,866 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4048\n",
            "2025-03-25 11:18:57,284 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3937\n",
            "2025-03-25 11:19:04,708 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4056\n",
            "2025-03-25 11:19:10,299 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3005\n",
            "2025-03-25 11:19:10,878 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1056\n",
            "2025-03-25 11:19:10,878 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:20:04,971 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4007, Val Loss: 0.3818, Val Acc: 0.8637\n",
            "2025-03-25 11:20:04,971 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-25 11:20:10,489 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2591\n",
            "2025-03-25 11:20:17,860 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3752\n",
            "2025-03-25 11:20:25,299 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3673\n",
            "2025-03-25 11:20:32,867 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3447\n",
            "2025-03-25 11:20:40,066 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3711\n",
            "2025-03-25 11:20:47,490 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3591\n",
            "2025-03-25 11:20:54,888 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3483\n",
            "2025-03-25 11:21:02,251 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3816\n",
            "2025-03-25 11:21:09,447 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4285\n",
            "2025-03-25 11:21:16,889 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3829\n",
            "2025-03-25 11:21:24,177 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4291\n",
            "2025-03-25 11:21:31,464 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4001\n",
            "2025-03-25 11:21:38,844 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3677\n",
            "2025-03-25 11:21:46,031 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4307\n",
            "2025-03-25 11:21:53,471 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4254\n",
            "2025-03-25 11:22:00,851 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3907\n",
            "2025-03-25 11:22:08,228 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4121\n",
            "2025-03-25 11:22:15,413 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3624\n",
            "2025-03-25 11:22:22,853 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3596\n",
            "2025-03-25 11:22:30,003 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3808\n",
            "2025-03-25 11:22:37,436 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4247\n",
            "2025-03-25 11:22:44,644 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3968\n",
            "2025-03-25 11:22:51,938 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3878\n",
            "2025-03-25 11:22:58,996 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4330\n",
            "2025-03-25 11:23:06,283 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3919\n",
            "2025-03-25 11:23:13,650 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4066\n",
            "2025-03-25 11:23:21,047 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4431\n",
            "2025-03-25 11:23:28,374 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3953\n",
            "2025-03-25 11:23:35,758 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3848\n",
            "2025-03-25 11:23:43,131 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3883\n",
            "2025-03-25 11:23:50,373 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3660\n",
            "2025-03-25 11:23:57,769 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3864\n",
            "2025-03-25 11:24:05,019 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4011\n",
            "2025-03-25 11:24:10,508 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2676\n",
            "2025-03-25 11:24:11,057 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1097\n",
            "2025-03-25 11:24:11,058 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:25:05,274 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3899, Val Loss: 0.4052, Val Acc: 0.8618\n",
            "2025-03-25 11:25:05,275 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-25 11:25:10,736 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2450\n",
            "2025-03-25 11:25:18,193 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3497\n",
            "2025-03-25 11:25:25,549 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3395\n",
            "2025-03-25 11:25:32,992 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3690\n",
            "2025-03-25 11:25:40,222 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3018\n",
            "2025-03-25 11:25:47,587 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3462\n",
            "2025-03-25 11:25:54,921 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3755\n",
            "2025-03-25 11:26:02,186 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3392\n",
            "2025-03-25 11:26:09,481 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3274\n",
            "2025-03-25 11:26:16,794 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3280\n",
            "2025-03-25 11:26:24,366 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3348\n",
            "2025-03-25 11:26:31,649 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3275\n",
            "2025-03-25 11:26:38,857 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3376\n",
            "2025-03-25 11:26:46,354 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3706\n",
            "2025-03-25 11:26:53,750 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3299\n",
            "2025-03-25 11:27:00,959 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3340\n",
            "2025-03-25 11:27:08,284 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3393\n",
            "2025-03-25 11:27:15,465 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3275\n",
            "2025-03-25 11:27:22,746 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3523\n",
            "2025-03-25 11:27:29,893 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3279\n",
            "2025-03-25 11:27:37,323 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3420\n",
            "2025-03-25 11:27:44,558 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3702\n",
            "2025-03-25 11:27:51,885 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3508\n",
            "2025-03-25 11:27:59,547 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3161\n",
            "2025-03-25 11:28:06,880 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3270\n",
            "2025-03-25 11:28:14,354 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3251\n",
            "2025-03-25 11:28:21,725 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3300\n",
            "2025-03-25 11:28:28,934 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3741\n",
            "2025-03-25 11:28:36,149 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3809\n",
            "2025-03-25 11:28:43,406 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3296\n",
            "2025-03-25 11:28:50,529 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3215\n",
            "2025-03-25 11:28:57,928 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3558\n",
            "2025-03-25 11:29:05,221 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3062\n",
            "2025-03-25 11:29:10,640 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2257\n",
            "2025-03-25 11:29:11,189 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0755\n",
            "2025-03-25 11:29:11,190 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:30:05,167 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3388, Val Loss: 0.3884, Val Acc: 0.8641\n",
            "2025-03-25 11:30:05,167 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 11:30:10,725 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2445\n",
            "2025-03-25 11:30:18,293 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3018\n",
            "2025-03-25 11:30:25,623 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3231\n",
            "2025-03-25 11:30:32,794 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2961\n",
            "2025-03-25 11:30:40,283 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3510\n",
            "2025-03-25 11:30:47,495 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-25 11:30:54,873 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3115\n",
            "2025-03-25 11:31:02,075 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2949\n",
            "2025-03-25 11:31:09,361 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3123\n",
            "2025-03-25 11:31:16,704 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3008\n",
            "2025-03-25 11:31:24,079 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3217\n",
            "2025-03-25 11:31:31,416 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3098\n",
            "2025-03-25 11:31:38,675 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3248\n",
            "2025-03-25 11:31:46,027 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3673\n",
            "2025-03-25 11:31:53,415 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3024\n",
            "2025-03-25 11:32:00,714 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3029\n",
            "2025-03-25 11:32:08,067 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3193\n",
            "2025-03-25 11:32:15,661 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3013\n",
            "2025-03-25 11:32:22,883 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3391\n",
            "2025-03-25 11:32:30,066 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3122\n",
            "2025-03-25 11:32:37,332 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-25 11:32:44,641 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3202\n",
            "2025-03-25 11:32:51,849 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-25 11:32:59,144 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2840\n",
            "2025-03-25 11:33:06,559 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3548\n",
            "2025-03-25 11:33:13,845 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2831\n",
            "2025-03-25 11:33:21,048 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3316\n",
            "2025-03-25 11:33:29,064 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3195\n",
            "2025-03-25 11:33:36,464 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3074\n",
            "2025-03-25 11:33:43,837 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3638\n",
            "2025-03-25 11:33:51,199 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3556\n",
            "2025-03-25 11:33:58,832 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3440\n",
            "2025-03-25 11:34:06,138 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-25 11:34:11,621 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2307\n",
            "2025-03-25 11:34:12,190 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0638\n",
            "2025-03-25 11:34:12,190 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:35:06,736 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3153, Val Loss: 0.3668, Val Acc: 0.8692\n",
            "2025-03-25 11:35:06,736 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 11:35:12,411 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2256\n",
            "2025-03-25 11:35:19,681 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3255\n",
            "2025-03-25 11:35:27,006 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3025\n",
            "2025-03-25 11:35:34,370 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2929\n",
            "2025-03-25 11:35:41,789 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3010\n",
            "2025-03-25 11:35:48,916 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2993\n",
            "2025-03-25 11:35:56,223 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2801\n",
            "2025-03-25 11:36:03,519 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3518\n",
            "2025-03-25 11:36:10,797 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2793\n",
            "2025-03-25 11:36:18,111 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3037\n",
            "2025-03-25 11:36:25,522 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2960\n",
            "2025-03-25 11:36:32,995 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2997\n",
            "2025-03-25 11:36:40,183 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3188\n",
            "2025-03-25 11:36:47,497 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3096\n",
            "2025-03-25 11:36:54,678 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3245\n",
            "2025-03-25 11:37:01,906 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3185\n",
            "2025-03-25 11:37:09,362 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3062\n",
            "2025-03-25 11:37:16,475 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3269\n",
            "2025-03-25 11:37:23,703 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2935\n",
            "2025-03-25 11:37:31,095 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3219\n",
            "2025-03-25 11:37:38,382 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-25 11:37:45,808 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3487\n",
            "2025-03-25 11:37:53,053 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-25 11:38:00,544 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3233\n",
            "2025-03-25 11:38:07,943 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3539\n",
            "2025-03-25 11:38:15,294 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3285\n",
            "2025-03-25 11:38:22,365 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3061\n",
            "2025-03-25 11:38:29,744 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2972\n",
            "2025-03-25 11:38:37,081 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3047\n",
            "2025-03-25 11:38:44,527 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2705\n",
            "2025-03-25 11:38:51,729 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3033\n",
            "2025-03-25 11:38:59,005 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2956\n",
            "2025-03-25 11:39:06,418 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2952\n",
            "2025-03-25 11:39:11,892 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2258\n",
            "2025-03-25 11:39:12,443 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0574\n",
            "2025-03-25 11:39:12,443 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:40:06,647 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3072, Val Loss: 0.3848, Val Acc: 0.8702\n",
            "2025-03-25 11:40:06,648 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 11:40:12,035 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2259\n",
            "2025-03-25 11:40:19,308 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-25 11:40:26,716 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-25 11:40:34,082 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-25 11:40:41,234 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-25 11:40:48,695 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2991\n",
            "2025-03-25 11:40:55,826 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3076\n",
            "2025-03-25 11:41:03,102 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-25 11:41:10,507 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3167\n",
            "2025-03-25 11:41:17,903 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3037\n",
            "2025-03-25 11:41:25,270 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-25 11:41:32,881 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2877\n",
            "2025-03-25 11:41:40,090 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-25 11:41:47,415 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3105\n",
            "2025-03-25 11:41:54,446 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3086\n",
            "2025-03-25 11:42:01,883 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2749\n",
            "2025-03-25 11:42:09,019 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3306\n",
            "2025-03-25 11:42:16,163 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2959\n",
            "2025-03-25 11:42:23,476 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3095\n",
            "2025-03-25 11:42:30,792 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3117\n",
            "2025-03-25 11:42:37,950 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3062\n",
            "2025-03-25 11:42:45,272 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2847\n",
            "2025-03-25 11:42:52,397 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2936\n",
            "2025-03-25 11:42:59,789 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-25 11:43:07,064 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-25 11:43:14,337 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2925\n",
            "2025-03-25 11:43:21,641 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-25 11:43:29,058 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2786\n",
            "2025-03-25 11:43:36,376 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3238\n",
            "2025-03-25 11:43:43,839 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2851\n",
            "2025-03-25 11:43:51,050 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-25 11:43:58,462 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2945\n",
            "2025-03-25 11:44:05,868 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-25 11:44:11,426 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2081\n",
            "2025-03-25 11:44:11,968 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1047\n",
            "2025-03-25 11:44:11,969 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:45:06,120 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.2972, Val Loss: 0.3825, Val Acc: 0.8655\n",
            "2025-03-25 11:45:06,121 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 11:45:11,619 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2049\n",
            "2025-03-25 11:45:19,203 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2929\n",
            "2025-03-25 11:45:26,430 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2958\n",
            "2025-03-25 11:45:33,821 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3158\n",
            "2025-03-25 11:45:41,052 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-25 11:45:48,417 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2729\n",
            "2025-03-25 11:45:55,678 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-25 11:46:02,851 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 11:46:10,163 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2807\n",
            "2025-03-25 11:46:17,406 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2876\n",
            "2025-03-25 11:46:24,520 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2835\n",
            "2025-03-25 11:46:31,999 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2798\n",
            "2025-03-25 11:46:39,298 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2660\n",
            "2025-03-25 11:46:46,615 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2676\n",
            "2025-03-25 11:46:53,828 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-25 11:47:01,017 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2663\n",
            "2025-03-25 11:47:08,588 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2807\n",
            "2025-03-25 11:47:15,743 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-25 11:47:23,372 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2958\n",
            "2025-03-25 11:47:30,715 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-25 11:47:38,182 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2922\n",
            "2025-03-25 11:47:45,462 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2663\n",
            "2025-03-25 11:47:52,779 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-25 11:48:00,091 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2757\n",
            "2025-03-25 11:48:07,284 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2704\n",
            "2025-03-25 11:48:14,694 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2723\n",
            "2025-03-25 11:48:22,149 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2745\n",
            "2025-03-25 11:48:29,359 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2818\n",
            "2025-03-25 11:48:36,728 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-25 11:48:44,282 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2890\n",
            "2025-03-25 11:48:51,745 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2745\n",
            "2025-03-25 11:48:58,899 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2795\n",
            "2025-03-25 11:49:06,371 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-25 11:49:11,966 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1971\n",
            "2025-03-25 11:49:12,497 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0868\n",
            "2025-03-25 11:49:12,498 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:50:06,591 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2801, Val Loss: 0.3819, Val Acc: 0.8720\n",
            "2025-03-25 11:50:06,592 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 11:50:12,051 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2312\n",
            "2025-03-25 11:50:19,346 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2440\n",
            "2025-03-25 11:50:26,744 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-25 11:50:34,026 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-25 11:50:41,522 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2807\n",
            "2025-03-25 11:50:48,822 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2472\n",
            "2025-03-25 11:50:56,306 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2847\n",
            "2025-03-25 11:51:03,466 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2961\n",
            "2025-03-25 11:51:10,716 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2710\n",
            "2025-03-25 11:51:17,880 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2834\n",
            "2025-03-25 11:51:25,190 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2714\n",
            "2025-03-25 11:51:32,502 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-25 11:51:39,603 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2837\n",
            "2025-03-25 11:51:46,833 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-25 11:51:54,099 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2663\n",
            "2025-03-25 11:52:01,461 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2802\n",
            "2025-03-25 11:52:08,695 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2506\n",
            "2025-03-25 11:52:16,281 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2673\n",
            "2025-03-25 11:52:23,695 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2673\n",
            "2025-03-25 11:52:30,887 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3138\n",
            "2025-03-25 11:52:38,284 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2860\n",
            "2025-03-25 11:52:45,437 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2728\n",
            "2025-03-25 11:52:52,883 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-25 11:53:00,247 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3087\n",
            "2025-03-25 11:53:07,438 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2593\n",
            "2025-03-25 11:53:15,065 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2723\n",
            "2025-03-25 11:53:22,172 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2745\n",
            "2025-03-25 11:53:29,429 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2590\n",
            "2025-03-25 11:53:36,867 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-25 11:53:44,080 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-25 11:53:51,655 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2869\n",
            "2025-03-25 11:53:58,910 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2974\n",
            "2025-03-25 11:54:06,458 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2937\n",
            "2025-03-25 11:54:12,048 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2116\n",
            "2025-03-25 11:54:12,579 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1144\n",
            "2025-03-25 11:54:12,580 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 11:55:06,741 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2775, Val Loss: 0.3830, Val Acc: 0.8674\n",
            "2025-03-25 11:55:06,742 - INFO - [TRAIN INFO] Early stopping at epoch 16 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 11:55:06,743 - INFO - [TRAIN INFO] Total Time: 4811.49s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▇▆▆▅▅▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇█▇█████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.2775</td></tr><tr><td>train_val_loss_diff</td><td>-0.10545</td></tr><tr><td>val_accuracy</td><td>0.86738</td></tr><tr><td>val_loss</td><td>0.38295</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_only_fold_2</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/7sd1wx0s' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/7sd1wx0s</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250325_103454-7sd1wx0s\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 11:55:09,009 - INFO - [TRAIN INFO] Fold 2 Training Complete at epoch 16. Total Time: 4813.76s\n",
            "2025-03-25 11:55:09,026 - INFO - [K-FOLD INFO] Fold 2 completed in 4815.37 seconds\n",
            "2025-03-25 11:55:09,026 - INFO - [K-FOLD INFO] ============================== Fold 3/5 ==============================\n",
            "2025-03-25 11:55:09,028 - INFO - [K-FOLD INFO] Fold 3:\n",
            "2025-03-25 11:55:09,029 - INFO -    Train Samples: 8594\n",
            "2025-03-25 11:55:09,029 - INFO -    Validation Samples: 2149\n",
            "2025-03-25 11:55:09,030 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 3\n",
            "2025-03-25 11:55:09,030 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 3:\n",
            "2025-03-25 11:55:09,031 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 11:55:09,689 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 3\n",
            "2025-03-25 11:55:09,690 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 3:\n",
            "2025-03-25 11:55:09,691 - INFO - [K-FOLD INFO] Loss function initialized for Fold 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_only\\wandb\\run-20250325_115509-wdvlzkot</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wdvlzkot' target=\"_blank\">experiment_multimodal_attention_only_fold_3</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wdvlzkot' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wdvlzkot</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 11:55:10,358 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 11:55:10,359 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-25 11:55:16,101 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.1385\n",
            "2025-03-25 11:55:23,444 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.5331\n",
            "2025-03-25 11:55:30,737 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.4471\n",
            "2025-03-25 11:55:38,052 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.4506\n",
            "2025-03-25 11:55:45,370 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3578\n",
            "2025-03-25 11:55:52,589 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3835\n",
            "2025-03-25 11:55:59,974 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.3249\n",
            "2025-03-25 11:56:07,425 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.3226\n",
            "2025-03-25 11:56:14,811 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2536\n",
            "2025-03-25 11:56:22,027 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2183\n",
            "2025-03-25 11:56:29,224 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2232\n",
            "2025-03-25 11:56:36,517 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1499\n",
            "2025-03-25 11:56:43,648 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1784\n",
            "2025-03-25 11:56:51,013 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1405\n",
            "2025-03-25 11:56:58,342 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1444\n",
            "2025-03-25 11:57:05,765 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.0631\n",
            "2025-03-25 11:57:13,108 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1045\n",
            "2025-03-25 11:57:20,406 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.1211\n",
            "2025-03-25 11:57:27,816 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.1338\n",
            "2025-03-25 11:57:34,987 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.1025\n",
            "2025-03-25 11:57:42,582 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0617\n",
            "2025-03-25 11:57:49,719 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0939\n",
            "2025-03-25 11:57:57,579 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0748\n",
            "2025-03-25 11:58:04,711 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0734\n",
            "2025-03-25 11:58:11,948 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 1.0191\n",
            "2025-03-25 11:58:19,182 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0210\n",
            "2025-03-25 11:58:26,385 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 1.0186\n",
            "2025-03-25 11:58:33,975 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9679\n",
            "2025-03-25 11:58:41,176 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9143\n",
            "2025-03-25 11:58:48,372 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9142\n",
            "2025-03-25 11:58:55,444 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8469\n",
            "2025-03-25 11:59:02,771 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 1.0215\n",
            "2025-03-25 11:59:09,824 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9395\n",
            "2025-03-25 11:59:15,398 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7289\n",
            "2025-03-25 11:59:15,964 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2455\n",
            "2025-03-25 11:59:15,964 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:00:10,540 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1476, Val Loss: 0.8016, Val Acc: 0.6705\n",
            "2025-03-25 12:00:10,832 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:00:10,832 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 12:00:16,565 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6666\n",
            "2025-03-25 12:00:23,931 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9069\n",
            "2025-03-25 12:00:31,289 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8930\n",
            "2025-03-25 12:00:38,697 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8756\n",
            "2025-03-25 12:00:46,141 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8690\n",
            "2025-03-25 12:00:53,510 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.9008\n",
            "2025-03-25 12:01:01,122 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8584\n",
            "2025-03-25 12:01:08,464 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8071\n",
            "2025-03-25 12:01:15,728 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8175\n",
            "2025-03-25 12:01:23,048 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7632\n",
            "2025-03-25 12:01:30,539 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8719\n",
            "2025-03-25 12:01:38,109 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8150\n",
            "2025-03-25 12:01:45,433 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8742\n",
            "2025-03-25 12:01:52,769 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8105\n",
            "2025-03-25 12:02:00,239 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7724\n",
            "2025-03-25 12:02:07,725 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8627\n",
            "2025-03-25 12:02:15,191 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7905\n",
            "2025-03-25 12:02:22,348 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7966\n",
            "2025-03-25 12:02:29,695 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7737\n",
            "2025-03-25 12:02:36,892 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8105\n",
            "2025-03-25 12:02:44,109 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7398\n",
            "2025-03-25 12:02:51,496 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.8136\n",
            "2025-03-25 12:02:58,599 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7958\n",
            "2025-03-25 12:03:05,696 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7660\n",
            "2025-03-25 12:03:13,093 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7342\n",
            "2025-03-25 12:03:20,177 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7186\n",
            "2025-03-25 12:03:27,427 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7459\n",
            "2025-03-25 12:03:34,705 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7887\n",
            "2025-03-25 12:03:42,036 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7478\n",
            "2025-03-25 12:03:49,671 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.8200\n",
            "2025-03-25 12:03:57,341 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7657\n",
            "2025-03-25 12:04:04,603 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7066\n",
            "2025-03-25 12:04:12,030 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6947\n",
            "2025-03-25 12:04:17,670 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5169\n",
            "2025-03-25 12:04:18,219 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1936\n",
            "2025-03-25 12:04:18,220 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:05:12,862 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8025, Val Loss: 0.5824, Val Acc: 0.7683\n",
            "2025-03-25 12:05:13,176 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:05:13,177 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 12:05:18,823 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4760\n",
            "2025-03-25 12:05:26,201 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6773\n",
            "2025-03-25 12:05:33,632 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6963\n",
            "2025-03-25 12:05:41,242 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6544\n",
            "2025-03-25 12:05:48,551 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7903\n",
            "2025-03-25 12:05:55,912 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6952\n",
            "2025-03-25 12:06:03,092 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6838\n",
            "2025-03-25 12:06:10,379 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6267\n",
            "2025-03-25 12:06:17,824 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6492\n",
            "2025-03-25 12:06:25,226 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6492\n",
            "2025-03-25 12:06:32,435 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6385\n",
            "2025-03-25 12:06:39,783 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6123\n",
            "2025-03-25 12:06:47,029 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7856\n",
            "2025-03-25 12:06:54,328 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6074\n",
            "2025-03-25 12:07:01,530 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6461\n",
            "2025-03-25 12:07:08,816 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6715\n",
            "2025-03-25 12:07:15,932 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7075\n",
            "2025-03-25 12:07:23,119 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6350\n",
            "2025-03-25 12:07:30,421 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6748\n",
            "2025-03-25 12:07:37,592 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5970\n",
            "2025-03-25 12:07:45,008 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6640\n",
            "2025-03-25 12:07:52,245 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6626\n",
            "2025-03-25 12:07:59,589 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6815\n",
            "2025-03-25 12:08:07,197 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6012\n",
            "2025-03-25 12:08:14,572 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6853\n",
            "2025-03-25 12:08:21,797 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6551\n",
            "2025-03-25 12:08:29,195 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6551\n",
            "2025-03-25 12:08:36,315 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6289\n",
            "2025-03-25 12:08:43,513 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6292\n",
            "2025-03-25 12:08:50,695 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6045\n",
            "2025-03-25 12:08:58,030 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6152\n",
            "2025-03-25 12:09:05,500 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6178\n",
            "2025-03-25 12:09:12,720 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5866\n",
            "2025-03-25 12:09:18,052 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4675\n",
            "2025-03-25 12:09:18,585 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1135\n",
            "2025-03-25 12:09:18,586 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:10:13,387 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6531, Val Loss: 0.4816, Val Acc: 0.8120\n",
            "2025-03-25 12:10:13,697 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:10:13,697 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 12:10:19,289 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4226\n",
            "2025-03-25 12:10:26,588 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5792\n",
            "2025-03-25 12:10:33,866 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5765\n",
            "2025-03-25 12:10:41,355 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6172\n",
            "2025-03-25 12:10:48,744 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5339\n",
            "2025-03-25 12:10:56,229 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5450\n",
            "2025-03-25 12:11:03,552 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5805\n",
            "2025-03-25 12:11:10,863 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5668\n",
            "2025-03-25 12:11:18,132 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5613\n",
            "2025-03-25 12:11:25,241 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5687\n",
            "2025-03-25 12:11:32,569 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5405\n",
            "2025-03-25 12:11:39,958 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5674\n",
            "2025-03-25 12:11:47,336 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6266\n",
            "2025-03-25 12:11:54,730 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5447\n",
            "2025-03-25 12:12:02,043 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6137\n",
            "2025-03-25 12:12:09,716 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5404\n",
            "2025-03-25 12:12:16,848 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5736\n",
            "2025-03-25 12:12:24,206 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4876\n",
            "2025-03-25 12:12:31,537 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6079\n",
            "2025-03-25 12:12:38,804 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5548\n",
            "2025-03-25 12:12:46,288 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5734\n",
            "2025-03-25 12:12:53,513 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5807\n",
            "2025-03-25 12:13:00,710 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5333\n",
            "2025-03-25 12:13:07,899 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4939\n",
            "2025-03-25 12:13:15,214 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5880\n",
            "2025-03-25 12:13:22,402 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5568\n",
            "2025-03-25 12:13:29,702 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5934\n",
            "2025-03-25 12:13:36,914 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6442\n",
            "2025-03-25 12:13:44,301 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5460\n",
            "2025-03-25 12:13:51,666 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5529\n",
            "2025-03-25 12:13:58,996 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6090\n",
            "2025-03-25 12:14:06,309 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5842\n",
            "2025-03-25 12:14:13,878 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5960\n",
            "2025-03-25 12:14:19,122 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3859\n",
            "2025-03-25 12:14:19,687 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1291\n",
            "2025-03-25 12:14:19,687 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:15:14,267 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5682, Val Loss: 0.4252, Val Acc: 0.8404\n",
            "2025-03-25 12:15:14,580 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:15:14,581 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 12:15:20,441 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3694\n",
            "2025-03-25 12:15:27,642 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5240\n",
            "2025-03-25 12:15:35,080 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5309\n",
            "2025-03-25 12:15:42,646 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4950\n",
            "2025-03-25 12:15:49,823 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5491\n",
            "2025-03-25 12:15:57,354 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5536\n",
            "2025-03-25 12:16:04,747 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5436\n",
            "2025-03-25 12:16:11,947 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4498\n",
            "2025-03-25 12:16:19,360 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4798\n",
            "2025-03-25 12:16:26,668 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5209\n",
            "2025-03-25 12:16:34,275 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5611\n",
            "2025-03-25 12:16:41,438 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5277\n",
            "2025-03-25 12:16:49,029 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5116\n",
            "2025-03-25 12:16:56,512 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5515\n",
            "2025-03-25 12:17:03,770 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5607\n",
            "2025-03-25 12:17:11,221 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5213\n",
            "2025-03-25 12:17:18,625 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5739\n",
            "2025-03-25 12:17:25,748 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5174\n",
            "2025-03-25 12:17:32,939 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5108\n",
            "2025-03-25 12:17:40,194 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5317\n",
            "2025-03-25 12:17:47,477 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5361\n",
            "2025-03-25 12:17:54,842 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5166\n",
            "2025-03-25 12:18:02,239 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4629\n",
            "2025-03-25 12:18:09,485 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5045\n",
            "2025-03-25 12:18:16,637 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6009\n",
            "2025-03-25 12:18:23,921 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5695\n",
            "2025-03-25 12:18:31,232 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4777\n",
            "2025-03-25 12:18:38,282 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4658\n",
            "2025-03-25 12:18:45,794 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6051\n",
            "2025-03-25 12:18:52,890 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5645\n",
            "2025-03-25 12:19:00,065 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4844\n",
            "2025-03-25 12:19:07,406 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4982\n",
            "2025-03-25 12:19:14,594 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5106\n",
            "2025-03-25 12:19:20,388 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3761\n",
            "2025-03-25 12:19:20,926 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0928\n",
            "2025-03-25 12:19:20,927 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:20:19,166 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5230, Val Loss: 0.4214, Val Acc: 0.8371\n",
            "2025-03-25 12:20:19,508 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:20:19,509 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 12:20:25,728 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4022\n",
            "2025-03-25 12:20:33,405 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4678\n",
            "2025-03-25 12:20:40,890 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4714\n",
            "2025-03-25 12:20:48,538 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4754\n",
            "2025-03-25 12:20:56,141 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5508\n",
            "2025-03-25 12:21:03,511 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5010\n",
            "2025-03-25 12:21:11,102 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4444\n",
            "2025-03-25 12:21:18,673 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4316\n",
            "2025-03-25 12:21:27,358 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5478\n",
            "2025-03-25 12:21:35,756 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4866\n",
            "2025-03-25 12:21:43,749 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4772\n",
            "2025-03-25 12:21:51,349 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4375\n",
            "2025-03-25 12:21:58,826 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4712\n",
            "2025-03-25 12:22:06,344 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4400\n",
            "2025-03-25 12:22:13,740 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5072\n",
            "2025-03-25 12:22:21,150 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5084\n",
            "2025-03-25 12:22:28,490 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5536\n",
            "2025-03-25 12:22:35,975 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4754\n",
            "2025-03-25 12:22:43,390 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4451\n",
            "2025-03-25 12:22:51,127 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4422\n",
            "2025-03-25 12:22:58,365 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4681\n",
            "2025-03-25 12:23:05,394 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5368\n",
            "2025-03-25 12:23:13,128 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5061\n",
            "2025-03-25 12:23:20,629 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4670\n",
            "2025-03-25 12:23:28,109 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5268\n",
            "2025-03-25 12:23:35,706 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4675\n",
            "2025-03-25 12:23:43,098 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4544\n",
            "2025-03-25 12:23:50,747 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4713\n",
            "2025-03-25 12:23:58,132 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4777\n",
            "2025-03-25 12:24:05,492 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5169\n",
            "2025-03-25 12:24:13,106 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5309\n",
            "2025-03-25 12:24:20,380 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5584\n",
            "2025-03-25 12:24:27,624 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5514\n",
            "2025-03-25 12:24:33,050 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3630\n",
            "2025-03-25 12:24:33,616 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1038\n",
            "2025-03-25 12:24:33,617 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:25:29,490 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4900, Val Loss: 0.4011, Val Acc: 0.8525\n",
            "2025-03-25 12:25:29,812 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:25:29,812 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 12:25:35,648 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3420\n",
            "2025-03-25 12:25:43,280 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4918\n",
            "2025-03-25 12:25:50,837 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4620\n",
            "2025-03-25 12:25:58,389 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3991\n",
            "2025-03-25 12:26:05,796 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4288\n",
            "2025-03-25 12:26:13,133 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4206\n",
            "2025-03-25 12:26:20,865 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4599\n",
            "2025-03-25 12:26:28,197 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4158\n",
            "2025-03-25 12:26:35,562 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4406\n",
            "2025-03-25 12:26:43,061 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5223\n",
            "2025-03-25 12:26:50,454 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4345\n",
            "2025-03-25 12:26:57,833 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4452\n",
            "2025-03-25 12:27:05,271 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4298\n",
            "2025-03-25 12:27:13,051 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5014\n",
            "2025-03-25 12:27:20,418 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4852\n",
            "2025-03-25 12:27:28,017 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4324\n",
            "2025-03-25 12:27:35,466 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4658\n",
            "2025-03-25 12:27:43,047 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4357\n",
            "2025-03-25 12:27:50,162 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4416\n",
            "2025-03-25 12:27:57,628 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4918\n",
            "2025-03-25 12:28:04,854 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4862\n",
            "2025-03-25 12:28:12,421 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4477\n",
            "2025-03-25 12:28:19,786 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5224\n",
            "2025-03-25 12:28:27,246 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4450\n",
            "2025-03-25 12:28:34,624 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4519\n",
            "2025-03-25 12:28:41,823 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4959\n",
            "2025-03-25 12:28:49,392 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4540\n",
            "2025-03-25 12:28:56,763 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4684\n",
            "2025-03-25 12:29:04,218 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5158\n",
            "2025-03-25 12:29:11,610 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4062\n",
            "2025-03-25 12:29:19,019 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4839\n",
            "2025-03-25 12:29:26,536 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4504\n",
            "2025-03-25 12:29:33,750 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4638\n",
            "2025-03-25 12:29:39,365 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3157\n",
            "2025-03-25 12:29:39,887 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1410\n",
            "2025-03-25 12:29:39,887 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:30:35,123 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4591, Val Loss: 0.4024, Val Acc: 0.8511\n",
            "2025-03-25 12:30:35,124 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-25 12:30:40,541 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3598\n",
            "2025-03-25 12:30:48,173 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3872\n",
            "2025-03-25 12:30:55,640 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4257\n",
            "2025-03-25 12:31:03,166 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4334\n",
            "2025-03-25 12:31:10,576 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4390\n",
            "2025-03-25 12:31:17,955 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4523\n",
            "2025-03-25 12:31:25,559 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4016\n",
            "2025-03-25 12:31:32,867 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4438\n",
            "2025-03-25 12:31:40,169 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4749\n",
            "2025-03-25 12:31:47,761 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4050\n",
            "2025-03-25 12:31:55,076 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3990\n",
            "2025-03-25 12:32:02,356 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3892\n",
            "2025-03-25 12:32:09,951 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4063\n",
            "2025-03-25 12:32:17,393 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4102\n",
            "2025-03-25 12:32:24,950 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4371\n",
            "2025-03-25 12:32:32,126 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4601\n",
            "2025-03-25 12:32:39,451 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4284\n",
            "2025-03-25 12:32:46,935 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4298\n",
            "2025-03-25 12:32:54,355 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4176\n",
            "2025-03-25 12:33:01,745 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4355\n",
            "2025-03-25 12:33:09,228 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4068\n",
            "2025-03-25 12:33:16,424 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4681\n",
            "2025-03-25 12:33:24,540 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4194\n",
            "2025-03-25 12:33:32,025 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4143\n",
            "2025-03-25 12:33:39,520 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3853\n",
            "2025-03-25 12:33:46,892 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4349\n",
            "2025-03-25 12:33:54,519 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3967\n",
            "2025-03-25 12:34:01,926 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4091\n",
            "2025-03-25 12:34:09,166 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4657\n",
            "2025-03-25 12:34:16,706 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5333\n",
            "2025-03-25 12:34:23,998 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4451\n",
            "2025-03-25 12:34:31,252 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4603\n",
            "2025-03-25 12:34:38,653 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4177\n",
            "2025-03-25 12:34:44,074 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3362\n",
            "2025-03-25 12:34:44,588 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1387\n",
            "2025-03-25 12:34:44,589 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:35:40,119 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4316, Val Loss: 0.4163, Val Acc: 0.8464\n",
            "2025-03-25 12:35:40,119 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-25 12:35:45,540 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3051\n",
            "2025-03-25 12:35:53,091 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3996\n",
            "2025-03-25 12:36:00,504 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4408\n",
            "2025-03-25 12:36:08,082 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3698\n",
            "2025-03-25 12:36:15,281 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4008\n",
            "2025-03-25 12:36:22,681 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4282\n",
            "2025-03-25 12:36:29,987 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3835\n",
            "2025-03-25 12:36:37,347 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3951\n",
            "2025-03-25 12:36:44,875 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3748\n",
            "2025-03-25 12:36:52,195 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4072\n",
            "2025-03-25 12:36:59,571 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3813\n",
            "2025-03-25 12:37:07,068 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4339\n",
            "2025-03-25 12:37:14,447 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4241\n",
            "2025-03-25 12:37:21,862 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4039\n",
            "2025-03-25 12:37:29,008 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4315\n",
            "2025-03-25 12:37:36,347 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4042\n",
            "2025-03-25 12:37:43,853 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4338\n",
            "2025-03-25 12:37:51,216 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3965\n",
            "2025-03-25 12:37:58,840 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4380\n",
            "2025-03-25 12:38:06,245 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3800\n",
            "2025-03-25 12:38:13,646 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4048\n",
            "2025-03-25 12:38:20,966 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4141\n",
            "2025-03-25 12:38:28,299 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4418\n",
            "2025-03-25 12:38:35,537 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4656\n",
            "2025-03-25 12:38:42,826 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4358\n",
            "2025-03-25 12:38:50,245 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4462\n",
            "2025-03-25 12:38:57,818 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3940\n",
            "2025-03-25 12:39:05,173 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4096\n",
            "2025-03-25 12:39:12,630 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4218\n",
            "2025-03-25 12:39:20,125 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3940\n",
            "2025-03-25 12:39:27,397 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3489\n",
            "2025-03-25 12:39:34,968 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3738\n",
            "2025-03-25 12:39:42,422 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4757\n",
            "2025-03-25 12:39:47,831 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2853\n",
            "2025-03-25 12:39:48,394 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1849\n",
            "2025-03-25 12:39:48,395 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:40:43,733 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4127, Val Loss: 0.3964, Val Acc: 0.8576\n",
            "2025-03-25 12:40:44,040 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:40:44,041 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-25 12:40:49,554 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2747\n",
            "2025-03-25 12:40:57,180 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4132\n",
            "2025-03-25 12:41:04,577 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4050\n",
            "2025-03-25 12:41:11,995 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3969\n",
            "2025-03-25 12:41:19,341 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3677\n",
            "2025-03-25 12:41:26,981 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3784\n",
            "2025-03-25 12:41:34,401 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3543\n",
            "2025-03-25 12:41:41,787 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3909\n",
            "2025-03-25 12:41:49,139 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3593\n",
            "2025-03-25 12:41:56,598 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3497\n",
            "2025-03-25 12:42:03,983 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3719\n",
            "2025-03-25 12:42:11,330 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3475\n",
            "2025-03-25 12:42:18,846 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4183\n",
            "2025-03-25 12:42:26,097 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4021\n",
            "2025-03-25 12:42:33,586 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4206\n",
            "2025-03-25 12:42:40,969 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3741\n",
            "2025-03-25 12:42:48,328 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4022\n",
            "2025-03-25 12:42:55,813 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3708\n",
            "2025-03-25 12:43:03,056 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4283\n",
            "2025-03-25 12:43:10,476 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3389\n",
            "2025-03-25 12:43:17,850 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4571\n",
            "2025-03-25 12:43:25,172 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3773\n",
            "2025-03-25 12:43:32,741 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3947\n",
            "2025-03-25 12:43:40,114 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3830\n",
            "2025-03-25 12:43:47,561 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3858\n",
            "2025-03-25 12:43:54,946 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3843\n",
            "2025-03-25 12:44:02,450 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3569\n",
            "2025-03-25 12:44:09,837 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4359\n",
            "2025-03-25 12:44:17,144 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3900\n",
            "2025-03-25 12:44:24,501 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3846\n",
            "2025-03-25 12:44:31,878 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4179\n",
            "2025-03-25 12:44:39,354 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4209\n",
            "2025-03-25 12:44:46,443 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3739\n",
            "2025-03-25 12:44:52,116 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3230\n",
            "2025-03-25 12:44:52,674 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1007\n",
            "2025-03-25 12:44:52,675 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:45:47,714 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3897, Val Loss: 0.3928, Val Acc: 0.8585\n",
            "2025-03-25 12:45:48,025 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:45:48,026 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-25 12:45:53,710 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2461\n",
            "2025-03-25 12:46:00,892 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3483\n",
            "2025-03-25 12:46:08,484 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3548\n",
            "2025-03-25 12:46:15,812 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3402\n",
            "2025-03-25 12:46:23,220 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3502\n",
            "2025-03-25 12:46:30,784 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3898\n",
            "2025-03-25 12:46:38,311 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3410\n",
            "2025-03-25 12:46:45,890 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3662\n",
            "2025-03-25 12:46:53,374 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3385\n",
            "2025-03-25 12:47:00,636 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3628\n",
            "2025-03-25 12:47:08,080 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3307\n",
            "2025-03-25 12:47:15,686 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3652\n",
            "2025-03-25 12:47:22,989 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3287\n",
            "2025-03-25 12:47:30,674 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4054\n",
            "2025-03-25 12:47:37,829 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3631\n",
            "2025-03-25 12:47:45,287 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3616\n",
            "2025-03-25 12:47:52,829 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3725\n",
            "2025-03-25 12:48:00,212 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3802\n",
            "2025-03-25 12:48:07,592 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3498\n",
            "2025-03-25 12:48:15,125 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3838\n",
            "2025-03-25 12:48:22,606 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3570\n",
            "2025-03-25 12:48:29,787 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3802\n",
            "2025-03-25 12:48:37,257 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3446\n",
            "2025-03-25 12:48:44,613 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3310\n",
            "2025-03-25 12:48:51,877 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4295\n",
            "2025-03-25 12:48:59,437 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4048\n",
            "2025-03-25 12:49:06,708 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4301\n",
            "2025-03-25 12:49:14,247 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3541\n",
            "2025-03-25 12:49:21,582 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3495\n",
            "2025-03-25 12:49:29,029 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3637\n",
            "2025-03-25 12:49:36,322 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3682\n",
            "2025-03-25 12:49:43,830 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3855\n",
            "2025-03-25 12:49:51,050 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4213\n",
            "2025-03-25 12:49:56,595 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2561\n",
            "2025-03-25 12:49:57,115 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0784\n",
            "2025-03-25 12:49:57,115 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:50:52,579 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3654, Val Loss: 0.3902, Val Acc: 0.8613\n",
            "2025-03-25 12:50:52,894 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 12:50:52,895 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 12:50:58,828 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3011\n",
            "2025-03-25 12:51:06,253 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3346\n",
            "2025-03-25 12:51:13,784 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3406\n",
            "2025-03-25 12:51:21,179 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3946\n",
            "2025-03-25 12:51:28,807 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3523\n",
            "2025-03-25 12:51:36,194 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3136\n",
            "2025-03-25 12:51:43,795 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3399\n",
            "2025-03-25 12:51:51,219 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3352\n",
            "2025-03-25 12:51:58,770 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3168\n",
            "2025-03-25 12:52:05,871 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4043\n",
            "2025-03-25 12:52:13,137 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3297\n",
            "2025-03-25 12:52:20,777 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3644\n",
            "2025-03-25 12:52:28,169 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-25 12:52:35,331 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3398\n",
            "2025-03-25 12:52:42,766 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3587\n",
            "2025-03-25 12:52:50,059 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3118\n",
            "2025-03-25 12:52:57,406 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3709\n",
            "2025-03-25 12:53:04,778 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3358\n",
            "2025-03-25 12:53:12,367 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3687\n",
            "2025-03-25 12:53:19,810 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3823\n",
            "2025-03-25 12:53:27,355 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3556\n",
            "2025-03-25 12:53:34,951 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3688\n",
            "2025-03-25 12:53:42,361 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3410\n",
            "2025-03-25 12:53:49,486 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3307\n",
            "2025-03-25 12:53:57,143 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3641\n",
            "2025-03-25 12:54:04,509 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3278\n",
            "2025-03-25 12:54:11,955 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3855\n",
            "2025-03-25 12:54:19,164 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-25 12:54:26,547 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3655\n",
            "2025-03-25 12:54:33,883 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3668\n",
            "2025-03-25 12:54:41,221 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3395\n",
            "2025-03-25 12:54:48,537 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3507\n",
            "2025-03-25 12:54:55,705 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3574\n",
            "2025-03-25 12:55:01,223 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-25 12:55:01,763 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0886\n",
            "2025-03-25 12:55:01,763 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 12:55:57,112 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3512, Val Loss: 0.4155, Val Acc: 0.8637\n",
            "2025-03-25 12:55:57,112 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 12:56:02,721 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2161\n",
            "2025-03-25 12:56:10,088 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-25 12:56:17,706 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3385\n",
            "2025-03-25 12:56:25,099 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3175\n",
            "2025-03-25 12:56:32,460 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3362\n",
            "2025-03-25 12:56:39,857 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3099\n",
            "2025-03-25 12:56:47,302 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3411\n",
            "2025-03-25 12:56:54,617 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3906\n",
            "2025-03-25 12:57:02,098 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3113\n",
            "2025-03-25 12:57:09,462 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3319\n",
            "2025-03-25 12:57:16,893 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3588\n",
            "2025-03-25 12:57:24,116 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3196\n",
            "2025-03-25 12:57:31,667 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-25 12:57:38,885 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3247\n",
            "2025-03-25 12:57:46,249 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3624\n",
            "2025-03-25 12:57:53,867 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3158\n",
            "2025-03-25 12:58:01,064 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3075\n",
            "2025-03-25 12:58:08,678 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3189\n",
            "2025-03-25 12:58:15,968 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3354\n",
            "2025-03-25 12:58:23,476 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3301\n",
            "2025-03-25 12:58:30,790 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3733\n",
            "2025-03-25 12:58:38,070 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3292\n",
            "2025-03-25 12:58:45,444 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3369\n",
            "2025-03-25 12:58:53,052 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3277\n",
            "2025-03-25 12:59:00,352 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3002\n",
            "2025-03-25 12:59:07,833 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3352\n",
            "2025-03-25 12:59:15,224 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3010\n",
            "2025-03-25 12:59:22,677 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3924\n",
            "2025-03-25 12:59:30,030 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3673\n",
            "2025-03-25 12:59:37,655 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3514\n",
            "2025-03-25 12:59:45,011 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3290\n",
            "2025-03-25 12:59:52,594 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2990\n",
            "2025-03-25 13:00:00,015 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3592\n",
            "2025-03-25 13:00:05,465 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2924\n",
            "2025-03-25 13:00:05,997 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1326\n",
            "2025-03-25 13:00:05,997 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:01:01,260 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3357, Val Loss: 0.4009, Val Acc: 0.8651\n",
            "2025-03-25 13:01:01,261 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 13:01:06,818 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2474\n",
            "2025-03-25 13:01:14,194 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3101\n",
            "2025-03-25 13:01:21,350 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-25 13:01:28,807 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3330\n",
            "2025-03-25 13:01:36,115 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3160\n",
            "2025-03-25 13:01:43,435 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3633\n",
            "2025-03-25 13:01:50,762 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2964\n",
            "2025-03-25 13:01:58,208 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3160\n",
            "2025-03-25 13:02:05,503 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3262\n",
            "2025-03-25 13:02:12,989 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3331\n",
            "2025-03-25 13:02:20,385 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3094\n",
            "2025-03-25 13:02:27,986 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-25 13:02:35,196 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3151\n",
            "2025-03-25 13:02:42,574 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3328\n",
            "2025-03-25 13:02:49,932 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3743\n",
            "2025-03-25 13:02:57,387 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3366\n",
            "2025-03-25 13:03:04,573 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3187\n",
            "2025-03-25 13:03:11,890 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3452\n",
            "2025-03-25 13:03:19,383 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3216\n",
            "2025-03-25 13:03:26,870 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3053\n",
            "2025-03-25 13:03:34,026 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3492\n",
            "2025-03-25 13:03:41,301 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3226\n",
            "2025-03-25 13:03:48,652 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3078\n",
            "2025-03-25 13:03:56,282 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3560\n",
            "2025-03-25 13:04:03,964 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3391\n",
            "2025-03-25 13:04:11,318 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3657\n",
            "2025-03-25 13:04:18,959 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3344\n",
            "2025-03-25 13:04:26,192 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3390\n",
            "2025-03-25 13:04:33,581 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3292\n",
            "2025-03-25 13:04:41,097 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3174\n",
            "2025-03-25 13:04:48,441 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3439\n",
            "2025-03-25 13:04:55,759 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3286\n",
            "2025-03-25 13:05:03,074 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3175\n",
            "2025-03-25 13:05:08,728 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2514\n",
            "2025-03-25 13:05:09,268 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0612\n",
            "2025-03-25 13:05:09,269 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:06:04,137 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3281, Val Loss: 0.4135, Val Acc: 0.8581\n",
            "2025-03-25 13:06:04,137 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 13:06:09,732 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2144\n",
            "2025-03-25 13:06:17,159 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2927\n",
            "2025-03-25 13:06:24,530 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3048\n",
            "2025-03-25 13:06:31,837 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2996\n",
            "2025-03-25 13:06:39,309 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2952\n",
            "2025-03-25 13:06:46,642 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3230\n",
            "2025-03-25 13:06:53,972 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-25 13:07:01,488 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3044\n",
            "2025-03-25 13:07:08,808 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3122\n",
            "2025-03-25 13:07:16,510 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3159\n",
            "2025-03-25 13:07:23,840 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3249\n",
            "2025-03-25 13:07:31,027 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3111\n",
            "2025-03-25 13:07:38,247 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3192\n",
            "2025-03-25 13:07:45,564 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3078\n",
            "2025-03-25 13:07:52,942 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-25 13:08:00,250 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2934\n",
            "2025-03-25 13:08:07,701 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3102\n",
            "2025-03-25 13:08:15,282 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3014\n",
            "2025-03-25 13:08:22,619 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3141\n",
            "2025-03-25 13:08:30,107 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3123\n",
            "2025-03-25 13:08:37,491 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3086\n",
            "2025-03-25 13:08:44,603 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3473\n",
            "2025-03-25 13:08:52,026 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3359\n",
            "2025-03-25 13:08:59,469 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2969\n",
            "2025-03-25 13:09:06,700 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3286\n",
            "2025-03-25 13:09:14,139 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3274\n",
            "2025-03-25 13:09:21,474 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-25 13:09:29,004 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-25 13:09:36,233 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3204\n",
            "2025-03-25 13:09:43,635 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3413\n",
            "2025-03-25 13:09:50,998 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3037\n",
            "2025-03-25 13:09:58,372 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3400\n",
            "2025-03-25 13:10:05,678 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2894\n",
            "2025-03-25 13:10:11,498 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2300\n",
            "2025-03-25 13:10:12,088 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0712\n",
            "2025-03-25 13:10:12,089 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:11:09,684 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.3101, Val Loss: 0.4089, Val Acc: 0.8744\n",
            "2025-03-25 13:11:09,685 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 13:11:15,535 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2078\n",
            "2025-03-25 13:11:22,993 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2632\n",
            "2025-03-25 13:11:30,585 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2751\n",
            "2025-03-25 13:11:38,245 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3412\n",
            "2025-03-25 13:11:45,731 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-25 13:11:53,320 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2913\n",
            "2025-03-25 13:12:01,211 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2612\n",
            "2025-03-25 13:12:08,613 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2576\n",
            "2025-03-25 13:12:16,521 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-25 13:12:24,211 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3013\n",
            "2025-03-25 13:12:31,812 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2939\n",
            "2025-03-25 13:12:39,191 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-25 13:12:46,808 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2888\n",
            "2025-03-25 13:12:54,284 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2543\n",
            "2025-03-25 13:13:01,800 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-25 13:13:09,555 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2965\n",
            "2025-03-25 13:13:17,108 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2866\n",
            "2025-03-25 13:13:24,797 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2550\n",
            "2025-03-25 13:13:32,366 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-25 13:13:40,183 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2638\n",
            "2025-03-25 13:13:47,978 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2753\n",
            "2025-03-25 13:13:55,506 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2768\n",
            "2025-03-25 13:14:03,187 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3154\n",
            "2025-03-25 13:14:10,879 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2733\n",
            "2025-03-25 13:14:18,569 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3043\n",
            "2025-03-25 13:14:26,182 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-25 13:14:33,772 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2380\n",
            "2025-03-25 13:14:41,373 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-25 13:14:49,017 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2945\n",
            "2025-03-25 13:14:56,691 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3083\n",
            "2025-03-25 13:15:04,360 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-25 13:15:11,888 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3024\n",
            "2025-03-25 13:15:19,248 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2617\n",
            "2025-03-25 13:15:25,160 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2138\n",
            "2025-03-25 13:15:25,758 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0771\n",
            "2025-03-25 13:15:25,759 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:16:21,217 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2811, Val Loss: 0.3822, Val Acc: 0.8744\n",
            "2025-03-25 13:16:21,547 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 13:16:21,548 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-25 13:16:27,709 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1957\n",
            "2025-03-25 13:16:35,833 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2773\n",
            "2025-03-25 13:16:43,594 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2984\n",
            "2025-03-25 13:16:51,344 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2569\n",
            "2025-03-25 13:16:59,039 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-25 13:17:06,722 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2757\n",
            "2025-03-25 13:17:14,525 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-25 13:17:22,416 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2492\n",
            "2025-03-25 13:17:30,290 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2800\n",
            "2025-03-25 13:17:38,673 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 13:17:46,423 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2474\n",
            "2025-03-25 13:17:54,315 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2515\n",
            "2025-03-25 13:18:01,740 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2558\n",
            "2025-03-25 13:18:09,988 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-25 13:18:17,635 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2596\n",
            "2025-03-25 13:18:26,515 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2481\n",
            "2025-03-25 13:18:35,074 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2848\n",
            "2025-03-25 13:18:43,471 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-25 13:18:51,672 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2666\n",
            "2025-03-25 13:18:59,713 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2478\n",
            "2025-03-25 13:19:07,338 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2590\n",
            "2025-03-25 13:19:15,171 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2409\n",
            "2025-03-25 13:19:23,101 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-25 13:19:31,466 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2511\n",
            "2025-03-25 13:19:39,423 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-25 13:19:47,084 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-25 13:19:55,209 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2459\n",
            "2025-03-25 13:20:03,128 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2567\n",
            "2025-03-25 13:20:13,140 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2906\n",
            "2025-03-25 13:20:24,436 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2663\n",
            "2025-03-25 13:20:33,864 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-25 13:20:42,628 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2472\n",
            "2025-03-25 13:20:51,064 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2587\n",
            "2025-03-25 13:20:57,462 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2392\n",
            "2025-03-25 13:20:58,093 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0611\n",
            "2025-03-25 13:20:58,095 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:21:59,427 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2661, Val Loss: 0.3946, Val Acc: 0.8767\n",
            "2025-03-25 13:21:59,428 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-25 13:22:05,754 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1810\n",
            "2025-03-25 13:22:13,941 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-25 13:22:22,221 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2541\n",
            "2025-03-25 13:22:29,813 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2478\n",
            "2025-03-25 13:22:37,586 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2382\n",
            "2025-03-25 13:22:45,869 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2634\n",
            "2025-03-25 13:22:54,630 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-25 13:23:02,794 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2429\n",
            "2025-03-25 13:23:11,183 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-25 13:23:19,615 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2799\n",
            "2025-03-25 13:23:27,815 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2486\n",
            "2025-03-25 13:23:36,145 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2583\n",
            "2025-03-25 13:23:44,779 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2438\n",
            "2025-03-25 13:23:53,197 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2498\n",
            "2025-03-25 13:24:01,318 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2445\n",
            "2025-03-25 13:24:08,999 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2562\n",
            "2025-03-25 13:24:16,969 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2466\n",
            "2025-03-25 13:24:25,381 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-25 13:24:33,591 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2503\n",
            "2025-03-25 13:24:41,806 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2434\n",
            "2025-03-25 13:24:50,168 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2593\n",
            "2025-03-25 13:24:58,649 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2418\n",
            "2025-03-25 13:25:07,925 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2452\n",
            "2025-03-25 13:25:16,355 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2439\n",
            "2025-03-25 13:25:24,628 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2663\n",
            "2025-03-25 13:25:33,368 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2676\n",
            "2025-03-25 13:25:41,680 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2749\n",
            "2025-03-25 13:25:50,223 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2633\n",
            "2025-03-25 13:25:58,769 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2485\n",
            "2025-03-25 13:26:06,862 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2479\n",
            "2025-03-25 13:26:14,935 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2573\n",
            "2025-03-25 13:26:23,841 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-25 13:26:31,922 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2664\n",
            "2025-03-25 13:26:38,180 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2137\n",
            "2025-03-25 13:26:38,811 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0779\n",
            "2025-03-25 13:26:38,811 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:27:40,438 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2563, Val Loss: 0.3888, Val Acc: 0.8772\n",
            "2025-03-25 13:27:40,438 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-25 13:27:46,537 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1888\n",
            "2025-03-25 13:27:54,668 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2567\n",
            "2025-03-25 13:28:02,678 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-25 13:28:10,929 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2533\n",
            "2025-03-25 13:28:18,733 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2518\n",
            "2025-03-25 13:28:26,739 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2466\n",
            "2025-03-25 13:28:34,908 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2405\n",
            "2025-03-25 13:28:43,387 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2416\n",
            "2025-03-25 13:28:52,035 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2764\n",
            "2025-03-25 13:29:00,669 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2628\n",
            "2025-03-25 13:29:08,735 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2462\n",
            "2025-03-25 13:29:17,107 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-25 13:29:25,824 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2507\n",
            "2025-03-25 13:29:33,824 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2465\n",
            "2025-03-25 13:29:42,068 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2489\n",
            "2025-03-25 13:29:50,301 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-25 13:29:58,880 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2402\n",
            "2025-03-25 13:30:07,604 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2647\n",
            "2025-03-25 13:30:15,853 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2448\n",
            "2025-03-25 13:30:24,297 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2553\n",
            "2025-03-25 13:30:32,601 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2544\n",
            "2025-03-25 13:30:40,688 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2645\n",
            "2025-03-25 13:30:48,863 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2610\n",
            "2025-03-25 13:30:57,077 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2660\n",
            "2025-03-25 13:31:05,316 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2662\n",
            "2025-03-25 13:31:13,437 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2920\n",
            "2025-03-25 13:31:21,296 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-25 13:31:29,598 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-25 13:31:37,669 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-25 13:31:45,800 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2639\n",
            "2025-03-25 13:31:53,921 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2527\n",
            "2025-03-25 13:32:02,058 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2599\n",
            "2025-03-25 13:32:10,447 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2364\n",
            "2025-03-25 13:32:16,616 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2026\n",
            "2025-03-25 13:32:17,223 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0824\n",
            "2025-03-25 13:32:17,224 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:33:17,335 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2590, Val Loss: 0.3862, Val Acc: 0.8744\n",
            "2025-03-25 13:33:17,335 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-25 13:33:23,440 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1777\n",
            "2025-03-25 13:33:31,638 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2332\n",
            "2025-03-25 13:33:39,988 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2559\n",
            "2025-03-25 13:33:48,025 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2364\n",
            "2025-03-25 13:33:56,006 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2477\n",
            "2025-03-25 13:34:04,212 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-25 13:34:12,379 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2437\n",
            "2025-03-25 13:34:20,591 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2455\n",
            "2025-03-25 13:34:28,813 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-25 13:34:37,010 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2444\n",
            "2025-03-25 13:34:45,574 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2405\n",
            "2025-03-25 13:34:53,600 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-25 13:35:01,803 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2577\n",
            "2025-03-25 13:35:09,943 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2497\n",
            "2025-03-25 13:35:18,182 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2629\n",
            "2025-03-25 13:35:26,396 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2513\n",
            "2025-03-25 13:35:34,806 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2436\n",
            "2025-03-25 13:35:43,593 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2328\n",
            "2025-03-25 13:35:52,261 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2581\n",
            "2025-03-25 13:36:01,180 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2672\n",
            "2025-03-25 13:36:09,929 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-25 13:36:18,141 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2416\n",
            "2025-03-25 13:36:26,939 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-25 13:36:35,342 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2449\n",
            "2025-03-25 13:36:43,670 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2379\n",
            "2025-03-25 13:36:52,874 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2491\n",
            "2025-03-25 13:37:01,395 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2352\n",
            "2025-03-25 13:37:09,699 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2549\n",
            "2025-03-25 13:37:17,923 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2447\n",
            "2025-03-25 13:37:26,082 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 13:37:34,167 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2609\n",
            "2025-03-25 13:37:42,324 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2590\n",
            "2025-03-25 13:37:50,749 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2538\n",
            "2025-03-25 13:37:56,911 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1764\n",
            "2025-03-25 13:37:57,698 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0577\n",
            "2025-03-25 13:37:57,699 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:39:00,400 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2501, Val Loss: 0.3952, Val Acc: 0.8706\n",
            "2025-03-25 13:39:00,401 - INFO - [TRAIN INFO] ============================== Epoch 21/50 ==============================\n",
            "2025-03-25 13:39:06,526 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1883\n",
            "2025-03-25 13:39:14,532 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2546\n",
            "2025-03-25 13:39:22,715 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2397\n",
            "2025-03-25 13:39:30,882 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2495\n",
            "2025-03-25 13:39:39,034 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2533\n",
            "2025-03-25 13:39:46,740 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2399\n",
            "2025-03-25 13:39:54,910 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2469\n",
            "2025-03-25 13:40:03,088 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-25 13:40:11,108 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2492\n",
            "2025-03-25 13:40:19,055 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2487\n",
            "2025-03-25 13:40:27,082 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2350\n",
            "2025-03-25 13:40:35,077 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2512\n",
            "2025-03-25 13:40:43,470 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2633\n",
            "2025-03-25 13:40:51,653 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2382\n",
            "2025-03-25 13:40:59,886 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2382\n",
            "2025-03-25 13:41:08,047 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2539\n",
            "2025-03-25 13:41:16,285 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2619\n",
            "2025-03-25 13:41:25,017 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2376\n",
            "2025-03-25 13:41:34,173 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2520\n",
            "2025-03-25 13:41:43,301 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2444\n",
            "2025-03-25 13:41:51,680 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2339\n",
            "2025-03-25 13:41:59,867 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2279\n",
            "2025-03-25 13:42:08,093 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2511\n",
            "2025-03-25 13:42:16,866 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2433\n",
            "2025-03-25 13:42:25,457 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2553\n",
            "2025-03-25 13:42:34,398 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2340\n",
            "2025-03-25 13:42:43,369 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2378\n",
            "2025-03-25 13:42:52,051 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2517\n",
            "2025-03-25 13:43:00,379 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2477\n",
            "2025-03-25 13:43:09,004 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2358\n",
            "2025-03-25 13:43:17,232 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2474\n",
            "2025-03-25 13:43:25,848 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2555\n",
            "2025-03-25 13:43:34,405 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2424\n",
            "2025-03-25 13:43:40,925 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1826\n",
            "2025-03-25 13:43:41,581 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0907\n",
            "2025-03-25 13:43:41,582 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:44:44,592 - INFO - [TRAIN INFO] Epoch 21/50, Train Loss: 0.2475, Val Loss: 0.3889, Val Acc: 0.8697\n",
            "2025-03-25 13:44:44,593 - INFO - [TRAIN INFO] ============================== Epoch 22/50 ==============================\n",
            "2025-03-25 13:44:50,801 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1664\n",
            "2025-03-25 13:44:58,723 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2434\n",
            "2025-03-25 13:45:06,986 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-25 13:45:14,960 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2313\n",
            "2025-03-25 13:45:23,016 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2508\n",
            "2025-03-25 13:45:30,977 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2348\n",
            "2025-03-25 13:45:39,184 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2465\n",
            "2025-03-25 13:45:47,187 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2421\n",
            "2025-03-25 13:45:55,188 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 13:46:03,370 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2364\n",
            "2025-03-25 13:46:11,491 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2387\n",
            "2025-03-25 13:46:19,591 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2429\n",
            "2025-03-25 13:46:27,559 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2443\n",
            "2025-03-25 13:46:35,782 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2450\n",
            "2025-03-25 13:46:44,497 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2537\n",
            "2025-03-25 13:46:52,771 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2566\n",
            "2025-03-25 13:47:00,971 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2417\n",
            "2025-03-25 13:47:09,176 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2326\n",
            "2025-03-25 13:47:17,399 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2463\n",
            "2025-03-25 13:47:25,765 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2278\n",
            "2025-03-25 13:47:33,731 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2551\n",
            "2025-03-25 13:47:41,760 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2291\n",
            "2025-03-25 13:47:50,545 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2241\n",
            "2025-03-25 13:47:59,423 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2485\n",
            "2025-03-25 13:48:07,798 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2421\n",
            "2025-03-25 13:48:16,340 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2305\n",
            "2025-03-25 13:48:24,546 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2494\n",
            "2025-03-25 13:48:32,788 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2347\n",
            "2025-03-25 13:48:41,108 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2370\n",
            "2025-03-25 13:48:49,326 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2673\n",
            "2025-03-25 13:48:57,527 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2376\n",
            "2025-03-25 13:49:05,941 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2466\n",
            "2025-03-25 13:49:14,148 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2388\n",
            "2025-03-25 13:49:20,290 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1886\n",
            "2025-03-25 13:49:20,899 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0700\n",
            "2025-03-25 13:49:20,903 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:50:23,323 - INFO - [TRAIN INFO] Epoch 22/50, Train Loss: 0.2421, Val Loss: 0.3954, Val Acc: 0.8702\n",
            "2025-03-25 13:50:23,323 - INFO - [TRAIN INFO] ============================== Epoch 23/50 ==============================\n",
            "2025-03-25 13:50:29,885 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1949\n",
            "2025-03-25 13:50:38,311 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2295\n",
            "2025-03-25 13:50:46,710 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2529\n",
            "2025-03-25 13:50:54,894 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2406\n",
            "2025-03-25 13:51:03,301 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2361\n",
            "2025-03-25 13:51:11,672 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2432\n",
            "2025-03-25 13:51:19,452 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2398\n",
            "2025-03-25 13:51:28,074 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2333\n",
            "2025-03-25 13:51:36,485 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2462\n",
            "2025-03-25 13:51:45,517 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-25 13:51:54,362 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2533\n",
            "2025-03-25 13:52:03,092 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2417\n",
            "2025-03-25 13:52:11,467 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2406\n",
            "2025-03-25 13:52:19,959 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2565\n",
            "2025-03-25 13:52:28,603 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2353\n",
            "2025-03-25 13:52:37,035 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2632\n",
            "2025-03-25 13:52:45,453 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2459\n",
            "2025-03-25 13:52:53,677 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2324\n",
            "2025-03-25 13:53:02,464 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2502\n",
            "2025-03-25 13:53:11,055 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2508\n",
            "2025-03-25 13:53:19,253 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2268\n",
            "2025-03-25 13:53:27,815 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2405\n",
            "2025-03-25 13:53:36,293 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2288\n",
            "2025-03-25 13:53:44,577 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2378\n",
            "2025-03-25 13:53:53,448 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2395\n",
            "2025-03-25 13:54:01,845 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2226\n",
            "2025-03-25 13:54:10,151 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2434\n",
            "2025-03-25 13:54:18,432 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2343\n",
            "2025-03-25 13:54:27,352 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2438\n",
            "2025-03-25 13:54:35,995 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2556\n",
            "2025-03-25 13:54:45,028 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-25 13:54:54,231 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2309\n",
            "2025-03-25 13:55:03,038 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2516\n",
            "2025-03-25 13:55:09,437 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1750\n",
            "2025-03-25 13:55:10,034 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0603\n",
            "2025-03-25 13:55:10,035 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 13:56:11,645 - INFO - [TRAIN INFO] Epoch 23/50, Train Loss: 0.2435, Val Loss: 0.3846, Val Acc: 0.8730\n",
            "2025-03-25 13:56:11,645 - INFO - [TRAIN INFO] ============================== Epoch 24/50 ==============================\n",
            "2025-03-25 13:56:18,088 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1797\n",
            "2025-03-25 13:56:26,892 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2434\n",
            "2025-03-25 13:56:35,738 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2285\n",
            "2025-03-25 13:56:44,615 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2404\n",
            "2025-03-25 13:56:53,181 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2347\n",
            "2025-03-25 13:57:01,192 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2265\n",
            "2025-03-25 13:57:08,996 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2486\n",
            "2025-03-25 13:57:17,146 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2425\n",
            "2025-03-25 13:57:25,367 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2424\n",
            "2025-03-25 13:57:33,383 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2547\n",
            "2025-03-25 13:57:41,382 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2422\n",
            "2025-03-25 13:57:49,525 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2518\n",
            "2025-03-25 13:57:57,742 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2378\n",
            "2025-03-25 13:58:05,982 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2437\n",
            "2025-03-25 13:58:14,068 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2324\n",
            "2025-03-25 13:58:21,948 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2527\n",
            "2025-03-25 13:58:30,138 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2658\n",
            "2025-03-25 13:58:38,170 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2475\n",
            "2025-03-25 13:58:46,155 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2544\n",
            "2025-03-25 13:58:54,146 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2447\n",
            "2025-03-25 13:59:02,100 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2530\n",
            "2025-03-25 13:59:10,080 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2508\n",
            "2025-03-25 13:59:18,302 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2520\n",
            "2025-03-25 13:59:26,261 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2357\n",
            "2025-03-25 13:59:34,537 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2427\n",
            "2025-03-25 13:59:42,919 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2510\n",
            "2025-03-25 13:59:51,353 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2667\n",
            "2025-03-25 13:59:59,741 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2290\n",
            "2025-03-25 14:00:08,816 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2468\n",
            "2025-03-25 14:00:17,106 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2562\n",
            "2025-03-25 14:00:25,334 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2261\n",
            "2025-03-25 14:00:33,581 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2481\n",
            "2025-03-25 14:00:41,689 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2315\n",
            "2025-03-25 14:00:48,068 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1812\n",
            "2025-03-25 14:00:48,677 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0720\n",
            "2025-03-25 14:00:48,678 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:01:48,746 - INFO - [TRAIN INFO] Epoch 24/50, Train Loss: 0.2447, Val Loss: 0.3856, Val Acc: 0.8748\n",
            "2025-03-25 14:01:48,747 - INFO - [TRAIN INFO] ============================== Epoch 25/50 ==============================\n",
            "2025-03-25 14:01:54,866 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1800\n",
            "2025-03-25 14:02:03,105 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2309\n",
            "2025-03-25 14:02:11,514 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2446\n",
            "2025-03-25 14:02:20,286 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-25 14:02:28,675 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2329\n",
            "2025-03-25 14:02:37,269 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2337\n",
            "2025-03-25 14:02:45,076 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2518\n",
            "2025-03-25 14:02:53,063 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2400\n",
            "2025-03-25 14:03:01,088 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2504\n",
            "2025-03-25 14:03:09,219 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2392\n",
            "2025-03-25 14:03:17,482 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2355\n",
            "2025-03-25 14:03:25,633 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2248\n",
            "2025-03-25 14:03:33,634 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2282\n",
            "2025-03-25 14:03:41,756 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2402\n",
            "2025-03-25 14:03:49,852 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2344\n",
            "2025-03-25 14:03:58,271 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2379\n",
            "2025-03-25 14:04:06,654 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2472\n",
            "2025-03-25 14:04:14,850 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2568\n",
            "2025-03-25 14:04:22,860 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2358\n",
            "2025-03-25 14:04:30,823 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2486\n",
            "2025-03-25 14:04:39,283 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2521\n",
            "2025-03-25 14:04:48,107 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2496\n",
            "2025-03-25 14:04:57,248 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2426\n",
            "2025-03-25 14:05:06,102 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2498\n",
            "2025-03-25 14:05:14,802 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2441\n",
            "2025-03-25 14:05:23,016 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2336\n",
            "2025-03-25 14:05:31,238 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2358\n",
            "2025-03-25 14:05:39,334 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2465\n",
            "2025-03-25 14:05:47,672 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2270\n",
            "2025-03-25 14:05:55,819 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2629\n",
            "2025-03-25 14:06:04,035 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2310\n",
            "2025-03-25 14:06:12,192 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2283\n",
            "2025-03-25 14:06:20,615 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2349\n",
            "2025-03-25 14:06:26,879 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1776\n",
            "2025-03-25 14:06:27,491 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0660\n",
            "2025-03-25 14:06:27,492 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:07:32,224 - INFO - [TRAIN INFO] Epoch 25/50, Train Loss: 0.2412, Val Loss: 0.3800, Val Acc: 0.8748\n",
            "2025-03-25 14:07:32,562 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-25 14:07:32,563 - INFO - [TRAIN INFO] ============================== Epoch 26/50 ==============================\n",
            "2025-03-25 14:07:39,609 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1851\n",
            "2025-03-25 14:07:48,144 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2287\n",
            "2025-03-25 14:07:56,587 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2522\n",
            "2025-03-25 14:08:05,017 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2281\n",
            "2025-03-25 14:08:13,388 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2235\n",
            "2025-03-25 14:08:21,745 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2249\n",
            "2025-03-25 14:08:30,411 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2349\n",
            "2025-03-25 14:08:38,693 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2305\n",
            "2025-03-25 14:08:47,801 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2312\n",
            "2025-03-25 14:08:56,778 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2290\n",
            "2025-03-25 14:09:05,720 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2400\n",
            "2025-03-25 14:09:14,950 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2253\n",
            "2025-03-25 14:09:23,143 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2370\n",
            "2025-03-25 14:09:31,482 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2290\n",
            "2025-03-25 14:09:40,773 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2344\n",
            "2025-03-25 14:09:49,634 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2388\n",
            "2025-03-25 14:09:57,624 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2447\n",
            "2025-03-25 14:10:05,706 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2385\n",
            "2025-03-25 14:10:14,111 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2361\n",
            "2025-03-25 14:10:22,075 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2282\n",
            "2025-03-25 14:10:31,166 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2484\n",
            "2025-03-25 14:10:40,367 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2376\n",
            "2025-03-25 14:10:49,279 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2300\n",
            "2025-03-25 14:10:57,854 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2232\n",
            "2025-03-25 14:11:06,301 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2345\n",
            "2025-03-25 14:11:14,562 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2453\n",
            "2025-03-25 14:11:23,613 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2465\n",
            "2025-03-25 14:11:32,453 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2369\n",
            "2025-03-25 14:11:41,199 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2282\n",
            "2025-03-25 14:11:49,541 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2397\n",
            "2025-03-25 14:11:58,549 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2380\n",
            "2025-03-25 14:12:07,607 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2409\n",
            "2025-03-25 14:12:15,571 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 14:12:21,396 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1752\n",
            "2025-03-25 14:12:21,973 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0525\n",
            "2025-03-25 14:12:21,974 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:13:22,589 - INFO - [TRAIN INFO] Epoch 26/50, Train Loss: 0.2351, Val Loss: 0.3808, Val Acc: 0.8772\n",
            "2025-03-25 14:13:22,590 - INFO - [TRAIN INFO] ============================== Epoch 27/50 ==============================\n",
            "2025-03-25 14:13:28,998 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1700\n",
            "2025-03-25 14:13:37,538 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2223\n",
            "2025-03-25 14:13:45,850 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2306\n",
            "2025-03-25 14:13:54,415 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2291\n",
            "2025-03-25 14:14:02,900 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2358\n",
            "2025-03-25 14:14:11,275 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2231\n",
            "2025-03-25 14:14:19,507 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2387\n",
            "2025-03-25 14:14:27,927 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2287\n",
            "2025-03-25 14:14:36,298 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2432\n",
            "2025-03-25 14:14:44,441 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2399\n",
            "2025-03-25 14:14:53,050 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2307\n",
            "2025-03-25 14:15:01,173 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2401\n",
            "2025-03-25 14:15:09,455 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2462\n",
            "2025-03-25 14:15:17,631 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2319\n",
            "2025-03-25 14:15:25,785 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2364\n",
            "2025-03-25 14:15:33,876 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2431\n",
            "2025-03-25 14:15:42,040 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2324\n",
            "2025-03-25 14:15:50,141 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2360\n",
            "2025-03-25 14:15:58,510 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2428\n",
            "2025-03-25 14:16:07,002 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2403\n",
            "2025-03-25 14:16:15,625 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2400\n",
            "2025-03-25 14:16:24,444 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2314\n",
            "2025-03-25 14:16:32,918 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2184\n",
            "2025-03-25 14:16:41,405 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2237\n",
            "2025-03-25 14:16:49,543 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2268\n",
            "2025-03-25 14:16:57,583 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2271\n",
            "2025-03-25 14:17:06,058 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2448\n",
            "2025-03-25 14:17:14,140 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2388\n",
            "2025-03-25 14:17:22,381 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2254\n",
            "2025-03-25 14:17:30,598 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2751\n",
            "2025-03-25 14:17:38,810 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2412\n",
            "2025-03-25 14:17:46,652 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2628\n",
            "2025-03-25 14:17:54,797 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2447\n",
            "2025-03-25 14:18:00,704 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1725\n",
            "2025-03-25 14:18:01,318 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1171\n",
            "2025-03-25 14:18:01,319 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:19:01,499 - INFO - [TRAIN INFO] Epoch 27/50, Train Loss: 0.2380, Val Loss: 0.3837, Val Acc: 0.8748\n",
            "2025-03-25 14:19:01,499 - INFO - [TRAIN INFO] ============================== Epoch 28/50 ==============================\n",
            "2025-03-25 14:19:07,766 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1747\n",
            "2025-03-25 14:19:15,713 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2313\n",
            "2025-03-25 14:19:23,971 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2513\n",
            "2025-03-25 14:19:32,181 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2300\n",
            "2025-03-25 14:19:40,571 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2547\n",
            "2025-03-25 14:19:48,398 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2224\n",
            "2025-03-25 14:19:56,585 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2341\n",
            "2025-03-25 14:20:05,172 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2309\n",
            "2025-03-25 14:20:13,053 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2296\n",
            "2025-03-25 14:20:20,751 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2453\n",
            "2025-03-25 14:20:28,481 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2367\n",
            "2025-03-25 14:20:36,345 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2383\n",
            "2025-03-25 14:20:44,055 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2286\n",
            "2025-03-25 14:20:51,647 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2307\n",
            "2025-03-25 14:20:59,341 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2284\n",
            "2025-03-25 14:21:07,062 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2313\n",
            "2025-03-25 14:21:14,655 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2362\n",
            "2025-03-25 14:21:22,815 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2429\n",
            "2025-03-25 14:21:31,146 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2318\n",
            "2025-03-25 14:21:39,506 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2268\n",
            "2025-03-25 14:21:47,924 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2332\n",
            "2025-03-25 14:21:56,334 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2443\n",
            "2025-03-25 14:22:04,730 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2553\n",
            "2025-03-25 14:22:13,321 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2350\n",
            "2025-03-25 14:22:21,507 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2407\n",
            "2025-03-25 14:22:30,129 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2217\n",
            "2025-03-25 14:22:38,641 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2277\n",
            "2025-03-25 14:22:47,318 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2440\n",
            "2025-03-25 14:22:55,733 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2348\n",
            "2025-03-25 14:23:04,394 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2275\n",
            "2025-03-25 14:23:13,010 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2390\n",
            "2025-03-25 14:23:21,192 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2445\n",
            "2025-03-25 14:23:29,697 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2585\n",
            "2025-03-25 14:23:35,876 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1821\n",
            "2025-03-25 14:23:36,526 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0597\n",
            "2025-03-25 14:23:36,528 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:24:38,158 - INFO - [TRAIN INFO] Epoch 28/50, Train Loss: 0.2366, Val Loss: 0.3858, Val Acc: 0.8734\n",
            "2025-03-25 14:24:38,159 - INFO - [TRAIN INFO] ============================== Epoch 29/50 ==============================\n",
            "2025-03-25 14:24:44,066 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1686\n",
            "2025-03-25 14:24:52,477 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2359\n",
            "2025-03-25 14:25:00,651 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2384\n",
            "2025-03-25 14:25:08,926 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2414\n",
            "2025-03-25 14:25:17,665 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2277\n",
            "2025-03-25 14:25:26,067 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2561\n",
            "2025-03-25 14:25:34,049 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2393\n",
            "2025-03-25 14:25:42,472 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2309\n",
            "2025-03-25 14:25:50,862 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2281\n",
            "2025-03-25 14:25:59,247 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2390\n",
            "2025-03-25 14:26:07,644 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2270\n",
            "2025-03-25 14:26:15,676 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2378\n",
            "2025-03-25 14:26:24,052 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2263\n",
            "2025-03-25 14:26:32,482 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2413\n",
            "2025-03-25 14:26:41,163 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2361\n",
            "2025-03-25 14:26:49,270 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2504\n",
            "2025-03-25 14:26:57,641 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2269\n",
            "2025-03-25 14:27:05,826 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2361\n",
            "2025-03-25 14:27:14,026 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2415\n",
            "2025-03-25 14:27:22,250 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2356\n",
            "2025-03-25 14:27:30,622 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2178\n",
            "2025-03-25 14:27:39,101 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2472\n",
            "2025-03-25 14:27:47,464 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2353\n",
            "2025-03-25 14:27:56,314 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2348\n",
            "2025-03-25 14:28:04,921 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2412\n",
            "2025-03-25 14:28:13,603 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2416\n",
            "2025-03-25 14:28:22,030 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2377\n",
            "2025-03-25 14:28:30,419 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2462\n",
            "2025-03-25 14:28:38,810 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2391\n",
            "2025-03-25 14:28:47,388 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2433\n",
            "2025-03-25 14:28:55,714 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2320\n",
            "2025-03-25 14:29:03,999 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2293\n",
            "2025-03-25 14:29:12,198 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2272\n",
            "2025-03-25 14:29:18,402 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1688\n",
            "2025-03-25 14:29:19,010 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0728\n",
            "2025-03-25 14:29:19,012 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:30:19,446 - INFO - [TRAIN INFO] Epoch 29/50, Train Loss: 0.2364, Val Loss: 0.3877, Val Acc: 0.8711\n",
            "2025-03-25 14:30:19,447 - INFO - [TRAIN INFO] ============================== Epoch 30/50 ==============================\n",
            "2025-03-25 14:30:25,619 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1954\n",
            "2025-03-25 14:30:33,975 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2291\n",
            "2025-03-25 14:30:42,371 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2670\n",
            "2025-03-25 14:30:50,771 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2341\n",
            "2025-03-25 14:30:59,357 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2427\n",
            "2025-03-25 14:31:08,071 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2379\n",
            "2025-03-25 14:31:16,537 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2313\n",
            "2025-03-25 14:31:24,959 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2430\n",
            "2025-03-25 14:31:33,352 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2225\n",
            "2025-03-25 14:31:41,561 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2294\n",
            "2025-03-25 14:31:50,074 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2249\n",
            "2025-03-25 14:31:59,288 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2356\n",
            "2025-03-25 14:32:08,319 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 14:32:16,461 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2262\n",
            "2025-03-25 14:32:25,564 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2317\n",
            "2025-03-25 14:32:34,536 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2420\n",
            "2025-03-25 14:32:43,587 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2315\n",
            "2025-03-25 14:32:52,704 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2590\n",
            "2025-03-25 14:33:01,518 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2236\n",
            "2025-03-25 14:33:09,715 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2301\n",
            "2025-03-25 14:33:17,929 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2239\n",
            "2025-03-25 14:33:26,110 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2596\n",
            "2025-03-25 14:33:34,510 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2300\n",
            "2025-03-25 14:33:43,119 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2459\n",
            "2025-03-25 14:33:51,666 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2401\n",
            "2025-03-25 14:33:59,897 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2393\n",
            "2025-03-25 14:34:07,863 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2305\n",
            "2025-03-25 14:34:16,084 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2379\n",
            "2025-03-25 14:34:24,256 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2372\n",
            "2025-03-25 14:34:32,900 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2237\n",
            "2025-03-25 14:34:41,137 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2285\n",
            "2025-03-25 14:34:49,718 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2339\n",
            "2025-03-25 14:34:58,039 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-25 14:35:04,483 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1774\n",
            "2025-03-25 14:35:05,070 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0893\n",
            "2025-03-25 14:35:05,070 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:36:07,587 - INFO - [TRAIN INFO] Epoch 30/50, Train Loss: 0.2378, Val Loss: 0.3867, Val Acc: 0.8730\n",
            "2025-03-25 14:36:07,588 - INFO - [TRAIN INFO] ============================== Epoch 31/50 ==============================\n",
            "2025-03-25 14:36:14,277 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1923\n",
            "2025-03-25 14:36:22,855 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2221\n",
            "2025-03-25 14:36:31,642 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-25 14:36:40,866 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2362\n",
            "2025-03-25 14:36:49,880 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2427\n",
            "2025-03-25 14:36:59,110 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2436\n",
            "2025-03-25 14:37:08,140 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2179\n",
            "2025-03-25 14:37:16,789 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2371\n",
            "2025-03-25 14:37:25,168 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2316\n",
            "2025-03-25 14:37:33,444 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2469\n",
            "2025-03-25 14:37:41,657 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2420\n",
            "2025-03-25 14:37:50,045 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2373\n",
            "2025-03-25 14:37:58,637 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2167\n",
            "2025-03-25 14:38:06,439 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2326\n",
            "2025-03-25 14:38:14,406 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2206\n",
            "2025-03-25 14:38:22,566 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2328\n",
            "2025-03-25 14:38:31,037 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2250\n",
            "2025-03-25 14:38:39,707 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2346\n",
            "2025-03-25 14:38:47,988 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2429\n",
            "2025-03-25 14:38:56,423 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2238\n",
            "2025-03-25 14:39:05,188 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2549\n",
            "2025-03-25 14:39:14,019 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2268\n",
            "2025-03-25 14:39:22,421 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2234\n",
            "2025-03-25 14:39:30,817 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2376\n",
            "2025-03-25 14:39:39,079 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2524\n",
            "2025-03-25 14:39:47,643 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2372\n",
            "2025-03-25 14:39:56,678 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2255\n",
            "2025-03-25 14:40:04,948 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2460\n",
            "2025-03-25 14:40:13,556 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2230\n",
            "2025-03-25 14:40:21,940 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2442\n",
            "2025-03-25 14:40:30,226 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2308\n",
            "2025-03-25 14:40:38,594 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2367\n",
            "2025-03-25 14:40:47,553 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2386\n",
            "2025-03-25 14:40:53,909 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1732\n",
            "2025-03-25 14:40:54,519 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0622\n",
            "2025-03-25 14:40:54,520 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:41:56,284 - INFO - [TRAIN INFO] Epoch 31/50, Train Loss: 0.2356, Val Loss: 0.3866, Val Acc: 0.8711\n",
            "2025-03-25 14:41:56,284 - INFO - [TRAIN INFO] ============================== Epoch 32/50 ==============================\n",
            "2025-03-25 14:42:03,061 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1818\n",
            "2025-03-25 14:42:12,157 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2286\n",
            "2025-03-25 14:42:20,876 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2235\n",
            "2025-03-25 14:42:29,099 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2471\n",
            "2025-03-25 14:42:37,326 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2354\n",
            "2025-03-25 14:42:45,348 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2226\n",
            "2025-03-25 14:42:53,516 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2476\n",
            "2025-03-25 14:43:01,890 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2225\n",
            "2025-03-25 14:43:09,946 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2328\n",
            "2025-03-25 14:43:18,127 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2283\n",
            "2025-03-25 14:43:26,342 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2376\n",
            "2025-03-25 14:43:34,550 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2425\n",
            "2025-03-25 14:43:42,766 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2389\n",
            "2025-03-25 14:43:51,041 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2216\n",
            "2025-03-25 14:43:59,312 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2455\n",
            "2025-03-25 14:44:07,429 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2353\n",
            "2025-03-25 14:44:15,484 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2370\n",
            "2025-03-25 14:44:23,727 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2406\n",
            "2025-03-25 14:44:32,119 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2253\n",
            "2025-03-25 14:44:40,288 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2313\n",
            "2025-03-25 14:44:48,337 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2284\n",
            "2025-03-25 14:44:56,411 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2241\n",
            "2025-03-25 14:45:04,692 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2418\n",
            "2025-03-25 14:45:12,892 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2282\n",
            "2025-03-25 14:45:21,041 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2178\n",
            "2025-03-25 14:45:29,489 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2245\n",
            "2025-03-25 14:45:37,901 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2409\n",
            "2025-03-25 14:45:46,298 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2324\n",
            "2025-03-25 14:45:54,690 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2307\n",
            "2025-03-25 14:46:03,295 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2342\n",
            "2025-03-25 14:46:11,846 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2377\n",
            "2025-03-25 14:46:20,682 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2414\n",
            "2025-03-25 14:46:29,285 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2500\n",
            "2025-03-25 14:46:36,004 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1733\n",
            "2025-03-25 14:46:36,631 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0560\n",
            "2025-03-25 14:46:36,631 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:47:37,349 - INFO - [TRAIN INFO] Epoch 32/50, Train Loss: 0.2337, Val Loss: 0.3870, Val Acc: 0.8730\n",
            "2025-03-25 14:47:37,349 - INFO - [TRAIN INFO] ============================== Epoch 33/50 ==============================\n",
            "2025-03-25 14:47:43,475 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1966\n",
            "2025-03-25 14:47:51,615 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2315\n",
            "2025-03-25 14:47:59,612 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2257\n",
            "2025-03-25 14:48:07,854 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2403\n",
            "2025-03-25 14:48:16,256 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2392\n",
            "2025-03-25 14:48:24,451 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2399\n",
            "2025-03-25 14:48:32,636 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2404\n",
            "2025-03-25 14:48:40,845 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2416\n",
            "2025-03-25 14:48:49,041 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2396\n",
            "2025-03-25 14:48:57,050 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2744\n",
            "2025-03-25 14:49:05,019 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2492\n",
            "2025-03-25 14:49:13,209 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2415\n",
            "2025-03-25 14:49:21,832 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2408\n",
            "2025-03-25 14:49:30,231 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2234\n",
            "2025-03-25 14:49:38,491 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2218\n",
            "2025-03-25 14:49:47,035 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2313\n",
            "2025-03-25 14:49:55,622 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2263\n",
            "2025-03-25 14:50:04,259 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2201\n",
            "2025-03-25 14:50:12,623 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2396\n",
            "2025-03-25 14:50:21,013 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2282\n",
            "2025-03-25 14:50:29,399 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2310\n",
            "2025-03-25 14:50:37,803 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2370\n",
            "2025-03-25 14:50:46,641 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2305\n",
            "2025-03-25 14:50:55,643 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2302\n",
            "2025-03-25 14:51:04,360 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2255\n",
            "2025-03-25 14:51:12,766 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2335\n",
            "2025-03-25 14:51:21,380 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2300\n",
            "2025-03-25 14:51:29,753 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2302\n",
            "2025-03-25 14:51:38,191 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2338\n",
            "2025-03-25 14:51:47,181 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 14:51:56,025 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2332\n",
            "2025-03-25 14:52:04,327 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2371\n",
            "2025-03-25 14:52:12,564 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2209\n",
            "2025-03-25 14:52:18,824 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1900\n",
            "2025-03-25 14:52:19,419 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0567\n",
            "2025-03-25 14:52:19,420 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:53:23,016 - INFO - [TRAIN INFO] Epoch 33/50, Train Loss: 0.2355, Val Loss: 0.3832, Val Acc: 0.8725\n",
            "2025-03-25 14:53:23,018 - INFO - [TRAIN INFO] ============================== Epoch 34/50 ==============================\n",
            "2025-03-25 14:53:29,978 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1696\n",
            "2025-03-25 14:53:39,177 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2322\n",
            "2025-03-25 14:53:48,369 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2265\n",
            "2025-03-25 14:53:57,369 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2353\n",
            "2025-03-25 14:54:06,372 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2446\n",
            "2025-03-25 14:54:15,184 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2350\n",
            "2025-03-25 14:54:23,556 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2238\n",
            "2025-03-25 14:54:31,917 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2330\n",
            "2025-03-25 14:54:40,123 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2430\n",
            "2025-03-25 14:54:48,529 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2375\n",
            "2025-03-25 14:54:56,716 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2376\n",
            "2025-03-25 14:55:04,688 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2266\n",
            "2025-03-25 14:55:13,113 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2261\n",
            "2025-03-25 14:55:21,268 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2315\n",
            "2025-03-25 14:55:29,519 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2320\n",
            "2025-03-25 14:55:37,671 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2249\n",
            "2025-03-25 14:55:46,103 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2360\n",
            "2025-03-25 14:55:54,508 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2324\n",
            "2025-03-25 14:56:02,907 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2253\n",
            "2025-03-25 14:56:11,305 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-25 14:56:19,705 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2372\n",
            "2025-03-25 14:56:27,899 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2390\n",
            "2025-03-25 14:56:36,052 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2434\n",
            "2025-03-25 14:56:44,368 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2371\n",
            "2025-03-25 14:56:52,697 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2295\n",
            "2025-03-25 14:57:01,189 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2377\n",
            "2025-03-25 14:57:09,871 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2372\n",
            "2025-03-25 14:57:18,085 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2318\n",
            "2025-03-25 14:57:26,218 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2394\n",
            "2025-03-25 14:57:34,509 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2365\n",
            "2025-03-25 14:57:42,635 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 14:57:50,981 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2248\n",
            "2025-03-25 14:57:59,055 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2383\n",
            "2025-03-25 14:58:04,977 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1715\n",
            "2025-03-25 14:58:05,551 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0673\n",
            "2025-03-25 14:58:05,551 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 14:59:06,000 - INFO - [TRAIN INFO] Epoch 34/50, Train Loss: 0.2345, Val Loss: 0.3864, Val Acc: 0.8744\n",
            "2025-03-25 14:59:06,000 - INFO - [TRAIN INFO] ============================== Epoch 35/50 ==============================\n",
            "2025-03-25 14:59:12,235 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1843\n",
            "2025-03-25 14:59:20,644 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2242\n",
            "2025-03-25 14:59:28,834 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2352\n",
            "2025-03-25 14:59:37,265 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2341\n",
            "2025-03-25 14:59:45,831 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2309\n",
            "2025-03-25 14:59:54,142 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2298\n",
            "2025-03-25 15:00:03,146 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2244\n",
            "2025-03-25 15:00:11,633 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2251\n",
            "2025-03-25 15:00:20,419 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2363\n",
            "2025-03-25 15:00:29,225 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2236\n",
            "2025-03-25 15:00:37,623 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2368\n",
            "2025-03-25 15:00:46,003 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2336\n",
            "2025-03-25 15:00:54,415 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2370\n",
            "2025-03-25 15:01:03,036 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2240\n",
            "2025-03-25 15:01:11,449 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2267\n",
            "2025-03-25 15:01:19,766 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2266\n",
            "2025-03-25 15:01:28,403 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2314\n",
            "2025-03-25 15:01:36,804 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2306\n",
            "2025-03-25 15:01:44,994 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2291\n",
            "2025-03-25 15:01:53,992 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2531\n",
            "2025-03-25 15:02:02,725 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2604\n",
            "2025-03-25 15:02:11,383 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2298\n",
            "2025-03-25 15:02:19,593 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2530\n",
            "2025-03-25 15:02:28,418 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2318\n",
            "2025-03-25 15:02:36,786 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2575\n",
            "2025-03-25 15:02:45,032 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2258\n",
            "2025-03-25 15:02:53,267 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2480\n",
            "2025-03-25 15:03:01,862 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2683\n",
            "2025-03-25 15:03:10,583 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2449\n",
            "2025-03-25 15:03:18,978 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2449\n",
            "2025-03-25 15:03:27,169 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2299\n",
            "2025-03-25 15:03:35,259 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2355\n",
            "2025-03-25 15:03:43,854 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2673\n",
            "2025-03-25 15:03:50,246 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1719\n",
            "2025-03-25 15:03:50,875 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0637\n",
            "2025-03-25 15:03:50,877 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:04:55,963 - INFO - [TRAIN INFO] Epoch 35/50, Train Loss: 0.2373, Val Loss: 0.3858, Val Acc: 0.8739\n",
            "2025-03-25 15:04:55,964 - INFO - [TRAIN INFO] Early stopping at epoch 35 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 15:04:55,965 - INFO - [TRAIN INFO] Total Time: 11385.61s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▂▃▁▁▁▂▃▃▄▁▂▃▃▄▅▆▆▇▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate_classifier</td><td>▂▃▄▄▅▆▇███████▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▂▃▄▄▅▆▇███████▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▂▃▄▄▅▆▇███████▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▂▃▄▄▅▆▇███████▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇▇▇▇▇▇██▇█████████████████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>35</td></tr><tr><td>learning_rate_classifier</td><td>1e-05</td></tr><tr><td>learning_rate_fusion</td><td>0.0</td></tr><tr><td>learning_rate_image</td><td>0.0</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.23732</td></tr><tr><td>train_val_loss_diff</td><td>-0.14849</td></tr><tr><td>val_accuracy</td><td>0.87389</td></tr><tr><td>val_loss</td><td>0.38581</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_only_fold_3</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wdvlzkot' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wdvlzkot</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250325_115509-wdvlzkot\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 15:04:58,270 - INFO - [TRAIN INFO] Fold 3 Training Complete at epoch 35. Total Time: 11387.91s\n",
            "2025-03-25 15:04:58,291 - INFO - [K-FOLD INFO] Fold 3 completed in 11389.26 seconds\n",
            "2025-03-25 15:04:58,293 - INFO - [K-FOLD INFO] ============================== Fold 4/5 ==============================\n",
            "2025-03-25 15:04:58,300 - INFO - [K-FOLD INFO] Fold 4:\n",
            "2025-03-25 15:04:58,300 - INFO -    Train Samples: 8595\n",
            "2025-03-25 15:04:58,301 - INFO -    Validation Samples: 2148\n",
            "2025-03-25 15:04:58,303 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 4\n",
            "2025-03-25 15:04:58,304 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 4:\n",
            "2025-03-25 15:04:58,305 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 15:04:59,220 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 4\n",
            "2025-03-25 15:04:59,224 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 4:\n",
            "2025-03-25 15:04:59,225 - INFO - [K-FOLD INFO] Loss function initialized for Fold 4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_only\\wandb\\run-20250325_150459-yheii86i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/yheii86i' target=\"_blank\">experiment_multimodal_attention_only_fold_4</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/yheii86i' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/yheii86i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 15:05:01,987 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 15:05:01,987 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-25 15:05:08,860 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.1155\n",
            "2025-03-25 15:05:18,103 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4118\n",
            "2025-03-25 15:05:26,806 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3859\n",
            "2025-03-25 15:05:35,167 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3636\n",
            "2025-03-25 15:05:43,526 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.2679\n",
            "2025-03-25 15:05:51,911 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2629\n",
            "2025-03-25 15:06:00,552 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2162\n",
            "2025-03-25 15:06:09,624 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2610\n",
            "2025-03-25 15:06:18,734 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2305\n",
            "2025-03-25 15:06:27,716 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2147\n",
            "2025-03-25 15:06:36,865 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1712\n",
            "2025-03-25 15:06:45,981 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.2160\n",
            "2025-03-25 15:06:55,078 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1951\n",
            "2025-03-25 15:07:04,200 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.0863\n",
            "2025-03-25 15:07:12,878 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1448\n",
            "2025-03-25 15:07:21,544 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1344\n",
            "2025-03-25 15:07:30,719 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1485\n",
            "2025-03-25 15:07:39,917 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0217\n",
            "2025-03-25 15:07:48,888 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0499\n",
            "2025-03-25 15:07:57,939 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.9910\n",
            "2025-03-25 15:08:06,900 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.9902\n",
            "2025-03-25 15:08:16,116 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0168\n",
            "2025-03-25 15:08:25,257 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0444\n",
            "2025-03-25 15:08:34,310 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0154\n",
            "2025-03-25 15:08:43,127 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9571\n",
            "2025-03-25 15:08:51,487 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.8881\n",
            "2025-03-25 15:08:59,904 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9244\n",
            "2025-03-25 15:09:08,074 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9568\n",
            "2025-03-25 15:09:16,525 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9618\n",
            "2025-03-25 15:09:25,079 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9523\n",
            "2025-03-25 15:09:33,593 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9837\n",
            "2025-03-25 15:09:41,856 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9126\n",
            "2025-03-25 15:09:50,028 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9812\n",
            "2025-03-25 15:09:56,401 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7077\n",
            "2025-03-25 15:09:57,186 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2335\n",
            "2025-03-25 15:09:57,187 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:10:58,681 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1086, Val Loss: 0.7703, Val Acc: 0.6983\n",
            "2025-03-25 15:10:58,959 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 15:10:58,960 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 15:11:05,375 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.7141\n",
            "2025-03-25 15:11:14,029 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9087\n",
            "2025-03-25 15:11:22,627 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8401\n",
            "2025-03-25 15:11:31,136 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8713\n",
            "2025-03-25 15:11:39,817 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8868\n",
            "2025-03-25 15:11:48,735 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8360\n",
            "2025-03-25 15:11:57,815 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8096\n",
            "2025-03-25 15:12:07,034 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8249\n",
            "2025-03-25 15:12:16,234 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8281\n",
            "2025-03-25 15:12:25,256 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7965\n",
            "2025-03-25 15:12:34,329 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.9039\n",
            "2025-03-25 15:12:43,526 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7926\n",
            "2025-03-25 15:12:51,970 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7246\n",
            "2025-03-25 15:13:00,244 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8306\n",
            "2025-03-25 15:13:08,394 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.8440\n",
            "2025-03-25 15:13:16,781 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8003\n",
            "2025-03-25 15:13:24,783 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7854\n",
            "2025-03-25 15:13:33,139 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7204\n",
            "2025-03-25 15:13:41,565 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.8533\n",
            "2025-03-25 15:13:49,779 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7562\n",
            "2025-03-25 15:13:57,998 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.8502\n",
            "2025-03-25 15:14:06,695 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7695\n",
            "2025-03-25 15:14:15,383 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7951\n",
            "2025-03-25 15:14:23,754 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7066\n",
            "2025-03-25 15:14:32,924 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7489\n",
            "2025-03-25 15:14:42,131 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7912\n",
            "2025-03-25 15:14:51,310 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7936\n",
            "2025-03-25 15:15:00,473 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7093\n",
            "2025-03-25 15:15:09,708 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7527\n",
            "2025-03-25 15:15:18,503 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7257\n",
            "2025-03-25 15:15:26,946 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.8351\n",
            "2025-03-25 15:15:34,954 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7360\n",
            "2025-03-25 15:15:43,111 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6749\n",
            "2025-03-25 15:15:49,438 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5716\n",
            "2025-03-25 15:15:50,123 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1732\n",
            "2025-03-25 15:15:50,124 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:16:57,438 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.7989, Val Loss: 0.5583, Val Acc: 0.7845\n",
            "2025-03-25 15:16:57,844 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 15:16:57,845 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 15:17:04,844 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5368\n",
            "2025-03-25 15:17:13,964 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6875\n",
            "2025-03-25 15:17:22,547 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6586\n",
            "2025-03-25 15:17:31,004 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.7364\n",
            "2025-03-25 15:17:39,319 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6542\n",
            "2025-03-25 15:17:47,682 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6760\n",
            "2025-03-25 15:17:55,997 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6366\n",
            "2025-03-25 15:18:04,221 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6431\n",
            "2025-03-25 15:18:12,748 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6427\n",
            "2025-03-25 15:18:20,860 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6650\n",
            "2025-03-25 15:18:29,286 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6630\n",
            "2025-03-25 15:18:37,514 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7438\n",
            "2025-03-25 15:18:45,690 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7185\n",
            "2025-03-25 15:18:54,470 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5888\n",
            "2025-03-25 15:19:03,901 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5998\n",
            "2025-03-25 15:19:12,878 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6192\n",
            "2025-03-25 15:19:21,310 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6597\n",
            "2025-03-25 15:19:30,025 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7569\n",
            "2025-03-25 15:19:38,549 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6025\n",
            "2025-03-25 15:19:46,889 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6794\n",
            "2025-03-25 15:19:55,257 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6331\n",
            "2025-03-25 15:20:04,083 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5937\n",
            "2025-03-25 15:20:12,909 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6201\n",
            "2025-03-25 15:20:21,679 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6124\n",
            "2025-03-25 15:20:30,383 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5688\n",
            "2025-03-25 15:20:38,210 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6694\n",
            "2025-03-25 15:20:45,989 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6045\n",
            "2025-03-25 15:20:54,262 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6250\n",
            "2025-03-25 15:21:02,639 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6433\n",
            "2025-03-25 15:21:10,859 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6076\n",
            "2025-03-25 15:21:19,045 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6652\n",
            "2025-03-25 15:21:27,316 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5829\n",
            "2025-03-25 15:21:35,827 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5994\n",
            "2025-03-25 15:21:42,034 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4758\n",
            "2025-03-25 15:21:42,639 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.3208\n",
            "2025-03-25 15:21:42,640 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:22:43,078 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6516, Val Loss: 0.4594, Val Acc: 0.8319\n",
            "2025-03-25 15:22:43,425 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 15:22:43,425 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 15:22:49,804 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4604\n",
            "2025-03-25 15:22:58,635 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5258\n",
            "2025-03-25 15:23:07,792 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5787\n",
            "2025-03-25 15:23:16,197 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5466\n",
            "2025-03-25 15:23:25,160 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5774\n",
            "2025-03-25 15:23:33,820 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5210\n",
            "2025-03-25 15:23:42,217 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5815\n",
            "2025-03-25 15:23:51,278 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5834\n",
            "2025-03-25 15:24:00,210 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5181\n",
            "2025-03-25 15:24:08,355 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5975\n",
            "2025-03-25 15:24:16,543 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5317\n",
            "2025-03-25 15:24:24,823 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5138\n",
            "2025-03-25 15:24:33,367 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5290\n",
            "2025-03-25 15:24:41,327 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5799\n",
            "2025-03-25 15:24:49,575 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5287\n",
            "2025-03-25 15:24:58,198 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5332\n",
            "2025-03-25 15:25:07,323 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6935\n",
            "2025-03-25 15:25:16,028 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6111\n",
            "2025-03-25 15:25:24,759 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6308\n",
            "2025-03-25 15:25:34,042 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4922\n",
            "2025-03-25 15:25:43,070 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5419\n",
            "2025-03-25 15:25:51,937 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5805\n",
            "2025-03-25 15:26:00,342 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5839\n",
            "2025-03-25 15:26:08,514 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5853\n",
            "2025-03-25 15:26:17,315 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6206\n",
            "2025-03-25 15:26:26,502 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5104\n",
            "2025-03-25 15:26:34,979 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6281\n",
            "2025-03-25 15:26:43,440 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5652\n",
            "2025-03-25 15:26:51,925 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5819\n",
            "2025-03-25 15:27:00,054 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5088\n",
            "2025-03-25 15:27:08,529 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5365\n",
            "2025-03-25 15:27:16,637 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5180\n",
            "2025-03-25 15:27:24,700 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5141\n",
            "2025-03-25 15:27:31,117 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4434\n",
            "2025-03-25 15:27:31,735 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1499\n",
            "2025-03-25 15:27:31,737 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:28:32,079 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5630, Val Loss: 0.4309, Val Acc: 0.8389\n",
            "2025-03-25 15:28:32,379 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 15:28:32,380 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 15:28:38,902 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3638\n",
            "2025-03-25 15:28:47,301 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5187\n",
            "2025-03-25 15:28:55,694 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5107\n",
            "2025-03-25 15:29:04,090 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4940\n",
            "2025-03-25 15:29:12,480 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5586\n",
            "2025-03-25 15:29:20,694 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5074\n",
            "2025-03-25 15:29:28,907 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5607\n",
            "2025-03-25 15:29:37,479 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4582\n",
            "2025-03-25 15:29:45,561 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4843\n",
            "2025-03-25 15:29:53,828 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5318\n",
            "2025-03-25 15:30:02,233 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6160\n",
            "2025-03-25 15:30:10,680 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5365\n",
            "2025-03-25 15:30:19,066 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5365\n",
            "2025-03-25 15:30:27,468 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5011\n",
            "2025-03-25 15:30:35,861 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5264\n",
            "2025-03-25 15:30:44,264 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5426\n",
            "2025-03-25 15:30:52,590 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5511\n",
            "2025-03-25 15:31:01,051 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5236\n",
            "2025-03-25 15:31:09,461 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5244\n",
            "2025-03-25 15:31:17,655 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5426\n",
            "2025-03-25 15:31:25,857 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5076\n",
            "2025-03-25 15:31:33,802 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4988\n",
            "2025-03-25 15:31:42,241 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5181\n",
            "2025-03-25 15:31:50,427 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5153\n",
            "2025-03-25 15:31:58,888 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5083\n",
            "2025-03-25 15:32:07,450 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5853\n",
            "2025-03-25 15:32:15,838 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5084\n",
            "2025-03-25 15:32:24,084 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4961\n",
            "2025-03-25 15:32:32,254 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5257\n",
            "2025-03-25 15:32:40,591 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5195\n",
            "2025-03-25 15:32:49,227 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4952\n",
            "2025-03-25 15:32:57,819 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5062\n",
            "2025-03-25 15:33:06,016 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5454\n",
            "2025-03-25 15:33:12,223 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3851\n",
            "2025-03-25 15:33:12,860 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1275\n",
            "2025-03-25 15:33:12,861 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:34:12,864 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5224, Val Loss: 0.3995, Val Acc: 0.8561\n",
            "2025-03-25 15:34:13,166 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 15:34:13,166 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 15:34:19,587 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3705\n",
            "2025-03-25 15:34:27,781 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5072\n",
            "2025-03-25 15:34:35,945 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4779\n",
            "2025-03-25 15:34:44,169 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4716\n",
            "2025-03-25 15:34:52,582 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4797\n",
            "2025-03-25 15:35:00,775 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4922\n",
            "2025-03-25 15:35:09,386 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4935\n",
            "2025-03-25 15:35:17,777 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4806\n",
            "2025-03-25 15:35:26,177 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4449\n",
            "2025-03-25 15:35:34,417 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4858\n",
            "2025-03-25 15:35:43,095 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4465\n",
            "2025-03-25 15:35:52,094 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4673\n",
            "2025-03-25 15:36:01,316 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4753\n",
            "2025-03-25 15:36:10,435 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4848\n",
            "2025-03-25 15:36:19,295 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5066\n",
            "2025-03-25 15:36:28,281 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5054\n",
            "2025-03-25 15:36:37,420 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4963\n",
            "2025-03-25 15:36:46,578 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5202\n",
            "2025-03-25 15:36:55,637 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5011\n",
            "2025-03-25 15:37:04,625 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4899\n",
            "2025-03-25 15:37:13,153 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4740\n",
            "2025-03-25 15:37:21,533 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4617\n",
            "2025-03-25 15:37:29,723 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4940\n",
            "2025-03-25 15:37:38,140 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4490\n",
            "2025-03-25 15:37:46,538 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4863\n",
            "2025-03-25 15:37:54,930 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4912\n",
            "2025-03-25 15:38:03,131 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4728\n",
            "2025-03-25 15:38:11,361 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5354\n",
            "2025-03-25 15:38:19,748 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5110\n",
            "2025-03-25 15:38:28,101 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5248\n",
            "2025-03-25 15:38:36,296 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4646\n",
            "2025-03-25 15:38:44,641 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5421\n",
            "2025-03-25 15:38:53,098 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4987\n",
            "2025-03-25 15:38:59,284 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3870\n",
            "2025-03-25 15:38:59,945 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1173\n",
            "2025-03-25 15:38:59,946 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:40:00,339 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4891, Val Loss: 0.3999, Val Acc: 0.8589\n",
            "2025-03-25 15:40:00,340 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 15:40:06,698 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3672\n",
            "2025-03-25 15:40:15,045 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4380\n",
            "2025-03-25 15:40:23,428 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3899\n",
            "2025-03-25 15:40:31,809 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4870\n",
            "2025-03-25 15:40:40,186 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4030\n",
            "2025-03-25 15:40:48,558 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5158\n",
            "2025-03-25 15:40:56,878 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5061\n",
            "2025-03-25 15:41:05,692 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4324\n",
            "2025-03-25 15:41:14,068 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3950\n",
            "2025-03-25 15:41:22,457 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4597\n",
            "2025-03-25 15:41:30,866 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4340\n",
            "2025-03-25 15:41:38,591 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4675\n",
            "2025-03-25 15:41:46,857 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4455\n",
            "2025-03-25 15:41:54,687 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4350\n",
            "2025-03-25 15:42:02,364 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4691\n",
            "2025-03-25 15:42:10,278 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4571\n",
            "2025-03-25 15:42:18,348 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4145\n",
            "2025-03-25 15:42:26,442 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4463\n",
            "2025-03-25 15:42:34,418 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5171\n",
            "2025-03-25 15:42:42,850 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4536\n",
            "2025-03-25 15:42:51,253 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4567\n",
            "2025-03-25 15:42:59,637 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4890\n",
            "2025-03-25 15:43:08,005 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4484\n",
            "2025-03-25 15:43:16,233 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5206\n",
            "2025-03-25 15:43:24,684 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4461\n",
            "2025-03-25 15:43:33,223 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4609\n",
            "2025-03-25 15:43:41,625 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5246\n",
            "2025-03-25 15:43:50,021 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4737\n",
            "2025-03-25 15:43:59,247 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4348\n",
            "2025-03-25 15:44:08,218 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3978\n",
            "2025-03-25 15:44:16,623 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4566\n",
            "2025-03-25 15:44:25,044 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4021\n",
            "2025-03-25 15:44:33,476 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4743\n",
            "2025-03-25 15:44:40,007 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3644\n",
            "2025-03-25 15:44:40,636 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0976\n",
            "2025-03-25 15:44:40,637 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:45:44,743 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4557, Val Loss: 0.4006, Val Acc: 0.8594\n",
            "2025-03-25 15:45:44,745 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-25 15:45:51,551 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2825\n",
            "2025-03-25 15:46:00,581 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3823\n",
            "2025-03-25 15:46:09,566 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4064\n",
            "2025-03-25 15:46:18,550 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4004\n",
            "2025-03-25 15:46:27,657 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3880\n",
            "2025-03-25 15:46:36,370 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4202\n",
            "2025-03-25 15:46:44,773 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4099\n",
            "2025-03-25 15:46:52,835 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4555\n",
            "2025-03-25 15:47:01,774 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4832\n",
            "2025-03-25 15:47:10,412 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3731\n",
            "2025-03-25 15:47:19,530 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3973\n",
            "2025-03-25 15:47:28,651 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4637\n",
            "2025-03-25 15:47:37,762 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4378\n",
            "2025-03-25 15:47:46,859 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4475\n",
            "2025-03-25 15:47:55,329 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4022\n",
            "2025-03-25 15:48:03,757 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4242\n",
            "2025-03-25 15:48:12,117 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4152\n",
            "2025-03-25 15:48:20,537 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4197\n",
            "2025-03-25 15:48:28,876 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4488\n",
            "2025-03-25 15:48:37,137 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3934\n",
            "2025-03-25 15:48:45,527 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4580\n",
            "2025-03-25 15:48:53,713 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3749\n",
            "2025-03-25 15:49:01,971 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4084\n",
            "2025-03-25 15:49:10,119 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4406\n",
            "2025-03-25 15:49:18,516 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4611\n",
            "2025-03-25 15:49:26,728 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3887\n",
            "2025-03-25 15:49:35,076 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4690\n",
            "2025-03-25 15:49:43,294 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3926\n",
            "2025-03-25 15:49:51,456 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4433\n",
            "2025-03-25 15:49:59,909 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3832\n",
            "2025-03-25 15:50:07,887 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4443\n",
            "2025-03-25 15:50:16,034 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4627\n",
            "2025-03-25 15:50:24,325 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4498\n",
            "2025-03-25 15:50:30,696 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3698\n",
            "2025-03-25 15:50:31,300 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0875\n",
            "2025-03-25 15:50:31,300 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:51:31,596 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4233, Val Loss: 0.3951, Val Acc: 0.8650\n",
            "2025-03-25 15:51:31,893 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 15:51:31,893 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-25 15:51:38,263 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-25 15:51:46,468 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4453\n",
            "2025-03-25 15:51:54,759 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4209\n",
            "2025-03-25 15:52:03,419 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3843\n",
            "2025-03-25 15:52:11,638 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3809\n",
            "2025-03-25 15:52:19,931 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3766\n",
            "2025-03-25 15:52:28,241 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3820\n",
            "2025-03-25 15:52:36,487 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3687\n",
            "2025-03-25 15:52:44,584 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4115\n",
            "2025-03-25 15:52:53,224 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3592\n",
            "2025-03-25 15:53:01,247 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3796\n",
            "2025-03-25 15:53:09,441 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3886\n",
            "2025-03-25 15:53:17,645 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3793\n",
            "2025-03-25 15:53:26,006 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4272\n",
            "2025-03-25 15:53:34,230 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3842\n",
            "2025-03-25 15:53:42,634 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3924\n",
            "2025-03-25 15:53:51,030 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3834\n",
            "2025-03-25 15:53:59,422 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3965\n",
            "2025-03-25 15:54:07,828 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4001\n",
            "2025-03-25 15:54:16,228 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3707\n",
            "2025-03-25 15:54:24,620 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4276\n",
            "2025-03-25 15:54:32,818 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4122\n",
            "2025-03-25 15:54:41,206 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4638\n",
            "2025-03-25 15:54:49,392 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3512\n",
            "2025-03-25 15:54:58,004 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4513\n",
            "2025-03-25 15:55:06,017 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3729\n",
            "2025-03-25 15:55:14,206 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4025\n",
            "2025-03-25 15:55:22,135 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3559\n",
            "2025-03-25 15:55:30,374 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4138\n",
            "2025-03-25 15:55:38,597 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4359\n",
            "2025-03-25 15:55:46,742 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4037\n",
            "2025-03-25 15:55:55,186 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3748\n",
            "2025-03-25 15:56:03,593 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4406\n",
            "2025-03-25 15:56:09,989 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3194\n",
            "2025-03-25 15:56:10,617 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0851\n",
            "2025-03-25 15:56:10,618 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 15:57:10,721 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.3980, Val Loss: 0.4033, Val Acc: 0.8659\n",
            "2025-03-25 15:57:10,721 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-25 15:57:16,727 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-25 15:57:24,958 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3645\n",
            "2025-03-25 15:57:33,356 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3781\n",
            "2025-03-25 15:57:41,488 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3472\n",
            "2025-03-25 15:57:49,488 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3989\n",
            "2025-03-25 15:57:57,727 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3888\n",
            "2025-03-25 15:58:06,014 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3637\n",
            "2025-03-25 15:58:14,182 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3674\n",
            "2025-03-25 15:58:22,547 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4004\n",
            "2025-03-25 15:58:30,641 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4068\n",
            "2025-03-25 15:58:38,576 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3529\n",
            "2025-03-25 15:58:46,757 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4140\n",
            "2025-03-25 15:58:55,131 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3646\n",
            "2025-03-25 15:59:03,527 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3322\n",
            "2025-03-25 15:59:11,876 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3802\n",
            "2025-03-25 15:59:20,334 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3860\n",
            "2025-03-25 15:59:28,536 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3806\n",
            "2025-03-25 15:59:36,705 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3547\n",
            "2025-03-25 15:59:44,733 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3780\n",
            "2025-03-25 15:59:52,956 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4040\n",
            "2025-03-25 16:00:01,153 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3830\n",
            "2025-03-25 16:00:09,320 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3483\n",
            "2025-03-25 16:00:17,556 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3417\n",
            "2025-03-25 16:00:26,102 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4099\n",
            "2025-03-25 16:00:34,302 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3698\n",
            "2025-03-25 16:00:42,422 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3741\n",
            "2025-03-25 16:00:50,682 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4028\n",
            "2025-03-25 16:00:59,102 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4062\n",
            "2025-03-25 16:01:07,125 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3564\n",
            "2025-03-25 16:01:15,240 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3688\n",
            "2025-03-25 16:01:23,401 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3595\n",
            "2025-03-25 16:01:31,495 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4067\n",
            "2025-03-25 16:01:39,775 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3657\n",
            "2025-03-25 16:01:46,078 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2960\n",
            "2025-03-25 16:01:46,685 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1340\n",
            "2025-03-25 16:01:46,685 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:02:47,005 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3780, Val Loss: 0.4085, Val Acc: 0.8641\n",
            "2025-03-25 16:02:47,006 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-25 16:02:52,991 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2618\n",
            "2025-03-25 16:03:01,072 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3257\n",
            "2025-03-25 16:03:09,256 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3089\n",
            "2025-03-25 16:03:17,417 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3226\n",
            "2025-03-25 16:03:25,886 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3160\n",
            "2025-03-25 16:03:34,449 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3369\n",
            "2025-03-25 16:03:42,626 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3484\n",
            "2025-03-25 16:03:50,739 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3071\n",
            "2025-03-25 16:03:58,948 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3688\n",
            "2025-03-25 16:04:07,119 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3303\n",
            "2025-03-25 16:04:15,379 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3604\n",
            "2025-03-25 16:04:23,752 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3586\n",
            "2025-03-25 16:04:31,886 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3677\n",
            "2025-03-25 16:04:40,141 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3546\n",
            "2025-03-25 16:04:48,443 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3724\n",
            "2025-03-25 16:04:56,825 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3625\n",
            "2025-03-25 16:05:05,203 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3743\n",
            "2025-03-25 16:05:13,428 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3981\n",
            "2025-03-25 16:05:21,452 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3568\n",
            "2025-03-25 16:05:29,741 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3707\n",
            "2025-03-25 16:05:37,963 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3514\n",
            "2025-03-25 16:05:46,388 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3575\n",
            "2025-03-25 16:05:54,570 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3657\n",
            "2025-03-25 16:06:02,807 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3828\n",
            "2025-03-25 16:06:11,149 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3890\n",
            "2025-03-25 16:06:19,410 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3654\n",
            "2025-03-25 16:06:27,680 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3811\n",
            "2025-03-25 16:06:35,942 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3982\n",
            "2025-03-25 16:06:44,207 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3646\n",
            "2025-03-25 16:06:52,593 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3725\n",
            "2025-03-25 16:07:00,941 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3778\n",
            "2025-03-25 16:07:09,158 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4033\n",
            "2025-03-25 16:07:17,393 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4339\n",
            "2025-03-25 16:07:23,583 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2528\n",
            "2025-03-25 16:07:24,208 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0707\n",
            "2025-03-25 16:07:24,208 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:08:24,396 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3606, Val Loss: 0.4039, Val Acc: 0.8561\n",
            "2025-03-25 16:08:24,396 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 16:08:30,572 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-25 16:08:38,940 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3010\n",
            "2025-03-25 16:08:47,155 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3127\n",
            "2025-03-25 16:08:55,543 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3157\n",
            "2025-03-25 16:09:03,562 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-25 16:09:11,736 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3429\n",
            "2025-03-25 16:09:20,142 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3270\n",
            "2025-03-25 16:09:28,460 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3059\n",
            "2025-03-25 16:09:36,717 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3199\n",
            "2025-03-25 16:09:44,923 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3356\n",
            "2025-03-25 16:09:52,943 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3427\n",
            "2025-03-25 16:10:01,195 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3531\n",
            "2025-03-25 16:10:09,499 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3467\n",
            "2025-03-25 16:10:17,912 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3627\n",
            "2025-03-25 16:10:26,113 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3490\n",
            "2025-03-25 16:10:34,131 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4125\n",
            "2025-03-25 16:10:42,307 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3795\n",
            "2025-03-25 16:10:50,737 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3222\n",
            "2025-03-25 16:10:59,150 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3669\n",
            "2025-03-25 16:11:07,552 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3132\n",
            "2025-03-25 16:11:15,870 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3215\n",
            "2025-03-25 16:11:24,129 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3607\n",
            "2025-03-25 16:11:32,325 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3686\n",
            "2025-03-25 16:11:40,546 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3712\n",
            "2025-03-25 16:11:48,705 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3210\n",
            "2025-03-25 16:11:56,898 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3834\n",
            "2025-03-25 16:12:05,237 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3159\n",
            "2025-03-25 16:12:13,659 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3033\n",
            "2025-03-25 16:12:22,279 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2977\n",
            "2025-03-25 16:12:30,885 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3553\n",
            "2025-03-25 16:12:39,715 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3365\n",
            "2025-03-25 16:12:48,983 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3220\n",
            "2025-03-25 16:12:58,132 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3248\n",
            "2025-03-25 16:13:05,036 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2620\n",
            "2025-03-25 16:13:05,745 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1295\n",
            "2025-03-25 16:13:05,746 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:14:07,942 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3400, Val Loss: 0.4246, Val Acc: 0.8566\n",
            "2025-03-25 16:14:07,943 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 16:14:14,439 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2378\n",
            "2025-03-25 16:14:22,864 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3240\n",
            "2025-03-25 16:14:31,138 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2998\n",
            "2025-03-25 16:14:39,657 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3430\n",
            "2025-03-25 16:14:48,254 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-25 16:14:56,839 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-25 16:15:05,049 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3308\n",
            "2025-03-25 16:15:13,641 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3126\n",
            "2025-03-25 16:15:21,839 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3121\n",
            "2025-03-25 16:15:30,244 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2776\n",
            "2025-03-25 16:15:38,781 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3188\n",
            "2025-03-25 16:15:47,265 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-25 16:15:55,672 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3292\n",
            "2025-03-25 16:16:04,508 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2953\n",
            "2025-03-25 16:16:13,413 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-25 16:16:21,917 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3264\n",
            "2025-03-25 16:16:30,982 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2928\n",
            "2025-03-25 16:16:40,165 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3131\n",
            "2025-03-25 16:16:49,239 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3191\n",
            "2025-03-25 16:16:58,198 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3031\n",
            "2025-03-25 16:17:07,443 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3125\n",
            "2025-03-25 16:17:16,765 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-25 16:17:26,029 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3379\n",
            "2025-03-25 16:17:35,042 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3108\n",
            "2025-03-25 16:17:44,302 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2969\n",
            "2025-03-25 16:17:53,496 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2987\n",
            "2025-03-25 16:18:02,757 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3072\n",
            "2025-03-25 16:18:11,996 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2950\n",
            "2025-03-25 16:18:21,186 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2980\n",
            "2025-03-25 16:18:30,220 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3283\n",
            "2025-03-25 16:18:38,579 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3244\n",
            "2025-03-25 16:18:47,007 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3005\n",
            "2025-03-25 16:18:55,832 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3021\n",
            "2025-03-25 16:19:02,802 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2310\n",
            "2025-03-25 16:19:03,504 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0764\n",
            "2025-03-25 16:19:03,505 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:20:11,086 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3096, Val Loss: 0.3944, Val Acc: 0.8655\n",
            "2025-03-25 16:20:11,086 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 16:20:17,594 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2120\n",
            "2025-03-25 16:20:26,299 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2952\n",
            "2025-03-25 16:20:35,105 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2970\n",
            "2025-03-25 16:20:43,895 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2877\n",
            "2025-03-25 16:20:52,759 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3013\n",
            "2025-03-25 16:21:01,035 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-25 16:21:09,277 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-25 16:21:17,768 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2843\n",
            "2025-03-25 16:21:26,176 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2871\n",
            "2025-03-25 16:21:34,487 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-25 16:21:42,885 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-25 16:21:51,088 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-25 16:21:59,514 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-25 16:22:07,884 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-25 16:22:16,306 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3417\n",
            "2025-03-25 16:22:24,693 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3092\n",
            "2025-03-25 16:22:33,115 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-25 16:22:41,502 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2658\n",
            "2025-03-25 16:22:49,889 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2557\n",
            "2025-03-25 16:22:58,257 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3055\n",
            "2025-03-25 16:23:06,691 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-25 16:23:15,105 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2892\n",
            "2025-03-25 16:23:23,505 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2702\n",
            "2025-03-25 16:23:31,876 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2763\n",
            "2025-03-25 16:23:40,076 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3124\n",
            "2025-03-25 16:23:48,086 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-25 16:23:56,448 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3020\n",
            "2025-03-25 16:24:04,683 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-25 16:24:13,031 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2856\n",
            "2025-03-25 16:24:21,280 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2913\n",
            "2025-03-25 16:24:29,532 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2768\n",
            "2025-03-25 16:24:37,811 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2753\n",
            "2025-03-25 16:24:46,252 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2976\n",
            "2025-03-25 16:24:52,651 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2171\n",
            "2025-03-25 16:24:53,259 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0588\n",
            "2025-03-25 16:24:53,259 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:25:53,604 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.2859, Val Loss: 0.3958, Val Acc: 0.8748\n",
            "2025-03-25 16:25:53,605 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 16:25:59,829 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1922\n",
            "2025-03-25 16:26:08,215 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2741\n",
            "2025-03-25 16:26:16,564 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2778\n",
            "2025-03-25 16:26:24,816 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2788\n",
            "2025-03-25 16:26:33,050 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-25 16:26:41,426 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2897\n",
            "2025-03-25 16:26:49,822 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3061\n",
            "2025-03-25 16:26:58,216 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-25 16:27:06,444 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2870\n",
            "2025-03-25 16:27:14,869 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-25 16:27:23,231 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-25 16:27:31,554 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3146\n",
            "2025-03-25 16:27:39,797 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2810\n",
            "2025-03-25 16:27:48,024 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-25 16:27:56,394 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2691\n",
            "2025-03-25 16:28:05,005 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2591\n",
            "2025-03-25 16:28:13,310 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2512\n",
            "2025-03-25 16:28:21,774 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3110\n",
            "2025-03-25 16:28:30,242 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-25 16:28:38,360 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-25 16:28:46,974 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2929\n",
            "2025-03-25 16:28:55,598 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-25 16:29:04,304 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2587\n",
            "2025-03-25 16:29:13,507 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-25 16:29:22,745 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2513\n",
            "2025-03-25 16:29:31,887 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2577\n",
            "2025-03-25 16:29:40,538 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3199\n",
            "2025-03-25 16:29:48,976 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2626\n",
            "2025-03-25 16:29:57,150 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-25 16:30:05,640 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-25 16:30:13,941 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2771\n",
            "2025-03-25 16:30:22,297 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2438\n",
            "2025-03-25 16:30:30,660 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2981\n",
            "2025-03-25 16:30:36,735 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1907\n",
            "2025-03-25 16:30:37,377 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0917\n",
            "2025-03-25 16:30:37,377 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:31:38,240 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2772, Val Loss: 0.4003, Val Acc: 0.8724\n",
            "2025-03-25 16:31:38,241 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 16:31:44,502 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2252\n",
            "2025-03-25 16:31:52,538 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2555\n",
            "2025-03-25 16:32:00,703 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2773\n",
            "2025-03-25 16:32:08,962 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2423\n",
            "2025-03-25 16:32:17,324 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2939\n",
            "2025-03-25 16:32:25,684 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-25 16:32:33,750 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2537\n",
            "2025-03-25 16:32:42,315 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-25 16:32:50,716 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2454\n",
            "2025-03-25 16:32:58,615 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3097\n",
            "2025-03-25 16:33:07,022 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-25 16:33:14,902 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2509\n",
            "2025-03-25 16:33:23,271 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2413\n",
            "2025-03-25 16:33:31,711 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-25 16:33:40,108 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2623\n",
            "2025-03-25 16:33:48,238 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2644\n",
            "2025-03-25 16:33:56,593 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2951\n",
            "2025-03-25 16:34:04,780 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2603\n",
            "2025-03-25 16:34:13,116 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2702\n",
            "2025-03-25 16:34:21,090 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3060\n",
            "2025-03-25 16:34:29,221 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2704\n",
            "2025-03-25 16:34:37,241 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2731\n",
            "2025-03-25 16:34:45,476 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2650\n",
            "2025-03-25 16:34:54,082 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2667\n",
            "2025-03-25 16:35:02,671 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-25 16:35:10,868 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2555\n",
            "2025-03-25 16:35:19,019 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-25 16:35:27,066 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2569\n",
            "2025-03-25 16:35:35,260 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2608\n",
            "2025-03-25 16:35:43,678 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-25 16:35:52,443 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2687\n",
            "2025-03-25 16:36:01,247 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2711\n",
            "2025-03-25 16:36:09,725 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-25 16:36:16,054 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2027\n",
            "2025-03-25 16:36:16,706 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1109\n",
            "2025-03-25 16:36:16,706 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:37:16,809 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2703, Val Loss: 0.4104, Val Acc: 0.8669\n",
            "2025-03-25 16:37:16,811 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-25 16:37:23,044 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1883\n",
            "2025-03-25 16:37:31,439 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-25 16:37:40,237 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-25 16:37:48,859 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-25 16:37:57,382 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2710\n",
            "2025-03-25 16:38:06,596 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-25 16:38:15,780 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2499\n",
            "2025-03-25 16:38:24,885 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2730\n",
            "2025-03-25 16:38:33,782 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2841\n",
            "2025-03-25 16:38:42,924 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2547\n",
            "2025-03-25 16:38:51,895 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2517\n",
            "2025-03-25 16:39:01,114 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2551\n",
            "2025-03-25 16:39:10,247 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2685\n",
            "2025-03-25 16:39:19,365 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2627\n",
            "2025-03-25 16:39:28,626 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-25 16:39:37,826 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2524\n",
            "2025-03-25 16:39:46,180 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2472\n",
            "2025-03-25 16:39:54,647 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2402\n",
            "2025-03-25 16:40:02,702 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-25 16:40:10,563 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2666\n",
            "2025-03-25 16:40:18,783 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2551\n",
            "2025-03-25 16:40:26,902 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2502\n",
            "2025-03-25 16:40:35,161 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2660\n",
            "2025-03-25 16:40:43,572 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-25 16:40:51,967 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2811\n",
            "2025-03-25 16:41:00,365 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2371\n",
            "2025-03-25 16:41:09,013 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-25 16:41:17,948 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2539\n",
            "2025-03-25 16:41:26,759 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2715\n",
            "2025-03-25 16:41:35,688 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2393\n",
            "2025-03-25 16:41:44,800 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2790\n",
            "2025-03-25 16:41:53,847 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2452\n",
            "2025-03-25 16:42:02,781 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3226\n",
            "2025-03-25 16:42:09,708 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1887\n",
            "2025-03-25 16:42:10,414 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0579\n",
            "2025-03-25 16:42:10,415 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:43:17,501 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2627, Val Loss: 0.4085, Val Acc: 0.8720\n",
            "2025-03-25 16:43:17,501 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-25 16:43:24,229 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2034\n",
            "2025-03-25 16:43:33,199 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-25 16:43:42,211 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2479\n",
            "2025-03-25 16:43:51,343 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2391\n",
            "2025-03-25 16:44:00,403 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 16:44:09,436 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2652\n",
            "2025-03-25 16:44:18,507 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-25 16:44:27,528 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2422\n",
            "2025-03-25 16:44:36,524 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2574\n",
            "2025-03-25 16:44:45,482 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2604\n",
            "2025-03-25 16:44:53,886 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2542\n",
            "2025-03-25 16:45:02,285 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2427\n",
            "2025-03-25 16:45:10,276 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2629\n",
            "2025-03-25 16:45:18,498 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-25 16:45:27,511 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2552\n",
            "2025-03-25 16:45:36,537 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-25 16:45:45,569 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-25 16:45:54,582 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2431\n",
            "2025-03-25 16:46:03,697 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-25 16:46:12,630 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2582\n",
            "2025-03-25 16:46:21,629 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2698\n",
            "2025-03-25 16:46:29,913 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-25 16:46:38,241 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-25 16:46:46,705 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2435\n",
            "2025-03-25 16:46:55,251 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2547\n",
            "2025-03-25 16:47:04,426 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-25 16:47:13,476 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2571\n",
            "2025-03-25 16:47:22,548 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2628\n",
            "2025-03-25 16:47:31,704 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2449\n",
            "2025-03-25 16:47:40,847 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-25 16:47:49,438 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2920\n",
            "2025-03-25 16:47:59,081 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2410\n",
            "2025-03-25 16:48:08,259 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2429\n",
            "2025-03-25 16:48:15,059 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1848\n",
            "2025-03-25 16:48:15,751 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0611\n",
            "2025-03-25 16:48:15,752 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:49:21,763 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2587, Val Loss: 0.4106, Val Acc: 0.8743\n",
            "2025-03-25 16:49:21,765 - INFO - [TRAIN INFO] Early stopping at epoch 18 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 16:49:21,766 - INFO - [TRAIN INFO] Total Time: 6259.78s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▂▃▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇████▃▃▃▃▃▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇████▃▃▃▃▃▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇████▃▃▃▃▃▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇████▃▃▃▃▃▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▇▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇▇███▇▇██████</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.25866</td></tr><tr><td>train_val_loss_diff</td><td>-0.1519</td></tr><tr><td>val_accuracy</td><td>0.8743</td></tr><tr><td>val_loss</td><td>0.41055</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_only_fold_4</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/yheii86i' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/yheii86i</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250325_150459-yheii86i\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 16:49:24,042 - INFO - [TRAIN INFO] Fold 4 Training Complete at epoch 18. Total Time: 6262.06s\n",
            "2025-03-25 16:49:24,058 - INFO - [K-FOLD INFO] Fold 4 completed in 6265.76 seconds\n",
            "2025-03-25 16:49:24,059 - INFO - [K-FOLD INFO] ============================== Fold 5/5 ==============================\n",
            "2025-03-25 16:49:24,067 - INFO - [K-FOLD INFO] Fold 5:\n",
            "2025-03-25 16:49:24,068 - INFO -    Train Samples: 8595\n",
            "2025-03-25 16:49:24,068 - INFO -    Validation Samples: 2148\n",
            "2025-03-25 16:49:24,069 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 5\n",
            "2025-03-25 16:49:24,070 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 5:\n",
            "2025-03-25 16:49:24,070 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 16:49:24,867 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 5\n",
            "2025-03-25 16:49:24,870 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 5:\n",
            "2025-03-25 16:49:24,870 - INFO - [K-FOLD INFO] Loss function initialized for Fold 5\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_only\\wandb\\run-20250325_164924-55o9tjej</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/55o9tjej' target=\"_blank\">experiment_multimodal_attention_only_fold_5</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/55o9tjej' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/55o9tjej</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 16:49:25,776 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 16:49:25,776 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-25 16:49:32,198 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0845\n",
            "2025-03-25 16:49:41,189 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.3896\n",
            "2025-03-25 16:49:49,808 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3877\n",
            "2025-03-25 16:49:58,758 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3464\n",
            "2025-03-25 16:50:07,802 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3208\n",
            "2025-03-25 16:50:16,390 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2944\n",
            "2025-03-25 16:50:24,791 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2311\n",
            "2025-03-25 16:50:33,463 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2880\n",
            "2025-03-25 16:50:42,463 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2714\n",
            "2025-03-25 16:50:51,321 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2180\n",
            "2025-03-25 16:51:00,368 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1977\n",
            "2025-03-25 16:51:09,180 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.2075\n",
            "2025-03-25 16:51:18,446 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1312\n",
            "2025-03-25 16:51:27,594 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1728\n",
            "2025-03-25 16:51:36,556 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.0844\n",
            "2025-03-25 16:51:45,738 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1276\n",
            "2025-03-25 16:51:54,759 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1090\n",
            "2025-03-25 16:52:03,842 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0918\n",
            "2025-03-25 16:52:12,757 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0781\n",
            "2025-03-25 16:52:22,050 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0193\n",
            "2025-03-25 16:52:31,216 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0822\n",
            "2025-03-25 16:52:40,314 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0769\n",
            "2025-03-25 16:52:49,604 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.9903\n",
            "2025-03-25 16:52:58,770 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.9775\n",
            "2025-03-25 16:53:07,964 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9360\n",
            "2025-03-25 16:53:17,227 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0165\n",
            "2025-03-25 16:53:25,707 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9395\n",
            "2025-03-25 16:53:34,098 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9139\n",
            "2025-03-25 16:53:42,542 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 1.0061\n",
            "2025-03-25 16:53:50,791 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9595\n",
            "2025-03-25 16:53:59,034 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9345\n",
            "2025-03-25 16:54:07,294 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9434\n",
            "2025-03-25 16:54:15,720 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9491\n",
            "2025-03-25 16:54:21,924 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6596\n",
            "2025-03-25 16:54:22,560 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1469\n",
            "2025-03-25 16:54:22,561 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 16:55:25,793 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1136, Val Loss: 0.7993, Val Acc: 0.6834\n",
            "2025-03-25 16:55:26,087 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 16:55:26,087 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 16:55:32,442 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6940\n",
            "2025-03-25 16:55:40,886 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.8426\n",
            "2025-03-25 16:55:49,481 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.9252\n",
            "2025-03-25 16:55:57,688 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8505\n",
            "2025-03-25 16:56:05,697 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8540\n",
            "2025-03-25 16:56:14,166 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7858\n",
            "2025-03-25 16:56:22,628 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8736\n",
            "2025-03-25 16:56:31,070 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8359\n",
            "2025-03-25 16:56:40,292 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8058\n",
            "2025-03-25 16:56:49,115 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8418\n",
            "2025-03-25 16:56:58,002 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8525\n",
            "2025-03-25 16:57:07,068 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8162\n",
            "2025-03-25 16:57:15,859 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7861\n",
            "2025-03-25 16:57:23,973 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8073\n",
            "2025-03-25 16:57:32,671 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.9188\n",
            "2025-03-25 16:57:41,749 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8370\n",
            "2025-03-25 16:57:50,651 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8274\n",
            "2025-03-25 16:57:58,852 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.8045\n",
            "2025-03-25 16:58:06,856 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7374\n",
            "2025-03-25 16:58:15,299 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8448\n",
            "2025-03-25 16:58:23,446 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.8393\n",
            "2025-03-25 16:58:31,827 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7378\n",
            "2025-03-25 16:58:39,979 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7302\n",
            "2025-03-25 16:58:47,971 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7553\n",
            "2025-03-25 16:58:56,424 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7743\n",
            "2025-03-25 16:59:04,818 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.8285\n",
            "2025-03-25 16:59:13,030 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7388\n",
            "2025-03-25 16:59:21,270 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7718\n",
            "2025-03-25 16:59:29,801 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7506\n",
            "2025-03-25 16:59:37,982 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7151\n",
            "2025-03-25 16:59:46,243 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7227\n",
            "2025-03-25 16:59:54,365 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6469\n",
            "2025-03-25 17:00:02,785 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7687\n",
            "2025-03-25 17:00:08,740 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5455\n",
            "2025-03-25 17:00:09,340 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1761\n",
            "2025-03-25 17:00:09,340 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:01:08,837 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8013, Val Loss: 0.5634, Val Acc: 0.7891\n",
            "2025-03-25 17:01:09,175 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 17:01:09,176 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 17:01:15,585 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5205\n",
            "2025-03-25 17:01:23,780 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6873\n",
            "2025-03-25 17:01:31,973 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6816\n",
            "2025-03-25 17:01:40,170 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6892\n",
            "2025-03-25 17:01:48,374 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6360\n",
            "2025-03-25 17:01:56,597 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6583\n",
            "2025-03-25 17:02:05,163 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6979\n",
            "2025-03-25 17:02:13,416 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6950\n",
            "2025-03-25 17:02:21,782 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6307\n",
            "2025-03-25 17:02:30,147 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6581\n",
            "2025-03-25 17:02:38,330 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6862\n",
            "2025-03-25 17:02:46,418 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6534\n",
            "2025-03-25 17:02:54,694 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6315\n",
            "2025-03-25 17:03:02,927 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6374\n",
            "2025-03-25 17:03:11,388 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6527\n",
            "2025-03-25 17:03:19,739 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6562\n",
            "2025-03-25 17:03:28,147 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6699\n",
            "2025-03-25 17:03:36,540 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6140\n",
            "2025-03-25 17:03:44,949 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6260\n",
            "2025-03-25 17:03:53,341 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5881\n",
            "2025-03-25 17:04:01,751 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6240\n",
            "2025-03-25 17:04:10,338 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6907\n",
            "2025-03-25 17:04:18,743 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6662\n",
            "2025-03-25 17:04:26,959 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6318\n",
            "2025-03-25 17:04:35,527 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6221\n",
            "2025-03-25 17:04:43,900 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6639\n",
            "2025-03-25 17:04:52,114 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6389\n",
            "2025-03-25 17:05:00,513 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6252\n",
            "2025-03-25 17:05:08,931 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6229\n",
            "2025-03-25 17:05:17,317 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6218\n",
            "2025-03-25 17:05:25,791 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6271\n",
            "2025-03-25 17:05:34,082 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6064\n",
            "2025-03-25 17:05:42,508 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6026\n",
            "2025-03-25 17:05:48,646 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4449\n",
            "2025-03-25 17:05:49,310 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1340\n",
            "2025-03-25 17:05:49,311 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:06:52,214 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6457, Val Loss: 0.4662, Val Acc: 0.8315\n",
            "2025-03-25 17:06:52,531 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 17:06:52,532 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 17:06:58,667 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4306\n",
            "2025-03-25 17:07:06,830 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5500\n",
            "2025-03-25 17:07:14,735 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5795\n",
            "2025-03-25 17:07:22,567 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5955\n",
            "2025-03-25 17:07:30,679 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5926\n",
            "2025-03-25 17:07:39,094 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6179\n",
            "2025-03-25 17:07:48,163 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5907\n",
            "2025-03-25 17:07:56,722 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5256\n",
            "2025-03-25 17:08:05,481 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5799\n",
            "2025-03-25 17:08:13,835 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6075\n",
            "2025-03-25 17:08:22,219 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6091\n",
            "2025-03-25 17:08:30,829 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5808\n",
            "2025-03-25 17:08:39,112 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5790\n",
            "2025-03-25 17:08:47,485 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5692\n",
            "2025-03-25 17:08:55,841 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6026\n",
            "2025-03-25 17:09:04,414 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5507\n",
            "2025-03-25 17:09:13,306 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5472\n",
            "2025-03-25 17:09:22,348 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5913\n",
            "2025-03-25 17:09:31,351 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5606\n",
            "2025-03-25 17:09:40,248 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5922\n",
            "2025-03-25 17:09:49,198 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5572\n",
            "2025-03-25 17:09:57,846 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5551\n",
            "2025-03-25 17:10:06,226 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5211\n",
            "2025-03-25 17:10:14,436 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6254\n",
            "2025-03-25 17:10:22,784 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5468\n",
            "2025-03-25 17:10:31,794 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5313\n",
            "2025-03-25 17:10:40,488 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5892\n",
            "2025-03-25 17:10:49,254 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6332\n",
            "2025-03-25 17:10:57,515 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5679\n",
            "2025-03-25 17:11:05,598 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5655\n",
            "2025-03-25 17:11:14,009 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6075\n",
            "2025-03-25 17:11:22,408 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6290\n",
            "2025-03-25 17:11:30,613 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5718\n",
            "2025-03-25 17:11:36,989 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3681\n",
            "2025-03-25 17:11:37,645 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1671\n",
            "2025-03-25 17:11:37,646 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:12:39,727 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5774, Val Loss: 0.4165, Val Acc: 0.8454\n",
            "2025-03-25 17:12:40,129 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 17:12:40,130 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 17:12:47,031 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4112\n",
            "2025-03-25 17:12:55,988 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4838\n",
            "2025-03-25 17:13:04,943 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5223\n",
            "2025-03-25 17:13:14,017 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5500\n",
            "2025-03-25 17:13:22,565 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5564\n",
            "2025-03-25 17:13:30,757 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4843\n",
            "2025-03-25 17:13:38,866 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5961\n",
            "2025-03-25 17:13:47,825 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5421\n",
            "2025-03-25 17:13:56,532 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5139\n",
            "2025-03-25 17:14:04,717 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4758\n",
            "2025-03-25 17:14:13,383 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6347\n",
            "2025-03-25 17:14:22,149 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5208\n",
            "2025-03-25 17:14:30,639 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4892\n",
            "2025-03-25 17:14:39,141 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4805\n",
            "2025-03-25 17:14:48,136 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5397\n",
            "2025-03-25 17:14:57,303 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4954\n",
            "2025-03-25 17:15:06,360 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4874\n",
            "2025-03-25 17:15:15,402 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4689\n",
            "2025-03-25 17:15:24,503 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5296\n",
            "2025-03-25 17:15:33,658 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5123\n",
            "2025-03-25 17:15:42,715 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5135\n",
            "2025-03-25 17:15:51,882 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5418\n",
            "2025-03-25 17:16:00,890 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5259\n",
            "2025-03-25 17:16:09,991 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5089\n",
            "2025-03-25 17:16:19,064 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4977\n",
            "2025-03-25 17:16:28,103 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4952\n",
            "2025-03-25 17:16:37,306 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5056\n",
            "2025-03-25 17:16:45,480 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5082\n",
            "2025-03-25 17:16:53,745 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5659\n",
            "2025-03-25 17:17:01,889 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5413\n",
            "2025-03-25 17:17:10,116 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4648\n",
            "2025-03-25 17:17:18,539 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4921\n",
            "2025-03-25 17:17:26,911 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5311\n",
            "2025-03-25 17:17:33,087 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4229\n",
            "2025-03-25 17:17:33,705 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1025\n",
            "2025-03-25 17:17:33,705 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:18:33,400 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5189, Val Loss: 0.3916, Val Acc: 0.8580\n",
            "2025-03-25 17:18:33,717 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 17:18:33,717 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 17:18:40,056 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3784\n",
            "2025-03-25 17:18:48,237 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4745\n",
            "2025-03-25 17:18:56,654 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4270\n",
            "2025-03-25 17:19:05,121 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4911\n",
            "2025-03-25 17:19:13,212 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5675\n",
            "2025-03-25 17:19:21,234 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4492\n",
            "2025-03-25 17:19:29,456 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4869\n",
            "2025-03-25 17:19:37,841 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4982\n",
            "2025-03-25 17:19:46,278 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5142\n",
            "2025-03-25 17:19:54,500 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5372\n",
            "2025-03-25 17:20:02,869 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4829\n",
            "2025-03-25 17:20:11,359 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4984\n",
            "2025-03-25 17:20:19,455 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4428\n",
            "2025-03-25 17:20:27,255 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5043\n",
            "2025-03-25 17:20:35,431 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4066\n",
            "2025-03-25 17:20:43,789 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4677\n",
            "2025-03-25 17:20:51,798 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5076\n",
            "2025-03-25 17:21:00,014 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4652\n",
            "2025-03-25 17:21:08,214 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5219\n",
            "2025-03-25 17:21:16,373 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4615\n",
            "2025-03-25 17:21:24,736 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4291\n",
            "2025-03-25 17:21:33,006 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4554\n",
            "2025-03-25 17:21:41,203 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4440\n",
            "2025-03-25 17:21:49,402 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5044\n",
            "2025-03-25 17:21:58,146 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5009\n",
            "2025-03-25 17:22:07,314 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5208\n",
            "2025-03-25 17:22:16,531 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4915\n",
            "2025-03-25 17:22:25,622 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4742\n",
            "2025-03-25 17:22:34,789 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4519\n",
            "2025-03-25 17:22:43,985 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4553\n",
            "2025-03-25 17:22:53,158 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5069\n",
            "2025-03-25 17:23:02,185 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4876\n",
            "2025-03-25 17:23:11,361 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5273\n",
            "2025-03-25 17:23:18,278 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4057\n",
            "2025-03-25 17:23:19,011 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1720\n",
            "2025-03-25 17:23:19,012 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:24:24,120 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4862, Val Loss: 0.3765, Val Acc: 0.8636\n",
            "2025-03-25 17:24:24,451 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 17:24:24,451 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 17:24:30,765 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3314\n",
            "2025-03-25 17:24:39,150 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4307\n",
            "2025-03-25 17:24:47,546 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4664\n",
            "2025-03-25 17:24:55,952 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4002\n",
            "2025-03-25 17:25:04,146 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4550\n",
            "2025-03-25 17:25:12,771 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4241\n",
            "2025-03-25 17:25:21,823 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4399\n",
            "2025-03-25 17:25:30,713 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4678\n",
            "2025-03-25 17:25:39,744 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4466\n",
            "2025-03-25 17:25:49,167 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4225\n",
            "2025-03-25 17:25:58,198 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3935\n",
            "2025-03-25 17:26:07,497 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4623\n",
            "2025-03-25 17:26:16,593 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4854\n",
            "2025-03-25 17:26:25,671 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4798\n",
            "2025-03-25 17:26:33,862 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4262\n",
            "2025-03-25 17:26:42,121 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4677\n",
            "2025-03-25 17:26:50,525 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4540\n",
            "2025-03-25 17:26:58,917 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4322\n",
            "2025-03-25 17:27:07,302 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4517\n",
            "2025-03-25 17:27:15,872 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5092\n",
            "2025-03-25 17:27:24,933 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5609\n",
            "2025-03-25 17:27:34,132 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5226\n",
            "2025-03-25 17:27:43,178 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4120\n",
            "2025-03-25 17:27:52,316 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5041\n",
            "2025-03-25 17:28:01,063 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4793\n",
            "2025-03-25 17:28:09,279 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4179\n",
            "2025-03-25 17:28:17,735 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4882\n",
            "2025-03-25 17:28:26,118 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4670\n",
            "2025-03-25 17:28:34,527 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5371\n",
            "2025-03-25 17:28:43,071 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4308\n",
            "2025-03-25 17:28:51,351 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4964\n",
            "2025-03-25 17:28:59,725 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4681\n",
            "2025-03-25 17:29:08,059 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4612\n",
            "2025-03-25 17:29:14,470 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-25 17:29:15,092 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0682\n",
            "2025-03-25 17:29:15,093 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:30:20,013 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4583, Val Loss: 0.3890, Val Acc: 0.8575\n",
            "2025-03-25 17:30:20,014 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-25 17:30:26,847 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3174\n",
            "2025-03-25 17:30:35,652 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4596\n",
            "2025-03-25 17:30:44,254 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3868\n",
            "2025-03-25 17:30:53,468 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4520\n",
            "2025-03-25 17:31:02,470 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4087\n",
            "2025-03-25 17:31:11,404 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4120\n",
            "2025-03-25 17:31:20,455 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4415\n",
            "2025-03-25 17:31:29,026 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4251\n",
            "2025-03-25 17:31:37,391 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4018\n",
            "2025-03-25 17:31:45,772 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4625\n",
            "2025-03-25 17:31:53,829 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4697\n",
            "2025-03-25 17:32:02,087 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4354\n",
            "2025-03-25 17:32:10,167 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4295\n",
            "2025-03-25 17:32:18,743 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3931\n",
            "2025-03-25 17:32:27,057 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3874\n",
            "2025-03-25 17:32:35,417 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4495\n",
            "2025-03-25 17:32:43,997 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4030\n",
            "2025-03-25 17:32:52,796 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4727\n",
            "2025-03-25 17:33:01,665 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4607\n",
            "2025-03-25 17:33:10,009 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4475\n",
            "2025-03-25 17:33:18,369 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4696\n",
            "2025-03-25 17:33:27,611 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3920\n",
            "2025-03-25 17:33:35,987 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4196\n",
            "2025-03-25 17:33:44,166 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5050\n",
            "2025-03-25 17:33:52,591 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4279\n",
            "2025-03-25 17:34:00,797 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4250\n",
            "2025-03-25 17:34:09,746 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4062\n",
            "2025-03-25 17:34:18,764 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4828\n",
            "2025-03-25 17:34:27,919 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4201\n",
            "2025-03-25 17:34:37,095 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4211\n",
            "2025-03-25 17:34:46,266 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3819\n",
            "2025-03-25 17:34:54,753 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4758\n",
            "2025-03-25 17:35:02,968 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4165\n",
            "2025-03-25 17:35:09,203 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3853\n",
            "2025-03-25 17:35:09,922 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2000\n",
            "2025-03-25 17:35:09,922 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:36:15,311 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4369, Val Loss: 0.4170, Val Acc: 0.8487\n",
            "2025-03-25 17:36:15,312 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-25 17:36:21,970 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-25 17:36:30,758 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3762\n",
            "2025-03-25 17:36:39,490 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3743\n",
            "2025-03-25 17:36:48,206 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4078\n",
            "2025-03-25 17:36:57,191 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4068\n",
            "2025-03-25 17:37:06,400 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3941\n",
            "2025-03-25 17:37:15,557 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4040\n",
            "2025-03-25 17:37:24,523 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3731\n",
            "2025-03-25 17:37:33,702 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3845\n",
            "2025-03-25 17:37:42,725 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3928\n",
            "2025-03-25 17:37:51,670 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4526\n",
            "2025-03-25 17:38:00,798 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4652\n",
            "2025-03-25 17:38:09,812 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4339\n",
            "2025-03-25 17:38:18,977 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4481\n",
            "2025-03-25 17:38:28,074 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5005\n",
            "2025-03-25 17:38:37,297 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4573\n",
            "2025-03-25 17:38:46,381 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4675\n",
            "2025-03-25 17:38:55,567 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4021\n",
            "2025-03-25 17:39:04,455 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4374\n",
            "2025-03-25 17:39:13,463 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3909\n",
            "2025-03-25 17:39:22,585 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4344\n",
            "2025-03-25 17:39:31,567 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4506\n",
            "2025-03-25 17:39:40,469 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3492\n",
            "2025-03-25 17:39:49,567 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4603\n",
            "2025-03-25 17:39:58,259 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4282\n",
            "2025-03-25 17:40:06,620 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4100\n",
            "2025-03-25 17:40:14,875 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4614\n",
            "2025-03-25 17:40:23,258 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4676\n",
            "2025-03-25 17:40:31,658 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4728\n",
            "2025-03-25 17:40:40,061 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4620\n",
            "2025-03-25 17:40:48,458 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4293\n",
            "2025-03-25 17:40:56,855 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3876\n",
            "2025-03-25 17:41:05,252 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4466\n",
            "2025-03-25 17:41:11,444 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3453\n",
            "2025-03-25 17:41:12,116 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1094\n",
            "2025-03-25 17:41:12,118 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:42:17,774 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4258, Val Loss: 0.3912, Val Acc: 0.8627\n",
            "2025-03-25 17:42:17,774 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-25 17:42:24,016 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2940\n",
            "2025-03-25 17:42:32,427 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3973\n",
            "2025-03-25 17:42:40,797 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3760\n",
            "2025-03-25 17:42:49,417 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4004\n",
            "2025-03-25 17:42:57,621 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3751\n",
            "2025-03-25 17:43:05,850 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3792\n",
            "2025-03-25 17:43:14,908 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4039\n",
            "2025-03-25 17:43:23,825 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3456\n",
            "2025-03-25 17:43:32,684 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3574\n",
            "2025-03-25 17:43:41,774 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3398\n",
            "2025-03-25 17:43:50,874 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3877\n",
            "2025-03-25 17:43:59,843 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4277\n",
            "2025-03-25 17:44:08,953 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3772\n",
            "2025-03-25 17:44:18,020 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3555\n",
            "2025-03-25 17:44:27,215 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3801\n",
            "2025-03-25 17:44:36,195 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3887\n",
            "2025-03-25 17:44:45,250 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4357\n",
            "2025-03-25 17:44:54,402 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3931\n",
            "2025-03-25 17:45:03,412 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3836\n",
            "2025-03-25 17:45:12,630 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3771\n",
            "2025-03-25 17:45:21,723 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4210\n",
            "2025-03-25 17:45:30,646 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4225\n",
            "2025-03-25 17:45:39,178 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3326\n",
            "2025-03-25 17:45:47,529 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3888\n",
            "2025-03-25 17:45:56,740 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4347\n",
            "2025-03-25 17:46:05,754 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3676\n",
            "2025-03-25 17:46:15,053 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4322\n",
            "2025-03-25 17:46:24,153 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4304\n",
            "2025-03-25 17:46:33,246 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4167\n",
            "2025-03-25 17:46:42,281 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3750\n",
            "2025-03-25 17:46:51,317 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3817\n",
            "2025-03-25 17:47:00,629 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3921\n",
            "2025-03-25 17:47:09,741 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3968\n",
            "2025-03-25 17:47:15,983 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2964\n",
            "2025-03-25 17:47:16,632 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2173\n",
            "2025-03-25 17:47:16,633 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:48:23,188 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3935, Val Loss: 0.3956, Val Acc: 0.8692\n",
            "2025-03-25 17:48:23,190 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-25 17:48:29,917 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-25 17:48:38,936 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3779\n",
            "2025-03-25 17:48:48,131 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3325\n",
            "2025-03-25 17:48:57,206 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4314\n",
            "2025-03-25 17:49:06,280 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3070\n",
            "2025-03-25 17:49:14,669 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3848\n",
            "2025-03-25 17:49:23,101 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3833\n",
            "2025-03-25 17:49:31,732 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3190\n",
            "2025-03-25 17:49:40,464 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3290\n",
            "2025-03-25 17:49:49,551 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3591\n",
            "2025-03-25 17:49:58,328 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3766\n",
            "2025-03-25 17:50:07,392 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3580\n",
            "2025-03-25 17:50:16,454 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3384\n",
            "2025-03-25 17:50:25,750 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3634\n",
            "2025-03-25 17:50:34,782 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3518\n",
            "2025-03-25 17:50:43,902 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3707\n",
            "2025-03-25 17:50:52,944 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2905\n",
            "2025-03-25 17:51:01,950 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3509\n",
            "2025-03-25 17:51:11,033 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3583\n",
            "2025-03-25 17:51:20,108 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3843\n",
            "2025-03-25 17:51:29,116 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3295\n",
            "2025-03-25 17:51:38,099 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3266\n",
            "2025-03-25 17:51:47,286 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3342\n",
            "2025-03-25 17:51:56,312 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3262\n",
            "2025-03-25 17:52:05,318 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3398\n",
            "2025-03-25 17:52:14,174 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3416\n",
            "2025-03-25 17:52:22,570 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3143\n",
            "2025-03-25 17:52:30,855 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3684\n",
            "2025-03-25 17:52:39,284 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3184\n",
            "2025-03-25 17:52:47,664 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3519\n",
            "2025-03-25 17:52:56,095 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3411\n",
            "2025-03-25 17:53:04,441 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-25 17:53:12,838 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3022\n",
            "2025-03-25 17:53:19,021 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2416\n",
            "2025-03-25 17:53:19,662 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0913\n",
            "2025-03-25 17:53:19,663 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 17:54:22,135 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3473, Val Loss: 0.3514, Val Acc: 0.8776\n",
            "2025-03-25 17:54:22,482 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 17:54:22,484 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 17:54:29,447 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2198\n",
            "2025-03-25 17:54:38,290 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-25 17:54:47,204 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3122\n",
            "2025-03-25 17:54:55,817 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3461\n",
            "2025-03-25 17:55:04,256 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3117\n",
            "2025-03-25 17:55:12,595 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-25 17:55:21,341 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-25 17:55:30,613 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-25 17:55:39,816 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3103\n",
            "2025-03-25 17:55:48,943 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3007\n",
            "2025-03-25 17:55:58,207 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3425\n",
            "2025-03-25 17:56:07,494 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2930\n",
            "2025-03-25 17:56:16,800 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2770\n",
            "2025-03-25 17:56:26,026 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3106\n",
            "2025-03-25 17:56:35,198 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2890\n",
            "2025-03-25 17:56:44,397 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2977\n",
            "2025-03-25 17:56:53,384 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3171\n",
            "2025-03-25 17:57:02,413 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3062\n",
            "2025-03-25 17:57:11,581 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-25 17:57:19,944 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2994\n",
            "2025-03-25 17:57:28,356 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2892\n",
            "2025-03-25 17:57:36,751 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2814\n",
            "2025-03-25 17:57:45,147 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3364\n",
            "2025-03-25 17:57:53,545 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2837\n",
            "2025-03-25 17:58:01,942 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3068\n",
            "2025-03-25 17:58:10,301 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3761\n",
            "2025-03-25 17:58:18,543 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3228\n",
            "2025-03-25 17:58:26,818 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3059\n",
            "2025-03-25 17:58:35,338 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3169\n",
            "2025-03-25 17:58:43,894 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3039\n",
            "2025-03-25 17:58:52,894 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3148\n",
            "2025-03-25 17:59:02,221 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3327\n",
            "2025-03-25 17:59:11,440 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3679\n",
            "2025-03-25 17:59:18,257 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2411\n",
            "2025-03-25 17:59:18,965 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0980\n",
            "2025-03-25 17:59:18,967 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:00:22,214 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3121, Val Loss: 0.3693, Val Acc: 0.8743\n",
            "2025-03-25 18:00:22,215 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 18:00:28,308 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2502\n",
            "2025-03-25 18:00:36,694 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2876\n",
            "2025-03-25 18:00:44,899 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3180\n",
            "2025-03-25 18:00:53,096 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-25 18:01:01,227 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3072\n",
            "2025-03-25 18:01:09,450 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2986\n",
            "2025-03-25 18:01:17,688 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3017\n",
            "2025-03-25 18:01:25,885 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3237\n",
            "2025-03-25 18:01:34,282 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3136\n",
            "2025-03-25 18:01:42,460 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3447\n",
            "2025-03-25 18:01:50,629 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-25 18:01:58,660 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2803\n",
            "2025-03-25 18:02:07,020 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2676\n",
            "2025-03-25 18:02:15,185 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3329\n",
            "2025-03-25 18:02:23,471 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2699\n",
            "2025-03-25 18:02:31,661 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-25 18:02:39,886 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2985\n",
            "2025-03-25 18:02:48,204 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2624\n",
            "2025-03-25 18:02:56,623 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3064\n",
            "2025-03-25 18:03:04,840 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3101\n",
            "2025-03-25 18:03:13,033 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3324\n",
            "2025-03-25 18:03:21,179 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2976\n",
            "2025-03-25 18:03:29,376 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3433\n",
            "2025-03-25 18:03:37,365 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2970\n",
            "2025-03-25 18:03:45,690 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3096\n",
            "2025-03-25 18:03:53,833 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2922\n",
            "2025-03-25 18:04:01,844 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3054\n",
            "2025-03-25 18:04:10,249 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3115\n",
            "2025-03-25 18:04:18,319 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3206\n",
            "2025-03-25 18:04:26,497 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2881\n",
            "2025-03-25 18:04:34,615 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3231\n",
            "2025-03-25 18:04:42,849 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3262\n",
            "2025-03-25 18:04:50,960 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2912\n",
            "2025-03-25 18:04:57,117 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2173\n",
            "2025-03-25 18:04:57,728 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0794\n",
            "2025-03-25 18:04:57,729 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:05:57,317 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3033, Val Loss: 0.3860, Val Acc: 0.8748\n",
            "2025-03-25 18:05:57,318 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 18:06:03,381 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2284\n",
            "2025-03-25 18:06:11,596 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2885\n",
            "2025-03-25 18:06:19,567 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3044\n",
            "2025-03-25 18:06:27,754 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2715\n",
            "2025-03-25 18:06:35,790 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3066\n",
            "2025-03-25 18:06:43,769 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3059\n",
            "2025-03-25 18:06:51,782 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3002\n",
            "2025-03-25 18:06:59,894 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3020\n",
            "2025-03-25 18:07:08,025 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-25 18:07:16,122 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3088\n",
            "2025-03-25 18:07:24,171 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2794\n",
            "2025-03-25 18:07:32,332 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2983\n",
            "2025-03-25 18:07:40,752 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-25 18:07:48,575 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3607\n",
            "2025-03-25 18:07:56,961 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-25 18:08:05,329 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2846\n",
            "2025-03-25 18:08:13,556 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2785\n",
            "2025-03-25 18:08:21,758 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2942\n",
            "2025-03-25 18:08:30,326 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2769\n",
            "2025-03-25 18:08:38,507 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-25 18:08:46,544 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-25 18:08:54,740 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2985\n",
            "2025-03-25 18:09:02,746 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2988\n",
            "2025-03-25 18:09:10,721 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-25 18:09:18,737 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2996\n",
            "2025-03-25 18:09:26,647 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2739\n",
            "2025-03-25 18:09:35,117 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2637\n",
            "2025-03-25 18:09:43,308 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2811\n",
            "2025-03-25 18:09:51,485 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-25 18:09:59,714 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3106\n",
            "2025-03-25 18:10:07,874 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3170\n",
            "2025-03-25 18:10:16,357 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2757\n",
            "2025-03-25 18:10:24,682 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2568\n",
            "2025-03-25 18:10:30,919 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2236\n",
            "2025-03-25 18:10:31,555 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1079\n",
            "2025-03-25 18:10:31,556 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:11:31,412 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.2954, Val Loss: 0.3811, Val Acc: 0.8790\n",
            "2025-03-25 18:11:31,412 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 18:11:37,686 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2404\n",
            "2025-03-25 18:11:45,827 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-25 18:11:53,963 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-25 18:12:02,072 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2753\n",
            "2025-03-25 18:12:10,452 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2671\n",
            "2025-03-25 18:12:18,581 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2795\n",
            "2025-03-25 18:12:26,669 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-25 18:12:34,906 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2911\n",
            "2025-03-25 18:12:42,992 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2956\n",
            "2025-03-25 18:12:51,176 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2863\n",
            "2025-03-25 18:12:59,245 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-25 18:13:07,461 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-25 18:13:15,368 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2810\n",
            "2025-03-25 18:13:23,461 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2653\n",
            "2025-03-25 18:13:31,389 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2543\n",
            "2025-03-25 18:13:39,843 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-25 18:13:48,223 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2933\n",
            "2025-03-25 18:13:56,335 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3013\n",
            "2025-03-25 18:14:04,619 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2997\n",
            "2025-03-25 18:14:12,844 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3072\n",
            "2025-03-25 18:14:20,821 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-25 18:14:28,914 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2926\n",
            "2025-03-25 18:14:37,015 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2992\n",
            "2025-03-25 18:14:45,039 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3118\n",
            "2025-03-25 18:14:53,203 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2937\n",
            "2025-03-25 18:15:01,436 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2926\n",
            "2025-03-25 18:15:09,697 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3152\n",
            "2025-03-25 18:15:18,304 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3137\n",
            "2025-03-25 18:15:26,665 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2744\n",
            "2025-03-25 18:15:34,790 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3109\n",
            "2025-03-25 18:15:43,380 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-25 18:15:51,655 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3075\n",
            "2025-03-25 18:15:59,775 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2984\n",
            "2025-03-25 18:16:06,089 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2369\n",
            "2025-03-25 18:16:06,694 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0845\n",
            "2025-03-25 18:16:06,694 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:17:06,584 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2916, Val Loss: 0.3726, Val Acc: 0.8790\n",
            "2025-03-25 18:17:06,585 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 18:17:12,593 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2039\n",
            "2025-03-25 18:17:20,689 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2920\n",
            "2025-03-25 18:17:28,986 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2537\n",
            "2025-03-25 18:17:37,207 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2959\n",
            "2025-03-25 18:17:45,544 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3053\n",
            "2025-03-25 18:17:53,746 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-25 18:18:01,778 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-25 18:18:09,733 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-25 18:18:17,929 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2747\n",
            "2025-03-25 18:18:26,130 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2603\n",
            "2025-03-25 18:18:34,168 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2630\n",
            "2025-03-25 18:18:42,318 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2674\n",
            "2025-03-25 18:18:50,761 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-25 18:18:59,145 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2779\n",
            "2025-03-25 18:19:07,548 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2685\n",
            "2025-03-25 18:19:15,949 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2889\n",
            "2025-03-25 18:19:23,959 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2572\n",
            "2025-03-25 18:19:32,029 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2905\n",
            "2025-03-25 18:19:40,750 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-25 18:19:48,898 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2538\n",
            "2025-03-25 18:19:57,392 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2551\n",
            "2025-03-25 18:20:05,715 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2598\n",
            "2025-03-25 18:20:14,337 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2890\n",
            "2025-03-25 18:20:22,341 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-25 18:20:30,928 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2685\n",
            "2025-03-25 18:20:39,331 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2762\n",
            "2025-03-25 18:20:47,723 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-25 18:20:56,124 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-25 18:21:04,515 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-25 18:21:12,904 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2541\n",
            "2025-03-25 18:21:21,114 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2468\n",
            "2025-03-25 18:21:29,304 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2970\n",
            "2025-03-25 18:21:37,552 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-25 18:21:43,714 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2094\n",
            "2025-03-25 18:21:44,312 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0839\n",
            "2025-03-25 18:21:44,313 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:22:44,679 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2743, Val Loss: 0.3762, Val Acc: 0.8813\n",
            "2025-03-25 18:22:44,679 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-25 18:22:50,883 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1948\n",
            "2025-03-25 18:22:58,889 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2871\n",
            "2025-03-25 18:23:07,106 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2489\n",
            "2025-03-25 18:23:15,429 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-25 18:23:23,800 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2556\n",
            "2025-03-25 18:23:32,016 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2747\n",
            "2025-03-25 18:23:40,473 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2449\n",
            "2025-03-25 18:23:48,874 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2461\n",
            "2025-03-25 18:23:57,025 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-25 18:24:05,463 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-25 18:24:13,679 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-25 18:24:22,246 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2576\n",
            "2025-03-25 18:24:30,417 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2730\n",
            "2025-03-25 18:24:38,845 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-25 18:24:47,048 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-25 18:24:55,447 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2929\n",
            "2025-03-25 18:25:03,648 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2618\n",
            "2025-03-25 18:25:12,050 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2444\n",
            "2025-03-25 18:25:20,221 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-25 18:25:28,339 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2935\n",
            "2025-03-25 18:25:36,832 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2525\n",
            "2025-03-25 18:25:45,225 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2866\n",
            "2025-03-25 18:25:53,623 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2474\n",
            "2025-03-25 18:26:01,818 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2525\n",
            "2025-03-25 18:26:09,788 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-25 18:26:17,839 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2602\n",
            "2025-03-25 18:26:26,132 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-25 18:26:34,218 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2764\n",
            "2025-03-25 18:26:42,375 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2846\n",
            "2025-03-25 18:26:50,480 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2477\n",
            "2025-03-25 18:26:58,571 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2590\n",
            "2025-03-25 18:27:06,798 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2702\n",
            "2025-03-25 18:27:15,006 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2584\n",
            "2025-03-25 18:27:21,125 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1824\n",
            "2025-03-25 18:27:21,743 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0773\n",
            "2025-03-25 18:27:21,743 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:28:21,016 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2680, Val Loss: 0.3777, Val Acc: 0.8794\n",
            "2025-03-25 18:28:21,017 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-25 18:28:27,218 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1947\n",
            "2025-03-25 18:28:35,572 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-25 18:28:43,777 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2602\n",
            "2025-03-25 18:28:51,789 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3004\n",
            "2025-03-25 18:29:00,145 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2714\n",
            "2025-03-25 18:29:08,558 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2817\n",
            "2025-03-25 18:29:16,565 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-25 18:29:24,563 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2579\n",
            "2025-03-25 18:29:32,619 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2519\n",
            "2025-03-25 18:29:40,908 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2611\n",
            "2025-03-25 18:29:49,160 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2846\n",
            "2025-03-25 18:29:57,319 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2457\n",
            "2025-03-25 18:30:05,546 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2542\n",
            "2025-03-25 18:30:13,752 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-25 18:30:21,906 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2529\n",
            "2025-03-25 18:30:29,939 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2591\n",
            "2025-03-25 18:30:38,163 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2751\n",
            "2025-03-25 18:30:46,334 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2703\n",
            "2025-03-25 18:30:54,541 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2541\n",
            "2025-03-25 18:31:02,719 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-25 18:31:10,914 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-25 18:31:19,317 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-25 18:31:27,514 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-25 18:31:35,911 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2569\n",
            "2025-03-25 18:31:44,109 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2668\n",
            "2025-03-25 18:31:52,251 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-25 18:32:00,284 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-25 18:32:08,463 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-25 18:32:16,843 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2420\n",
            "2025-03-25 18:32:24,846 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-25 18:32:33,009 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2582\n",
            "2025-03-25 18:32:41,555 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2442\n",
            "2025-03-25 18:32:49,561 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2548\n",
            "2025-03-25 18:32:55,376 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1957\n",
            "2025-03-25 18:32:55,951 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0584\n",
            "2025-03-25 18:32:55,952 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:33:57,277 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2641, Val Loss: 0.3815, Val Acc: 0.8734\n",
            "2025-03-25 18:33:57,277 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-25 18:34:03,687 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2016\n",
            "2025-03-25 18:34:12,498 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-25 18:34:21,312 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2781\n",
            "2025-03-25 18:34:30,490 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2822\n",
            "2025-03-25 18:34:39,282 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2574\n",
            "2025-03-25 18:34:48,295 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2789\n",
            "2025-03-25 18:34:57,064 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2862\n",
            "2025-03-25 18:35:06,082 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2557\n",
            "2025-03-25 18:35:15,335 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2677\n",
            "2025-03-25 18:35:24,288 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2536\n",
            "2025-03-25 18:35:33,305 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2423\n",
            "2025-03-25 18:35:42,304 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2522\n",
            "2025-03-25 18:35:51,258 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2661\n",
            "2025-03-25 18:36:00,349 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2691\n",
            "2025-03-25 18:36:09,311 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-25 18:36:18,315 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-25 18:36:27,256 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2855\n",
            "2025-03-25 18:36:36,285 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2466\n",
            "2025-03-25 18:36:45,255 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2446\n",
            "2025-03-25 18:36:54,245 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2745\n",
            "2025-03-25 18:37:03,364 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2676\n",
            "2025-03-25 18:37:12,315 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2655\n",
            "2025-03-25 18:37:21,511 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2474\n",
            "2025-03-25 18:37:30,638 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2566\n",
            "2025-03-25 18:37:39,617 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2753\n",
            "2025-03-25 18:37:48,872 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2523\n",
            "2025-03-25 18:37:57,916 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-25 18:38:06,558 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2625\n",
            "2025-03-25 18:38:14,798 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2593\n",
            "2025-03-25 18:38:23,010 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2892\n",
            "2025-03-25 18:38:31,122 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-25 18:38:39,193 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2673\n",
            "2025-03-25 18:38:47,750 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-25 18:38:54,540 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1885\n",
            "2025-03-25 18:38:55,224 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0747\n",
            "2025-03-25 18:38:55,227 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:39:58,841 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2657, Val Loss: 0.3857, Val Acc: 0.8766\n",
            "2025-03-25 18:39:58,842 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-25 18:40:05,618 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1836\n",
            "2025-03-25 18:40:14,442 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2454\n",
            "2025-03-25 18:40:23,274 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2528\n",
            "2025-03-25 18:40:32,144 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2583\n",
            "2025-03-25 18:40:40,511 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2413\n",
            "2025-03-25 18:40:49,047 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2623\n",
            "2025-03-25 18:40:57,000 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2395\n",
            "2025-03-25 18:41:04,896 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2538\n",
            "2025-03-25 18:41:12,944 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2633\n",
            "2025-03-25 18:41:21,022 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2500\n",
            "2025-03-25 18:41:29,582 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2583\n",
            "2025-03-25 18:41:37,719 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2548\n",
            "2025-03-25 18:41:45,704 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-25 18:41:53,732 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2559\n",
            "2025-03-25 18:42:01,502 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2494\n",
            "2025-03-25 18:42:09,505 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2608\n",
            "2025-03-25 18:42:17,463 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2546\n",
            "2025-03-25 18:42:25,450 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2555\n",
            "2025-03-25 18:42:33,448 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2526\n",
            "2025-03-25 18:42:41,690 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2422\n",
            "2025-03-25 18:42:49,475 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2444\n",
            "2025-03-25 18:42:57,471 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2514\n",
            "2025-03-25 18:43:05,709 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2504\n",
            "2025-03-25 18:43:13,284 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-25 18:43:21,309 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-25 18:43:29,921 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-25 18:43:37,901 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2588\n",
            "2025-03-25 18:43:45,894 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-25 18:43:53,701 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-25 18:44:01,564 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-25 18:44:09,888 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2807\n",
            "2025-03-25 18:44:18,065 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2697\n",
            "2025-03-25 18:44:26,267 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2494\n",
            "2025-03-25 18:44:32,607 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2054\n",
            "2025-03-25 18:44:33,250 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0631\n",
            "2025-03-25 18:44:33,251 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:45:33,194 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2581, Val Loss: 0.3808, Val Acc: 0.8771\n",
            "2025-03-25 18:45:33,195 - INFO - [TRAIN INFO] ============================== Epoch 21/50 ==============================\n",
            "2025-03-25 18:45:39,680 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1831\n",
            "2025-03-25 18:45:48,097 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2637\n",
            "2025-03-25 18:45:56,406 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-25 18:46:04,666 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2747\n",
            "2025-03-25 18:46:13,052 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-25 18:46:21,257 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2770\n",
            "2025-03-25 18:46:29,461 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2604\n",
            "2025-03-25 18:46:37,828 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2644\n",
            "2025-03-25 18:46:46,249 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2528\n",
            "2025-03-25 18:46:54,687 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2445\n",
            "2025-03-25 18:47:03,209 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2714\n",
            "2025-03-25 18:47:11,423 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2735\n",
            "2025-03-25 18:47:19,526 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-25 18:47:27,658 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2569\n",
            "2025-03-25 18:47:36,223 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-25 18:47:44,422 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2625\n",
            "2025-03-25 18:47:52,594 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2457\n",
            "2025-03-25 18:48:01,008 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2599\n",
            "2025-03-25 18:48:09,380 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2464\n",
            "2025-03-25 18:48:17,805 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-25 18:48:26,200 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2580\n",
            "2025-03-25 18:48:34,604 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2705\n",
            "2025-03-25 18:48:42,995 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2427\n",
            "2025-03-25 18:48:51,396 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2596\n",
            "2025-03-25 18:48:59,799 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2685\n",
            "2025-03-25 18:49:08,004 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2508\n",
            "2025-03-25 18:49:16,389 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2808\n",
            "2025-03-25 18:49:24,587 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2666\n",
            "2025-03-25 18:49:33,084 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2526\n",
            "2025-03-25 18:49:41,562 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2568\n",
            "2025-03-25 18:49:49,775 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-25 18:49:57,930 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2517\n",
            "2025-03-25 18:50:06,368 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2618\n",
            "2025-03-25 18:50:12,561 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1806\n",
            "2025-03-25 18:50:13,206 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0843\n",
            "2025-03-25 18:50:13,207 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 18:51:14,113 - INFO - [TRAIN INFO] Epoch 21/50, Train Loss: 0.2631, Val Loss: 0.3839, Val Acc: 0.8762\n",
            "2025-03-25 18:51:14,113 - INFO - [TRAIN INFO] Early stopping at epoch 21 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 18:51:14,114 - INFO - [TRAIN INFO] Total Time: 7308.34s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▂▃▃▄▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▂▃▄▄▅▆▇██▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▂▃▄▄▅▆▇██▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▂▃▄▄▅▆▇██▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▂▃▄▄▅▆▇██▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▇▆▆▅▅▄▃▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇▇▇▇████████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▂▂▂▂▁▁▂▁▁▁▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>21</td></tr><tr><td>learning_rate_classifier</td><td>0.00014</td></tr><tr><td>learning_rate_fusion</td><td>3e-05</td></tr><tr><td>learning_rate_image</td><td>3e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.26315</td></tr><tr><td>train_val_loss_diff</td><td>-0.12076</td></tr><tr><td>val_accuracy</td><td>0.87616</td></tr><tr><td>val_loss</td><td>0.38391</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_only_fold_5</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/55o9tjej' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/55o9tjej</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250325_164924-55o9tjej\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 18:51:16,475 - INFO - [TRAIN INFO] Fold 5 Training Complete at epoch 21. Total Time: 7310.70s\n",
            "2025-03-25 18:51:16,490 - INFO - [K-FOLD INFO] Fold 5 completed in 7312.43 seconds\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "logging.info(\"[K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "\n",
        "    \n",
        "    fold_start_time = time.time()  # Start timing for this fold\n",
        "    logging.info(f\"[K-FOLD INFO] ============================== Fold {fold+1}/{K_FOLDS} ==============================\")\n",
        "\n",
        "    # Get train and validation subsets\n",
        "    train_texts_fold = train_texts[train_idx]\n",
        "    val_texts_fold = train_texts[val_idx]\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "    train_image_paths_fold = train_image_paths[train_idx]\n",
        "    val_image_paths_fold = train_image_paths[val_idx]\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train Samples: {len(train_texts_fold)}\")\n",
        "    logging.info(f\"   Validation Samples: {len(val_texts_fold)}\")\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_image_dataset = ImageDataset(train_image_paths_fold, train_labels_fold, transform[\"train\"])\n",
        "    val_image_dataset = ImageDataset(val_image_paths_fold, val_labels_fold, transform[\"val\"])\n",
        "    \n",
        "    train_text_dataset = CustomTextDataset(train_texts_fold, train_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "    val_text_dataset = CustomTextDataset(val_texts_fold, val_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # Create multimodal datasets\n",
        "    train_multimodal_dataset = MultimodalDataset(train_image_dataset, train_text_dataset)\n",
        "    val_multimodal_dataset = MultimodalDataset(val_image_dataset, val_text_dataset)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Created multimodal datasets for Fold {fold+1}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    dataloaders = {\n",
        "        \"train_loader\": DataLoader(train_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=True),\n",
        "        \"val_loader\": DataLoader(val_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    }\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] DataLoaders initialized for Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train batches: {len(dataloaders['train_loader'])}, Validation batches: {len(dataloaders['val_loader'])}\")\n",
        "\n",
        "    # Initialize model, optimizer, and criterion\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Model initialized on {device} for Fold {fold+1}\")\n",
        "\n",
        "    # Define Optimizer using AdamW\n",
        "    optimizer = optim.AdamW([\n",
        "        {\"params\": model.image_model.features[-3:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_IMAGE, \"weight_decay\": WEIGHT_DECAY_IMAGE},  # Unfrozen EfficientNet layer\n",
        "        {\"params\": model.text_model.transformer.layer[-2:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_TEXT, \"weight_decay\": WEIGHT_DECAY_TEXT},  # Unfrozen DistilBERT layer\n",
        "        {\"params\": model.image_fc.parameters(), \"lr\": LEARNING_RATE_IMAGE, \"weight_decay\": 0}, \n",
        "        {\"params\": model.text_fc.parameters(), \"lr\": LEARNING_RATE_TEXT, \"weight_decay\": 0},\n",
        "        {\"params\": model.fusion_fc.parameters(), \"lr\": LEARNING_RATE_FUSION, \"weight_decay\": WEIGHT_DECAY_FUSION},  \n",
        "        {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER, \"weight_decay\": WEIGHT_DECAY_CLASSIFIER}  \n",
        "    ], betas=(0.9, 0.999), eps=1e-8)  # Default AdamW betas and eps\n",
        "\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Optimizer initialized for Fold {fold+1}:\")\n",
        "    # Define Loss Function\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING_PREDICTION) \n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Loss function initialized for Fold {fold+1}\")\n",
        "\n",
        "    # Train model for this fold\n",
        "    train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True)\n",
        "\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Measure Fold Time\n",
        "    fold_time = time.time() - fold_start_time\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1} completed in {fold_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for fold in range(K_FOLDS):\n",
        "#     logging.info(f\"\\n[TEST INFO] Evaluating Fold {fold + 1} on Test Set...\")\n",
        "\n",
        "#     # Load best model for the fold\n",
        "#     model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "#     model_path = f\"best_model_fold_{fold + 1}.pth\"\n",
        "    \n",
        "#     try:\n",
        "#         model.load_state_dict(torch.load(model_path))\n",
        "#         logging.info(f\"[TEST INFO] Loaded best model for Fold {fold + 1} from {model_path}\")\n",
        "#     except FileNotFoundError:\n",
        "#         logging.error(f\"[ERROR] Model file {model_path} not found! Skipping Fold {fold + 1} evaluation.\")\n",
        "#         continue  # Skip to the next fold if model file is missing\n",
        "\n",
        "#     model.eval()  # Set to evaluation mode\n",
        "\n",
        "#     # Evaluate model on test data\n",
        "#     test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "\n",
        "#     # Log test set performance for the fold\n",
        "#     logging.info(f\"[TEST INFO] Fold {fold + 1} Test Performance:\")\n",
        "#     logging.info(f\"   Test Loss: {test_loss:.4f}\")\n",
        "#     logging.info(f\"   Test Accuracy: {test_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "enel645_torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
