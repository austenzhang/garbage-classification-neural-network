{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLOOi0GQM5B"
      },
      "source": [
        "## Garbage Classification Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
        "from torchvision.models.efficientnet import EfficientNet_B0_Weights\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import wandb\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import time\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "NOTES = '''\n",
        "'''\n",
        "\n",
        "# ========================================= GLOBAL CONFIGURATION ================================================\n",
        "# Data Directories\n",
        "DATA_DIR = r\"C:\\NN Data\\garbage_data\\kfold_garbage_data\"\n",
        "CLASSES = [\"Black\", \"Blue\", \"Green\", \"TTR\"]\n",
        "\n",
        "# ========================================= Experiment Settings =========================================\n",
        "WANDB_RUN_NAME = \"experiment_multimodal_attention_gated_fusion\"\n",
        "MODEL_NAME = \"experiment_multimodal_attention_gated_fusion\"\n",
        "\n",
        "# ========================================= Data Settings =========================================\n",
        "IMAGE_SIZE = (224, 224)  # Input image size for EfficientNetV2-S\n",
        "NUM_CLASSES = 4  # Number of output classes for classification\n",
        "MAX_LEN = 40  # Maximum token length for DistilBERT tokenizer\n",
        "TEST_SIZE = 0.2  # Test dataset size split\n",
        "K_FOLDS = 5  # Number of folds for stratified k-fold cross-validation\n",
        "\n",
        "# ========================================= Training Hyperparameters =========================================\n",
        "BATCH_SIZE = 64  # Number of samples per batch\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "EPOCHS = 50  # Maximum number of training epochs\n",
        "DROPOUT_IMAGE = 0.2 # Reduce from 0.3\n",
        "DROPOUT_TEXT = 0.1 # Reduce from 0.2\n",
        "DROPOUT_FUSION = 0.2 \n",
        "DROPOUT_CLASSIFIER = 0.1\n",
        "PATIENCE = 10  # Number of epochs to wait before early stopping\n",
        "CONVERGENCE_THRESHOLD = 0.001  # Minimum improvement in validation loss to continue training\n",
        "\n",
        "# ========================================= Optimization Settings =========================================\n",
        "OPTIMIZER = \"AdamW\"\n",
        "LR_SCHEDULING_FACTOR = 0.3\n",
        "LEARNING_RATE_UNFREEZE_IMAGE = 1e-5\n",
        "LEARNING_RATE_UNFREEZE_TEXT = 1e-5\n",
        "LEARNING_RATE_FUSION = 1e-3\n",
        "LEARNING_RATE_CLASSIFIER = 5e-3\n",
        "LEARNING_RATE_IMAGE = 0.001 # # EfficientNetB0\n",
        "LEARNING_RATE_TEXT = 0.00002 # DistilBERT Uncased\n",
        "WEIGHT_DECAY_TEXT = 1e-3  # Reduce from 1e-2\n",
        "WEIGHT_DECAY_IMAGE = 1e-4  # Reduce from 1e-3\n",
        "WEIGHT_DECAY_FUSION = 4e-4 \n",
        "WEIGHT_DECAY_CLASSIFIER = 1e-3  # Reduce from 1e-4\n",
        "LABEL_SMOOTHING_PREDICTION = 0.05 # Reduce from 0.1\n",
        "\n",
        "# ========================================= System Settings =========================================\n",
        "NUM_WORKERS = 4  # Dataloader parallelization\n",
        "\n",
        "# Wandb Configuration\n",
        "WANDB_CONFIG = {\n",
        "    \"entity\": \"shcau-university-of-calgary-in-alberta\",\n",
        "    \"project\": \"transfer_learning_garbage\",\n",
        "    \"name\": WANDB_RUN_NAME,\n",
        "    \"tags\": [\"distilBERT\", \"efficientnet\", \"CVPR_2024_dataset\"],\n",
        "    \"notes\": NOTES,\n",
        "    \"config\": {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"dataset\": \"CVPR_2024_dataset\",\n",
        "        \"image_size\": IMAGE_SIZE,\n",
        "        \"num_workers\": NUM_WORKERS,\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"max_len\": MAX_LEN,\n",
        "        \"learning_rate_image\": LEARNING_RATE_IMAGE,\n",
        "        \"learning_rate_text\": LEARNING_RATE_TEXT,\n",
        "        \"learning_rate_fusion\": LEARNING_RATE_FUSION,\n",
        "        \"learning_rate_classifier\": LEARNING_RATE_CLASSIFIER,\n",
        "        \"learning_rate_unfreeze_image\": LEARNING_RATE_UNFREEZE_IMAGE, # learning rate for unfrozen EfficientNet layers\n",
        "        \"learning_rate_unfreeze_text\": LEARNING_RATE_UNFREEZE_TEXT, # learning rate for unfrozen DistilBERT layers\n",
        "        \"dropout_image\": DROPOUT_IMAGE,\n",
        "        \"dropout_text\": DROPOUT_TEXT,\n",
        "        \"dropout_classifier\": DROPOUT_CLASSIFIER,\n",
        "        \"convergence_threshold\": CONVERGENCE_THRESHOLD,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"weight_decay_text\": WEIGHT_DECAY_TEXT,\n",
        "        \"weight_decay_image\": WEIGHT_DECAY_IMAGE,\n",
        "        \"weight_decay_classifier\": WEIGHT_DECAY_CLASSIFIER,\n",
        "        \"label_smoothing_prediction\": LABEL_SMOOTHING_PREDICTION,\n",
        "        \"optimizer\": OPTIMIZER \n",
        "    },\n",
        "    \"job_type\": \"train\",\n",
        "    \"resume\": \"allow\",\n",
        "}\n",
        "\n",
        "# Normalization Stats\n",
        "NORMALIZATION_STATS = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOG_FILE = \"experiment_multimodal_attention_gated_fusion\"  # Log file name\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Log everything (INFO and above)\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(LOG_FILE, mode='w'),  # Overwrite log file on each run\n",
        "        logging.StreamHandler(sys.stdout)  # Print log messages to console too\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:55:05,297 - INFO - [CONFIG] ============================== Experiment Configuration ==============================\n",
            "2025-03-24 19:55:05,298 - INFO - [CONFIG] Experiment Name: experiment_multimodal_attention_gated_fusion\n",
            "2025-03-24 19:55:05,298 - INFO - [CONFIG] Entity: shcau-university-of-calgary-in-alberta\n",
            "2025-03-24 19:55:05,299 - INFO - [CONFIG] Project: transfer_learning_garbage\n",
            "2025-03-24 19:55:05,300 - INFO - [CONFIG] Tags: distilBERT, efficientnet, CVPR_2024_dataset\n",
            "2025-03-24 19:55:05,300 - INFO - [CONFIG] Notes: \n",
            "\n",
            "2025-03-24 19:55:05,301 - INFO - [CONFIG] Job Type: train\n",
            "2025-03-24 19:55:05,301 - INFO - [CONFIG] Resume: allow\n",
            "2025-03-24 19:55:05,302 - INFO - [CONFIG] ------------------------------ Hyperparameters ------------------------------\n",
            "2025-03-24 19:55:05,303 - INFO - [CONFIG] epochs: 50\n",
            "2025-03-24 19:55:05,303 - INFO - [CONFIG] batch_size: 64\n",
            "2025-03-24 19:55:05,304 - INFO - [CONFIG] dataset: CVPR_2024_dataset\n",
            "2025-03-24 19:55:05,304 - INFO - [CONFIG] image_size: (224, 224)\n",
            "2025-03-24 19:55:05,305 - INFO - [CONFIG] num_workers: 4\n",
            "2025-03-24 19:55:05,305 - INFO - [CONFIG] num_classes: 4\n",
            "2025-03-24 19:55:05,306 - INFO - [CONFIG] max_len: 40\n",
            "2025-03-24 19:55:05,307 - INFO - [CONFIG] learning_rate_image: 0.001\n",
            "2025-03-24 19:55:05,307 - INFO - [CONFIG] learning_rate_text: 2e-05\n",
            "2025-03-24 19:55:05,308 - INFO - [CONFIG] learning_rate_fusion: 0.001\n",
            "2025-03-24 19:55:05,308 - INFO - [CONFIG] learning_rate_classifier: 0.005\n",
            "2025-03-24 19:55:05,309 - INFO - [CONFIG] learning_rate_unfreeze_image: 1e-05\n",
            "2025-03-24 19:55:05,309 - INFO - [CONFIG] learning_rate_unfreeze_text: 1e-05\n",
            "2025-03-24 19:55:05,310 - INFO - [CONFIG] dropout_image: 0.2\n",
            "2025-03-24 19:55:05,310 - INFO - [CONFIG] dropout_text: 0.1\n",
            "2025-03-24 19:55:05,311 - INFO - [CONFIG] dropout_classifier: 0.1\n",
            "2025-03-24 19:55:05,312 - INFO - [CONFIG] convergence_threshold: 0.001\n",
            "2025-03-24 19:55:05,312 - INFO - [CONFIG] patience: 10\n",
            "2025-03-24 19:55:05,313 - INFO - [CONFIG] weight_decay_text: 0.001\n",
            "2025-03-24 19:55:05,313 - INFO - [CONFIG] weight_decay_image: 0.0001\n",
            "2025-03-24 19:55:05,314 - INFO - [CONFIG] weight_decay_classifier: 0.001\n",
            "2025-03-24 19:55:05,314 - INFO - [CONFIG] label_smoothing_prediction: 0.05\n",
            "2025-03-24 19:55:05,315 - INFO - [CONFIG] optimizer: AdamW\n",
            "2025-03-24 19:55:05,316 - INFO - [CONFIG] =============================================================================\n"
          ]
        }
      ],
      "source": [
        "# Log the configuration\n",
        "logging.info(\"[CONFIG] ============================== Experiment Configuration ==============================\")\n",
        "\n",
        "# Log top-level keys\n",
        "logging.info(f\"[CONFIG] Experiment Name: {WANDB_CONFIG['name']}\")\n",
        "logging.info(f\"[CONFIG] Entity: {WANDB_CONFIG['entity']}\")\n",
        "logging.info(f\"[CONFIG] Project: {WANDB_CONFIG['project']}\")\n",
        "logging.info(f\"[CONFIG] Tags: {', '.join(WANDB_CONFIG['tags'])}\")\n",
        "logging.info(f\"[CONFIG] Notes: {WANDB_CONFIG['notes']}\")\n",
        "logging.info(f\"[CONFIG] Job Type: {WANDB_CONFIG['job_type']}\")\n",
        "logging.info(f\"[CONFIG] Resume: {WANDB_CONFIG['resume']}\")\n",
        "\n",
        "# Log nested configuration (under 'config')\n",
        "logging.info(\"[CONFIG] ------------------------------ Hyperparameters ------------------------------\")\n",
        "for key, value in WANDB_CONFIG[\"config\"].items():\n",
        "    logging.info(f\"[CONFIG] {key}: {value}\")\n",
        "\n",
        "logging.info(\"[CONFIG] =============================================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_wandb(fold):\n",
        "    \"\"\"Initialize wandb for each fold with a unique run name.\"\"\"\n",
        "    wandb.init(\n",
        "        entity=WANDB_CONFIG[\"entity\"],\n",
        "        project=WANDB_CONFIG[\"project\"],\n",
        "        name=f\"{WANDB_RUN_NAME}_fold_{fold + 1}\",\n",
        "        tags=WANDB_CONFIG[\"tags\"],\n",
        "        notes=WANDB_CONFIG[\"notes\"],\n",
        "        config=WANDB_CONFIG[\"config\"],\n",
        "        job_type=WANDB_CONFIG[\"job_type\"],\n",
        "        resume=WANDB_CONFIG[\"resume\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SpaCy for lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load NLTK stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Standardize text, remove stopwords, and apply lemmatization.\"\"\"\n",
        "    # 1. Standardize text (lowercasing & trimming spaces)\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # 2. Remove stopwords\n",
        "    text_tokens = text.split()\n",
        "    text = \" \".join([word for word in text_tokens if word not in stop_words])\n",
        "\n",
        "    # 3. Lemmatization\n",
        "    doc = nlp(text)\n",
        "    text = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "    return text\n",
        "\n",
        "def read_text_files_with_labels_and_image_paths(path):\n",
        "    \"\"\"Extract text from file names, apply preprocessing, and return labels with image paths.\"\"\"\n",
        "    texts, labels, image_paths = [], [], []\n",
        "    class_folders = sorted(os.listdir(path))\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = sorted(os.listdir(class_path))  # Sort to ensure order consistency\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    # Extract filename without extension\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "\n",
        "                    # Replace underscores with spaces\n",
        "                    text = file_name_no_ext.replace(\"_\", \" \")\n",
        "\n",
        "                    # Remove numbers\n",
        "                    text_without_digits = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "                    # Apply preprocessing\n",
        "                    preprocessed_text = preprocess_text(text_without_digits)\n",
        "\n",
        "                    texts.append(preprocessed_text)\n",
        "                    labels.append(label_map[class_name])\n",
        "                    image_paths.append(file_path)\n",
        "\n",
        "    return np.array(texts), np.array(labels), np.array(image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    \"\"\"Dataset class for text data.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "    \n",
        "# Custom dataset class for images\n",
        "class ImageDataset(Dataset):\n",
        "    \"\"\"Dataset class for image data.\"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    \"\"\"Dataset class for multimodal data (image + text).\"\"\"\n",
        "    def __init__(self, image_dataset, text_dataset):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_dataset), len(self.text_dataset))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.image_dataset[idx]\n",
        "        text_data = self.text_dataset[idx]\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": text_data[\"input_ids\"],\n",
        "            \"attention_mask\": text_data[\"attention_mask\"],\n",
        "            \"label\": label\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================== Gated Fusion ========================\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(GatedFusion, self).__init__()\n",
        "        self.gate = nn.Linear(2 * feature_dim, feature_dim)  # Learnable gate\n",
        "        self.sigmoid = nn.Sigmoid()  # Activation\n",
        "\n",
        "    def forward(self, text_feat, image_feat):\n",
        "        combined_feat = torch.cat((text_feat, image_feat), dim=1)\n",
        "        gate_value = self.sigmoid(self.gate(combined_feat))  # Value between 0-1\n",
        "        fused_feat = (gate_value * text_feat) + ((1 - gate_value) * image_feat)  # Weighted fusion\n",
        "        return fused_feat\n",
        "\n",
        "# ======================== Attention Fusion ========================\n",
        "class AttentionFusion(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(AttentionFusion, self).__init__()\n",
        "        self.Wq = nn.Linear(feature_dim, feature_dim)  # Query\n",
        "        self.Wk = nn.Linear(feature_dim, feature_dim)  # Key\n",
        "        self.Wv = nn.Linear(feature_dim, feature_dim)  # Value\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, fused_feat):\n",
        "        q = self.Wq(fused_feat)\n",
        "        k = self.Wk(fused_feat)\n",
        "        v = self.Wv(fused_feat)\n",
        "        \n",
        "        attention_scores = self.softmax(torch.bmm(q.unsqueeze(1), k.unsqueeze(2)).squeeze(2))\n",
        "        refined_feat = attention_scores * v  # Weighted fusion\n",
        "        return refined_feat + fused_feat  # Residual connection\n",
        "\n",
        "# ======================== Multimodal Classifier (Last Feature Extractor Layer Unfrozen) ========================\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    \"\"\"Multimodal model combining EfficientNetB0 and DistilBERT with partial fine-tuning.\"\"\"\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # ----------- Image Feature Extractor (EfficientNetB0) -----------\n",
        "        self.image_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        \n",
        "        # Freeze all layers except the last one\n",
        "        for param in self.image_model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.image_model.features[-3:].parameters():  # Unfreeze last feature layer\n",
        "            param.requires_grad = True\n",
        "\n",
        "        num_ftrs = self.image_model.classifier[1].in_features\n",
        "        self.image_model.classifier = nn.Identity()  # Remove classifier\n",
        "        self.image_fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_IMAGE)\n",
        "        )\n",
        "\n",
        "        # ----------- Text Feature Extractor (DistilBERT) -----------\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # Freeze all layers except the last transformer layer\n",
        "        for param in self.text_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.text_model.transformer.layer[-2:].parameters():  # Unfreeze last transformer layer\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(self.text_model.config.hidden_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_TEXT)\n",
        "        )\n",
        "\n",
        "        # ----------- Normalize Features -----------\n",
        "        self.text_norm = nn.LayerNorm(512)\n",
        "        self.image_norm = nn.LayerNorm(512)\n",
        "\n",
        "        # ----------- Gated Fusion -----------\n",
        "        self.gated_fusion = GatedFusion(feature_dim=512)\n",
        "\n",
        "        # ----------- Attention Fusion -----------\n",
        "        self.attention_fusion = AttentionFusion(feature_dim=512)\n",
        "\n",
        "        # ----------- Fully Connected Fusion & Classification -----------\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512, 512),  # Increase dimension\n",
        "            nn.BatchNorm1d(512),  # Add batch normalization\n",
        "            nn.ReLU(),            # Use GELU activation\n",
        "\n",
        "            nn.Linear(512, 256),  # Intermediate layer\n",
        "            nn.BatchNorm1d(256),  # Batch normalization\n",
        "            nn.ReLU(),            # GELU activation\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT_CLASSIFIER)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, image_inputs):\n",
        "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(text_output.last_hidden_state[:, 0, :])\n",
        "        text_features = self.text_norm(text_features)\n",
        "        image_features = self.image_fc(self.image_model(image_inputs))\n",
        "        image_features = self.image_norm(image_features)\n",
        "        gated_feat = self.gated_fusion(text_features, image_features)\n",
        "        refined_feat = self.attention_fusion(gated_feat)\n",
        "        fused_features = self.fusion_fc(refined_feat)\n",
        "        output = self.classifier(self.dropout(fused_features))\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:57:22,599 - INFO - First 4 samples of dataset:\n",
            "\n",
            "2025-03-24 19:57:22,599 - INFO - Texts: ['aero bar wrapper' 'break glass' 'break rubber' 'butter paper']\n",
            "2025-03-24 19:57:22,600 - INFO - Labels: [0 0 0 0]\n",
            "2025-03-24 19:57:22,601 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Aero_bar_wrapper_1.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_Glass_5291.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_rubber_7263.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Butter_Paper_9976.png']\n",
            "2025-03-24 19:57:22,602 - INFO - \n",
            "Last 4 samples of dataset:\n",
            "\n",
            "2025-03-24 19:57:22,602 - INFO - Texts: ['wristwatch' 'xbox controller' 'xbox one controller' 'zipper file bag']\n",
            "2025-03-24 19:57:22,603 - INFO - Labels: [3 3 3 3]\n",
            "2025-03-24 19:57:22,603 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\wristwatch_3782.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_controller_2047.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_one_controller_2048.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\zipper_file_bag_2049.png']\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "texts, labels, image_paths = read_text_files_with_labels_and_image_paths(DATA_DIR)\n",
        "\n",
        "# Log first and last 4 samples\n",
        "logging.info(\"First 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[:4]}\")\n",
        "logging.info(f\"Labels: {labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[-4:]}\")\n",
        "logging.info(f\"Labels: {labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split into test set and development set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:57:23,420 - INFO - First 4 samples of test set:\n",
            "\n",
            "2025-03-24 19:57:23,420 - INFO - Texts: ['ballast light' 'old phone' 'milk jug lid tab' 'dirty dish sponge']\n",
            "2025-03-24 19:57:23,421 - INFO - Labels: [3 3 0 0]\n",
            "2025-03-24 19:57:23,422 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\ballast_light_286.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\Old_Phones_7828.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\milk_jug_lid_tab_1137.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\dirty_dish_sponge_437.png']\n",
            "2025-03-24 19:57:23,423 - INFO - \n",
            "Last 4 samples of test set:\n",
            "\n",
            "2025-03-24 19:57:23,423 - INFO - Texts: ['empty glass jar' 'non - stretchy plastic' 'backpack' 'piece break glass']\n",
            "2025-03-24 19:57:23,424 - INFO - Labels: [1 0 3 0]\n",
            "2025-03-24 19:57:23,424 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Blue\\\\empty_glass_jar_1609.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\non-stretchy plastic.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\backpack_216.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\piece_of_broken_glass_1315.png']\n"
          ]
        }
      ],
      "source": [
        "# Split into a test set and development set\n",
        "train_texts, test_texts, train_labels, test_labels, train_image_paths, test_image_paths = train_test_split(\n",
        "    texts, labels, image_paths, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Log first 4 samples of test set\n",
        "logging.info(\"First 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[:4]}\")\n",
        "logging.info(f\"Labels: {test_labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[-4:]}\")\n",
        "logging.info(f\"Labels: {test_labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE), \n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Apply correct normalization\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Only resize + normalize\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Only resize + normalize\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Tokenizer for DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoader for test set\n",
        "\n",
        "Create the dataloader for the test set and set aside for model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test dataset\n",
        "test_image_dataset = ImageDataset(test_image_paths, test_labels, transform[\"test\"])\n",
        "test_text_dataset = CustomTextDataset(test_texts, test_labels, tokenizer, max_len=MAX_LEN)  # Ensure tokenizer is defined\n",
        "test_multimodal_dataset = MultimodalDataset(test_image_dataset, test_text_dataset)\n",
        "\n",
        "# DataLoader for test set\n",
        "test_loader = DataLoader(test_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a peek at a batch in the test set to verify that data has been correctly organized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:57:30,447 - INFO - [INFO] One Batch Sample Inspection:\n",
            "2025-03-24 19:57:30,447 - INFO -    Images Shape: torch.Size([64, 3, 224, 224])\n",
            "2025-03-24 19:57:30,448 - INFO -    Input IDs Shape: torch.Size([64, 40])\n",
            "2025-03-24 19:57:30,448 - INFO -    Attention Mask Shape: torch.Size([64, 40])\n",
            "2025-03-24 19:57:30,449 - INFO -    Labels Shape: torch.Size([64])\n",
            "2025-03-24 19:57:30,450 - INFO - \n",
            "[INFO] First Sample:\n",
            "2025-03-24 19:57:30,477 - INFO -    Image Tensor: tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         ...,\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
            "\n",
            "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         ...,\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
            "\n",
            "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         ...,\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]])\n",
            "2025-03-24 19:57:30,479 - INFO -    Input IDs: tensor([  101, 28030,  2422,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "2025-03-24 19:57:30,480 - INFO -    Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "2025-03-24 19:57:30,480 - INFO -    Label: 3\n"
          ]
        }
      ],
      "source": [
        "# Get one batch\n",
        "for batch in test_loader:\n",
        "    images = batch[\"image\"]  # Image tensor\n",
        "    input_ids = batch[\"input_ids\"]  # Tokenized text tensor\n",
        "    attention_mask = batch[\"attention_mask\"]  # Attention mask\n",
        "    labels = batch[\"label\"]  # Labels tensor\n",
        "\n",
        "    # Log shapes of tensors\n",
        "    logging.info(\"[INFO] One Batch Sample Inspection:\")\n",
        "    logging.info(f\"   Images Shape: {images.shape}\")\n",
        "    logging.info(f\"   Input IDs Shape: {input_ids.shape}\")\n",
        "    logging.info(f\"   Attention Mask Shape: {attention_mask.shape}\")\n",
        "    logging.info(f\"   Labels Shape: {labels.shape}\")\n",
        "\n",
        "    # Log first sample details\n",
        "    logging.info(\"\\n[INFO] First Sample:\")\n",
        "    logging.info(f\"   Image Tensor: {images[0]}\")\n",
        "    logging.info(f\"   Input IDs: {input_ids[0]}\")\n",
        "    logging.info(f\"   Attention Mask: {attention_mask[0]}\")\n",
        "    logging.info(f\"   Label: {labels[0]}\")\n",
        "\n",
        "    break  # Stop after inspecting one batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Stratified K-Fold on the development set to split into train/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:57:31,626 - INFO - [INFO] Fold 1/5\n",
            "2025-03-24 19:57:31,627 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 19:57:31,628 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 19:57:31,629 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 19:57:31,629 - INFO - [INFO] Fold 2/5\n",
            "2025-03-24 19:57:31,630 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 19:57:31,631 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 19:57:31,632 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 19:57:31,633 - INFO - [INFO] Fold 3/5\n",
            "2025-03-24 19:57:31,633 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 19:57:31,634 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-24 19:57:31,635 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-24 19:57:31,636 - INFO - [INFO] Fold 4/5\n",
            "2025-03-24 19:57:31,636 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 19:57:31,637 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-24 19:57:31,638 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n",
            "2025-03-24 19:57:31,639 - INFO - [INFO] Fold 5/5\n",
            "2025-03-24 19:57:31,639 - INFO - [INFO] Class Distributions:\n",
            "2025-03-24 19:57:31,641 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-24 19:57:31,641 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    logging.info(f\"[INFO] Fold {fold + 1}/{K_FOLDS}\")\n",
        "\n",
        "    # Extract labels for current fold\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "\n",
        "    # Log class distributions\n",
        "    logging.info(\"[INFO] Class Distributions:\")\n",
        "    logging.info(f\"   Train Class Distribution: {Counter(train_labels_fold)}\")\n",
        "    logging.info(f\"   Validation Class Distribution: {Counter(val_labels_fold)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify k-fold was applied correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:57:36,471 - INFO - [INFO] No data leakage detected in Fold 1\n",
            "2025-03-24 19:57:36,473 - INFO - [INFO] No data leakage detected in Fold 2\n",
            "2025-03-24 19:57:36,474 - INFO - [INFO] No data leakage detected in Fold 3\n",
            "2025-03-24 19:57:36,475 - INFO - [INFO] No data leakage detected in Fold 4\n",
            "2025-03-24 19:57:36,477 - INFO - [INFO] No data leakage detected in Fold 5\n"
          ]
        }
      ],
      "source": [
        "# Ensure no data leakage in folds\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    train_set = set(train_idx)\n",
        "    val_set = set(val_idx)\n",
        "\n",
        "    # Check for intersection (should be empty)\n",
        "    intersection = train_set.intersection(val_set)\n",
        "    assert len(intersection) == 0, f\"Data leakage detected in Fold {fold + 1}\"\n",
        "\n",
        "    logging.info(f\"[INFO] No data leakage detected in Fold {fold + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "#     train_labels_fold = train_labels[train_idx]\n",
        "#     val_labels_fold = train_labels[val_idx]\n",
        "\n",
        "#     plt.figure(figsize=(10, 4))\n",
        "#     plt.hist(train_labels_fold, bins=len(set(train_labels)), alpha=0.6, label=\"Train\")\n",
        "#     plt.hist(val_labels_fold, bins=len(set(train_labels)), alpha=0.6, label=\"Validation\")\n",
        "#     plt.title(f\"Class Distribution in Fold {fold + 1}\")\n",
        "#     plt.legend()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Move data to the appropriate device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)  # Compute batch loss\n",
        "\n",
        "            # Aggregate loss for averaging\n",
        "            total_loss += loss.item() * labels.size(0)  # Multiply by batch size for proper averaging\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total  # Normalize loss over total samples\n",
        "    accuracy = correct / total  # Compute accuracy\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Weight Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adaptive_weight_decay(epoch, warmup_epochs=5, decay_factors=(0.1, 1.0)):\n",
        "    \"\"\"\n",
        "    Returns a scaled weight decay based on epoch number.\n",
        "    During warm-up, it applies a lower decay (decay_factors[0]).\n",
        "    After warm-up, it applies full weight decay (decay_factors[1]).\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return decay_factors[0]  # Use lower decay during warm-up\n",
        "    return decay_factors[1]  # Use normal decay afterward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_warmup_lr(epoch, warmup_epochs, base_lr):\n",
        "    \"\"\"\n",
        "    Linear warmup schedule for the learning rate.\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return base_lr * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        return base_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True):\n",
        "    initialize_wandb(fold)\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    best_val_loss = float(\"inf\")  # Track best validation loss\n",
        "    epochs_without_improvement = 0  # Track epochs without improvement until equals patience\n",
        "\n",
        "    # ================ ReduceLROnPlateau Scheduler ================\n",
        "    plateau_scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=LR_SCHEDULING_FACTOR, patience=3, verbose=True\n",
        "    )\n",
        "    \n",
        "    # AMP GradScaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    epoch_start_time = time.time()  # Start total training timer\n",
        "    logging.info(\"[TRAIN INFO] Starting Training...\")\n",
        "\n",
        "    # Warmup settings\n",
        "    WARMUP_EPOCHS = 8  # Number of epochs for warmup\n",
        "    base_lr_image = LEARNING_RATE_IMAGE  # Base learning rate for EfficientNet\n",
        "    base_lr_text = LEARNING_RATE_TEXT  # Base learning rate for DistilBERT\n",
        "    base_lr_fusion = LEARNING_RATE_FUSION  # Base learning rate for fusion layer\n",
        "    base_lr_classifier = LEARNING_RATE_CLASSIFIER  # Base learning rate for classifier\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        logging.info(f\"[TRAIN INFO] ============================== Epoch {epoch + 1}/{EPOCHS} ==============================\")\n",
        "        \n",
        "        # Apply learning rate warmup\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            warmup_lr_image = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_image)\n",
        "            warmup_lr_text = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_text)\n",
        "            warmup_lr_fusion = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_fusion)\n",
        "            warmup_lr_classifier = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_classifier)\n",
        "\n",
        "            # Update learning rates for each parameter group\n",
        "            optimizer.param_groups[0][\"lr\"] = warmup_lr_image  # Unfrozen EfficientNet layer\n",
        "            optimizer.param_groups[1][\"lr\"] = warmup_lr_text  # Unfrozen DistilBERT layer\n",
        "            optimizer.param_groups[2][\"lr\"] = warmup_lr_image  # Image FC layer\n",
        "            optimizer.param_groups[3][\"lr\"] = warmup_lr_text  # Text FC layer\n",
        "            optimizer.param_groups[4][\"lr\"] = warmup_lr_fusion  # Fusion layer\n",
        "            optimizer.param_groups[5][\"lr\"] = warmup_lr_classifier  # Classifier layer\n",
        "\n",
        "        model.train()  # Set model to training modes\n",
        "        total_train_loss = 0  # Track total training loss for the epoch\n",
        "        batch_train_loss = 0  # Track batch loss for gradient accumulation\n",
        "        step = 0  # Track the number of batches processed\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Training phase\n",
        "        for step, batch in enumerate(dataloaders[\"train_loader\"], 1):\n",
        "            # Move data to device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask, images)  # Send inputs to network and receive outputs\n",
        "                loss = criterion(outputs, labels) / GRAD_ACCUM_STEPS  # Compute loss (no normalization for gradient accumulation)\n",
        "\n",
        "            # Backward pass and optimizer step\n",
        "            scaler.scale(loss).backward()  # Scale loss and backpropagate\n",
        "\n",
        "            batch_train_loss += loss.item()\n",
        "            total_train_loss += loss.item() * GRAD_ACCUM_STEPS  # Undo normalization for total loss\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            # Perform optimizer step before learning rate scheduler step\n",
        "            if step % GRAD_ACCUM_STEPS == 0 or step == len(dataloaders[\"train_loader\"]):\n",
        "                # Gradient Clipping\n",
        "                scaler.unscale_(optimizer)  # Unscale gradients before clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to a max norm of 1.0\n",
        "\n",
        "                # Optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Log batch loss\n",
        "                logging.info(f\"[TRAIN INFO] Batch {step}/{len(dataloaders['train_loader'])}, Accumulated loss over {GRAD_ACCUM_STEPS} batches: {batch_train_loss:.4f}\")\n",
        "                batch_train_loss = 0  # Reset batch loss for the next accumulation\n",
        "\n",
        "        # Validation step to see how well model performs this epoch\n",
        "        logging.info(f\"[TRAIN INFO] Evaluating model...\")\n",
        "        val_loss, val_acc = evaluate_model(model, dataloaders[\"val_loader\"], device)\n",
        "        avg_train_loss = total_train_loss / len(dataloaders[\"train_loader\"])\n",
        "\n",
        "        # **Learning Rate Scheduler Handling**\n",
        "        plateau_scheduler.step(val_loss)  \n",
        "\n",
        "        # Log weight decay and learning rate updates\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_val_loss_diff\": avg_train_loss - val_loss,  # Track overfitting tendency\n",
        "            \"early_stopping_epochs\": epochs_without_improvement,  # Track early stopping\n",
        "            \"learning_rate_image\": optimizer.param_groups[0][\"lr\"],  # Log learning rates\n",
        "            \"learning_rate_text\": optimizer.param_groups[1][\"lr\"],\n",
        "            \"learning_rate_fusion\": optimizer.param_groups[4][\"lr\"],\n",
        "            \"learning_rate_classifier\": optimizer.param_groups[5][\"lr\"],\n",
        "        })\n",
        "\n",
        "        logging.info(f\"[TRAIN INFO] Epoch {epoch + 1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss - CONVERGENCE_THRESHOLD:  # If loss improves, save the model\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0  # Reset epochs without improvement counter for patience\n",
        "            torch.save(model.state_dict(), f\"{MODEL_NAME}_fold_{fold+1}.pth\")\n",
        "            logging.info(f\"[TRAIN INFO] Best Model Saved for Fold {fold + 1}\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1  # Increment until patience reached\n",
        "\n",
        "        # Early stopping if no improvement for epochs\n",
        "        if epochs_without_improvement >= PATIENCE:\n",
        "            total_training_time = time.time() - epoch_start_time\n",
        "            logging.info(f\"[TRAIN INFO] Early stopping at epoch {epoch + 1} as validation loss did not improve for {PATIENCE} epochs.\")\n",
        "            logging.info(f\"[TRAIN INFO] Total Time: {total_training_time:.2f}s\")\n",
        "            wandb.finish()\n",
        "            break\n",
        "\n",
        "    total_training_time = time.time() - epoch_start_time\n",
        "    logging.info(f\"[TRAIN INFO] Fold {fold + 1} Training Complete at epoch {epoch + 1}. Total Time: {total_training_time:.2f}s\")\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:57:41,557 - INFO - [K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\n",
            "2025-03-24 19:57:41,560 - INFO - [K-FOLD INFO] ============================== Fold 1/5 ==============================\n",
            "2025-03-24 19:57:41,562 - INFO - [K-FOLD INFO] Fold 1:\n",
            "2025-03-24 19:57:41,562 - INFO -    Train Samples: 8594\n",
            "2025-03-24 19:57:41,563 - INFO -    Validation Samples: 2149\n",
            "2025-03-24 19:57:41,563 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 1\n",
            "2025-03-24 19:57:41,564 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 1:\n",
            "2025-03-24 19:57:41,564 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 19:57:42,340 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 1\n",
            "2025-03-24 19:57:42,342 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 1:\n",
            "2025-03-24 19:57:42,342 - INFO - [K-FOLD INFO] Loss function initialized for Fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: shcau (shcau-university-of-calgary-in-alberta) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_gated\\wandb\\run-20250324_195743-wft1mavq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wft1mavq' target=\"_blank\">experiment_multimodal_attention_gated_fusion_fold_1</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wft1mavq' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wft1mavq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arkzs\\miniforge3\\envs\\enel645_torch_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_14776\\836902376.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:57:44,725 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 19:57:44,726 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_14776\\836902376.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 19:57:53,108 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0900\n",
            "2025-03-24 19:58:01,723 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4612\n",
            "2025-03-24 19:58:10,457 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.4687\n",
            "2025-03-24 19:58:19,185 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.4256\n",
            "2025-03-24 19:58:27,869 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3135\n",
            "2025-03-24 19:58:36,616 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2988\n",
            "2025-03-24 19:58:45,574 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.3134\n",
            "2025-03-24 19:58:54,296 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2207\n",
            "2025-03-24 19:59:02,968 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2519\n",
            "2025-03-24 19:59:11,632 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2108\n",
            "2025-03-24 19:59:20,450 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1877\n",
            "2025-03-24 19:59:29,361 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1944\n",
            "2025-03-24 19:59:37,275 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1124\n",
            "2025-03-24 19:59:45,264 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1553\n",
            "2025-03-24 19:59:53,149 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.0751\n",
            "2025-03-24 20:00:01,495 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.0675\n",
            "2025-03-24 20:00:10,147 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.0828\n",
            "2025-03-24 20:00:18,009 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0650\n",
            "2025-03-24 20:00:25,653 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0551\n",
            "2025-03-24 20:00:33,738 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0179\n",
            "2025-03-24 20:00:42,130 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.9624\n",
            "2025-03-24 20:00:49,982 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.9807\n",
            "2025-03-24 20:00:57,539 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0036\n",
            "2025-03-24 20:01:05,351 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0280\n",
            "2025-03-24 20:01:13,336 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 1.0384\n",
            "2025-03-24 20:01:21,128 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9962\n",
            "2025-03-24 20:01:29,124 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9766\n",
            "2025-03-24 20:01:37,130 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9425\n",
            "2025-03-24 20:01:45,132 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9477\n",
            "2025-03-24 20:01:52,716 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9591\n",
            "2025-03-24 20:02:00,501 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9850\n",
            "2025-03-24 20:02:08,297 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9776\n",
            "2025-03-24 20:02:15,921 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9433\n",
            "2025-03-24 20:02:21,699 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7050\n",
            "2025-03-24 20:02:22,461 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1808\n",
            "2025-03-24 20:02:22,462 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:03:20,954 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1169, Val Loss: 0.7659, Val Acc: 0.7073\n",
            "2025-03-24 20:03:21,252 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 20:03:21,252 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 20:03:27,220 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6576\n",
            "2025-03-24 20:03:34,643 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.8833\n",
            "2025-03-24 20:03:42,425 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8644\n",
            "2025-03-24 20:03:50,238 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8646\n",
            "2025-03-24 20:03:57,960 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.9075\n",
            "2025-03-24 20:04:05,583 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8700\n",
            "2025-03-24 20:04:13,653 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8537\n",
            "2025-03-24 20:04:21,751 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.9124\n",
            "2025-03-24 20:04:29,439 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8481\n",
            "2025-03-24 20:04:37,258 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8190\n",
            "2025-03-24 20:04:45,236 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8274\n",
            "2025-03-24 20:04:53,055 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8822\n",
            "2025-03-24 20:05:00,741 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8172\n",
            "2025-03-24 20:05:08,426 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8244\n",
            "2025-03-24 20:05:16,652 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.8597\n",
            "2025-03-24 20:05:24,090 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7632\n",
            "2025-03-24 20:05:31,475 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7491\n",
            "2025-03-24 20:05:39,446 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.9251\n",
            "2025-03-24 20:05:47,507 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7647\n",
            "2025-03-24 20:05:55,111 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7640\n",
            "2025-03-24 20:06:02,782 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7985\n",
            "2025-03-24 20:06:10,736 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.8333\n",
            "2025-03-24 20:06:18,751 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7810\n",
            "2025-03-24 20:06:26,672 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8103\n",
            "2025-03-24 20:06:34,823 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.8464\n",
            "2025-03-24 20:06:42,581 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7603\n",
            "2025-03-24 20:06:50,519 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7339\n",
            "2025-03-24 20:06:58,738 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7217\n",
            "2025-03-24 20:07:06,547 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.8072\n",
            "2025-03-24 20:07:14,554 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7899\n",
            "2025-03-24 20:07:22,394 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7119\n",
            "2025-03-24 20:07:30,204 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7307\n",
            "2025-03-24 20:07:38,011 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7342\n",
            "2025-03-24 20:07:44,138 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5917\n",
            "2025-03-24 20:07:44,724 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2335\n",
            "2025-03-24 20:07:44,724 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:08:41,181 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8161, Val Loss: 0.5460, Val Acc: 0.7925\n",
            "2025-03-24 20:08:41,585 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 20:08:41,586 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 20:08:47,476 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5951\n",
            "2025-03-24 20:08:54,787 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6936\n",
            "2025-03-24 20:09:02,568 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.7210\n",
            "2025-03-24 20:09:10,072 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.7250\n",
            "2025-03-24 20:09:17,471 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7304\n",
            "2025-03-24 20:09:24,968 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7437\n",
            "2025-03-24 20:09:32,417 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7061\n",
            "2025-03-24 20:09:40,369 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5744\n",
            "2025-03-24 20:09:48,299 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7472\n",
            "2025-03-24 20:09:56,193 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6113\n",
            "2025-03-24 20:10:03,530 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7164\n",
            "2025-03-24 20:10:11,328 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6545\n",
            "2025-03-24 20:10:18,960 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5960\n",
            "2025-03-24 20:10:26,559 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6312\n",
            "2025-03-24 20:10:34,245 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6757\n",
            "2025-03-24 20:10:41,787 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7236\n",
            "2025-03-24 20:10:49,015 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6965\n",
            "2025-03-24 20:10:56,423 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6622\n",
            "2025-03-24 20:11:03,926 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6266\n",
            "2025-03-24 20:11:11,420 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6802\n",
            "2025-03-24 20:11:19,021 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6409\n",
            "2025-03-24 20:11:26,763 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6592\n",
            "2025-03-24 20:11:34,535 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6236\n",
            "2025-03-24 20:11:41,910 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6440\n",
            "2025-03-24 20:11:49,532 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6683\n",
            "2025-03-24 20:11:57,157 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6773\n",
            "2025-03-24 20:12:04,872 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6839\n",
            "2025-03-24 20:12:12,486 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5776\n",
            "2025-03-24 20:12:20,254 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6922\n",
            "2025-03-24 20:12:27,479 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6225\n",
            "2025-03-24 20:12:35,175 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6646\n",
            "2025-03-24 20:12:42,620 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5791\n",
            "2025-03-24 20:12:50,104 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6420\n",
            "2025-03-24 20:12:55,647 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4592\n",
            "2025-03-24 20:12:56,202 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2010\n",
            "2025-03-24 20:12:56,203 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:13:52,801 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6680, Val Loss: 0.4370, Val Acc: 0.8404\n",
            "2025-03-24 20:13:53,137 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 20:13:53,137 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 20:13:59,091 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4804\n",
            "2025-03-24 20:14:06,892 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6031\n",
            "2025-03-24 20:14:14,482 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6061\n",
            "2025-03-24 20:14:22,277 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5565\n",
            "2025-03-24 20:14:29,899 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6183\n",
            "2025-03-24 20:14:37,609 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5435\n",
            "2025-03-24 20:14:45,471 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6439\n",
            "2025-03-24 20:14:53,201 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5816\n",
            "2025-03-24 20:15:00,870 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5321\n",
            "2025-03-24 20:15:08,534 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6576\n",
            "2025-03-24 20:15:16,072 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6971\n",
            "2025-03-24 20:15:23,553 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5786\n",
            "2025-03-24 20:15:31,084 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5956\n",
            "2025-03-24 20:15:38,743 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5883\n",
            "2025-03-24 20:15:46,333 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5410\n",
            "2025-03-24 20:15:53,809 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5710\n",
            "2025-03-24 20:16:01,449 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5340\n",
            "2025-03-24 20:16:09,054 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5861\n",
            "2025-03-24 20:16:16,519 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5245\n",
            "2025-03-24 20:16:24,029 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5583\n",
            "2025-03-24 20:16:31,829 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5817\n",
            "2025-03-24 20:16:39,272 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6100\n",
            "2025-03-24 20:16:46,714 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5541\n",
            "2025-03-24 20:16:54,209 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5700\n",
            "2025-03-24 20:17:01,653 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5493\n",
            "2025-03-24 20:17:09,024 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5155\n",
            "2025-03-24 20:17:16,578 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6361\n",
            "2025-03-24 20:17:23,923 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6105\n",
            "2025-03-24 20:17:31,399 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5684\n",
            "2025-03-24 20:17:39,028 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6409\n",
            "2025-03-24 20:17:46,600 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5672\n",
            "2025-03-24 20:17:54,293 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5551\n",
            "2025-03-24 20:18:01,808 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5058\n",
            "2025-03-24 20:18:07,598 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4657\n",
            "2025-03-24 20:18:08,130 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2228\n",
            "2025-03-24 20:18:08,131 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:19:03,664 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5852, Val Loss: 0.4058, Val Acc: 0.8446\n",
            "2025-03-24 20:19:03,979 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 20:19:03,979 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 20:19:09,992 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3854\n",
            "2025-03-24 20:19:17,352 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5097\n",
            "2025-03-24 20:19:24,840 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5638\n",
            "2025-03-24 20:19:32,588 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5926\n",
            "2025-03-24 20:19:40,185 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5209\n",
            "2025-03-24 20:19:47,795 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5183\n",
            "2025-03-24 20:19:55,226 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5792\n",
            "2025-03-24 20:20:02,981 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5275\n",
            "2025-03-24 20:20:10,578 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5240\n",
            "2025-03-24 20:20:18,081 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5595\n",
            "2025-03-24 20:20:25,664 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5435\n",
            "2025-03-24 20:20:33,150 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4940\n",
            "2025-03-24 20:20:41,153 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5481\n",
            "2025-03-24 20:20:48,462 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5207\n",
            "2025-03-24 20:20:55,774 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4815\n",
            "2025-03-24 20:21:03,463 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5062\n",
            "2025-03-24 20:21:10,941 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5970\n",
            "2025-03-24 20:21:18,551 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5202\n",
            "2025-03-24 20:21:26,178 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5013\n",
            "2025-03-24 20:21:33,717 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5474\n",
            "2025-03-24 20:21:41,081 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4987\n",
            "2025-03-24 20:21:48,756 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5961\n",
            "2025-03-24 20:21:55,996 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5437\n",
            "2025-03-24 20:22:03,511 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6128\n",
            "2025-03-24 20:22:10,994 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6071\n",
            "2025-03-24 20:22:18,371 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5536\n",
            "2025-03-24 20:22:26,072 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5858\n",
            "2025-03-24 20:22:33,732 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5103\n",
            "2025-03-24 20:22:41,062 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5287\n",
            "2025-03-24 20:22:48,715 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5496\n",
            "2025-03-24 20:22:56,083 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4913\n",
            "2025-03-24 20:23:03,655 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5523\n",
            "2025-03-24 20:23:10,927 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5074\n",
            "2025-03-24 20:23:16,736 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4070\n",
            "2025-03-24 20:23:17,284 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0868\n",
            "2025-03-24 20:23:17,285 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:24:13,106 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5384, Val Loss: 0.3843, Val Acc: 0.8599\n",
            "2025-03-24 20:24:13,457 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 20:24:13,458 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 20:24:19,298 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3761\n",
            "2025-03-24 20:24:26,688 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4734\n",
            "2025-03-24 20:24:34,322 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4262\n",
            "2025-03-24 20:24:42,083 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4836\n",
            "2025-03-24 20:24:49,691 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5247\n",
            "2025-03-24 20:24:57,122 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5111\n",
            "2025-03-24 20:25:04,816 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5844\n",
            "2025-03-24 20:25:12,246 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5279\n",
            "2025-03-24 20:25:19,882 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4796\n",
            "2025-03-24 20:25:27,469 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4942\n",
            "2025-03-24 20:25:35,077 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4774\n",
            "2025-03-24 20:25:42,667 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4621\n",
            "2025-03-24 20:25:50,116 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5223\n",
            "2025-03-24 20:25:57,794 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4844\n",
            "2025-03-24 20:26:05,465 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4587\n",
            "2025-03-24 20:26:13,068 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4423\n",
            "2025-03-24 20:26:20,562 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4970\n",
            "2025-03-24 20:26:28,246 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5013\n",
            "2025-03-24 20:26:35,861 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4542\n",
            "2025-03-24 20:26:43,342 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4714\n",
            "2025-03-24 20:26:51,024 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4520\n",
            "2025-03-24 20:26:58,667 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4548\n",
            "2025-03-24 20:27:06,162 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5245\n",
            "2025-03-24 20:27:13,760 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5630\n",
            "2025-03-24 20:27:21,368 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5112\n",
            "2025-03-24 20:27:28,793 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5305\n",
            "2025-03-24 20:27:36,334 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4906\n",
            "2025-03-24 20:27:43,837 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4996\n",
            "2025-03-24 20:27:51,429 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4755\n",
            "2025-03-24 20:27:59,236 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4912\n",
            "2025-03-24 20:28:06,864 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5424\n",
            "2025-03-24 20:28:14,341 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4720\n",
            "2025-03-24 20:28:21,958 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4683\n",
            "2025-03-24 20:28:27,805 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3876\n",
            "2025-03-24 20:28:28,364 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1049\n",
            "2025-03-24 20:28:28,365 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:29:24,930 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4925, Val Loss: 0.3563, Val Acc: 0.8674\n",
            "2025-03-24 20:29:25,247 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-24 20:29:25,247 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 20:29:31,390 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3658\n",
            "2025-03-24 20:29:38,733 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4799\n",
            "2025-03-24 20:29:46,175 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4134\n",
            "2025-03-24 20:29:53,552 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4300\n",
            "2025-03-24 20:30:01,151 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4905\n",
            "2025-03-24 20:30:08,507 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4498\n",
            "2025-03-24 20:30:15,914 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4328\n",
            "2025-03-24 20:30:23,398 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4696\n",
            "2025-03-24 20:30:30,691 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4906\n",
            "2025-03-24 20:30:38,311 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4506\n",
            "2025-03-24 20:30:45,719 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3819\n",
            "2025-03-24 20:30:53,193 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4938\n",
            "2025-03-24 20:31:00,962 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4346\n",
            "2025-03-24 20:31:08,693 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4700\n",
            "2025-03-24 20:31:16,187 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4688\n",
            "2025-03-24 20:31:23,879 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4882\n",
            "2025-03-24 20:31:31,594 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4793\n",
            "2025-03-24 20:31:39,107 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4429\n",
            "2025-03-24 20:31:46,321 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4247\n",
            "2025-03-24 20:31:53,695 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4659\n",
            "2025-03-24 20:32:01,068 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5124\n",
            "2025-03-24 20:32:08,555 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4236\n",
            "2025-03-24 20:32:15,894 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4840\n",
            "2025-03-24 20:32:23,350 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4489\n",
            "2025-03-24 20:32:30,743 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5275\n",
            "2025-03-24 20:32:38,024 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4268\n",
            "2025-03-24 20:32:45,561 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4187\n",
            "2025-03-24 20:32:53,022 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4885\n",
            "2025-03-24 20:33:00,414 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4074\n",
            "2025-03-24 20:33:07,725 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5645\n",
            "2025-03-24 20:33:14,902 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4683\n",
            "2025-03-24 20:33:22,347 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4262\n",
            "2025-03-24 20:33:29,748 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4782\n",
            "2025-03-24 20:33:35,252 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3693\n",
            "2025-03-24 20:33:35,813 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1076\n",
            "2025-03-24 20:33:35,814 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:34:30,388 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4615, Val Loss: 0.3601, Val Acc: 0.8655\n",
            "2025-03-24 20:34:30,389 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 20:34:35,903 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3576\n",
            "2025-03-24 20:34:43,112 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4272\n",
            "2025-03-24 20:34:50,452 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4035\n",
            "2025-03-24 20:34:58,097 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4696\n",
            "2025-03-24 20:35:05,282 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3543\n",
            "2025-03-24 20:35:12,710 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4220\n",
            "2025-03-24 20:35:20,282 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4377\n",
            "2025-03-24 20:35:27,598 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4103\n",
            "2025-03-24 20:35:34,841 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5574\n",
            "2025-03-24 20:35:42,468 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4628\n",
            "2025-03-24 20:35:49,838 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4225\n",
            "2025-03-24 20:35:57,482 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4453\n",
            "2025-03-24 20:36:04,840 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4600\n",
            "2025-03-24 20:36:12,629 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4468\n",
            "2025-03-24 20:36:19,965 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4370\n",
            "2025-03-24 20:36:27,179 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4410\n",
            "2025-03-24 20:36:34,668 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4223\n",
            "2025-03-24 20:36:41,848 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5327\n",
            "2025-03-24 20:36:49,263 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4430\n",
            "2025-03-24 20:36:56,616 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4729\n",
            "2025-03-24 20:37:04,089 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3860\n",
            "2025-03-24 20:37:11,364 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5107\n",
            "2025-03-24 20:37:18,741 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4511\n",
            "2025-03-24 20:37:25,985 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5007\n",
            "2025-03-24 20:37:33,468 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4545\n",
            "2025-03-24 20:37:40,812 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4561\n",
            "2025-03-24 20:37:48,179 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3817\n",
            "2025-03-24 20:37:55,482 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4251\n",
            "2025-03-24 20:38:02,849 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4121\n",
            "2025-03-24 20:38:10,398 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4790\n",
            "2025-03-24 20:38:17,787 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4530\n",
            "2025-03-24 20:38:24,978 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4356\n",
            "2025-03-24 20:38:32,364 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4580\n",
            "2025-03-24 20:38:38,053 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3232\n",
            "2025-03-24 20:38:38,643 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1206\n",
            "2025-03-24 20:38:38,644 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:39:33,193 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4466, Val Loss: 0.3673, Val Acc: 0.8692\n",
            "2025-03-24 20:39:33,193 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 20:39:38,883 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3353\n",
            "2025-03-24 20:39:46,140 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4015\n",
            "2025-03-24 20:39:53,466 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3583\n",
            "2025-03-24 20:40:00,991 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4445\n",
            "2025-03-24 20:40:08,411 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4443\n",
            "2025-03-24 20:40:15,692 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3946\n",
            "2025-03-24 20:40:23,077 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3679\n",
            "2025-03-24 20:40:30,394 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3964\n",
            "2025-03-24 20:40:37,804 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3985\n",
            "2025-03-24 20:40:45,279 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4107\n",
            "2025-03-24 20:40:52,644 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4042\n",
            "2025-03-24 20:40:59,757 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3870\n",
            "2025-03-24 20:41:07,185 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3735\n",
            "2025-03-24 20:41:14,501 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4303\n",
            "2025-03-24 20:41:21,965 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4073\n",
            "2025-03-24 20:41:29,132 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4102\n",
            "2025-03-24 20:41:36,242 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4787\n",
            "2025-03-24 20:41:43,770 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3972\n",
            "2025-03-24 20:41:51,066 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4434\n",
            "2025-03-24 20:41:58,394 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3953\n",
            "2025-03-24 20:42:05,972 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4427\n",
            "2025-03-24 20:42:13,385 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4638\n",
            "2025-03-24 20:42:20,680 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5203\n",
            "2025-03-24 20:42:27,952 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4433\n",
            "2025-03-24 20:42:35,554 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3574\n",
            "2025-03-24 20:42:42,838 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4607\n",
            "2025-03-24 20:42:50,139 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3435\n",
            "2025-03-24 20:42:57,554 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3791\n",
            "2025-03-24 20:43:04,690 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3779\n",
            "2025-03-24 20:43:12,112 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4109\n",
            "2025-03-24 20:43:19,469 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4444\n",
            "2025-03-24 20:43:26,991 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4337\n",
            "2025-03-24 20:43:34,513 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3688\n",
            "2025-03-24 20:43:40,007 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3266\n",
            "2025-03-24 20:43:40,559 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1650\n",
            "2025-03-24 20:43:40,560 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:44:34,994 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4153, Val Loss: 0.3624, Val Acc: 0.8739\n",
            "2025-03-24 20:44:34,995 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 20:44:40,684 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2999\n",
            "2025-03-24 20:44:48,122 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3847\n",
            "2025-03-24 20:44:55,337 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3703\n",
            "2025-03-24 20:45:02,715 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3989\n",
            "2025-03-24 20:45:10,059 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3476\n",
            "2025-03-24 20:45:17,532 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3721\n",
            "2025-03-24 20:45:24,943 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4051\n",
            "2025-03-24 20:45:32,229 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3716\n",
            "2025-03-24 20:45:39,527 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3924\n",
            "2025-03-24 20:45:46,922 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3425\n",
            "2025-03-24 20:45:54,316 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3444\n",
            "2025-03-24 20:46:01,652 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3601\n",
            "2025-03-24 20:46:08,986 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4302\n",
            "2025-03-24 20:46:16,491 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3882\n",
            "2025-03-24 20:46:23,764 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3125\n",
            "2025-03-24 20:46:31,473 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3977\n",
            "2025-03-24 20:46:38,722 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3815\n",
            "2025-03-24 20:46:45,989 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4005\n",
            "2025-03-24 20:46:53,502 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3445\n",
            "2025-03-24 20:47:00,880 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3703\n",
            "2025-03-24 20:47:08,242 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3695\n",
            "2025-03-24 20:47:15,872 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4247\n",
            "2025-03-24 20:47:23,077 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3793\n",
            "2025-03-24 20:47:30,382 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4431\n",
            "2025-03-24 20:47:37,827 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3606\n",
            "2025-03-24 20:47:45,117 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4173\n",
            "2025-03-24 20:47:52,336 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3929\n",
            "2025-03-24 20:47:59,563 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3867\n",
            "2025-03-24 20:48:06,875 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3815\n",
            "2025-03-24 20:48:14,654 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4225\n",
            "2025-03-24 20:48:22,030 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4590\n",
            "2025-03-24 20:48:29,470 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4038\n",
            "2025-03-24 20:48:36,867 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3947\n",
            "2025-03-24 20:48:42,408 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3447\n",
            "2025-03-24 20:48:42,932 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0687\n",
            "2025-03-24 20:48:42,932 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:49:37,698 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3871, Val Loss: 0.3823, Val Acc: 0.8692\n",
            "2025-03-24 20:49:37,699 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 20:49:43,225 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-24 20:49:50,432 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3369\n",
            "2025-03-24 20:49:57,791 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3932\n",
            "2025-03-24 20:50:05,069 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4080\n",
            "2025-03-24 20:50:12,318 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3782\n",
            "2025-03-24 20:50:19,491 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3891\n",
            "2025-03-24 20:50:27,114 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3350\n",
            "2025-03-24 20:50:34,470 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3388\n",
            "2025-03-24 20:50:41,697 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4127\n",
            "2025-03-24 20:50:49,196 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3671\n",
            "2025-03-24 20:50:56,410 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3409\n",
            "2025-03-24 20:51:03,817 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3415\n",
            "2025-03-24 20:51:11,399 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3445\n",
            "2025-03-24 20:51:18,599 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3479\n",
            "2025-03-24 20:51:25,922 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3582\n",
            "2025-03-24 20:51:33,286 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3174\n",
            "2025-03-24 20:51:40,782 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3617\n",
            "2025-03-24 20:51:48,161 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3209\n",
            "2025-03-24 20:51:55,589 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3657\n",
            "2025-03-24 20:52:02,886 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3096\n",
            "2025-03-24 20:52:10,205 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3076\n",
            "2025-03-24 20:52:17,557 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3390\n",
            "2025-03-24 20:52:24,985 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3382\n",
            "2025-03-24 20:52:32,394 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3528\n",
            "2025-03-24 20:52:39,790 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3490\n",
            "2025-03-24 20:52:47,187 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-24 20:52:54,708 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3834\n",
            "2025-03-24 20:53:02,093 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3312\n",
            "2025-03-24 20:53:09,392 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3308\n",
            "2025-03-24 20:53:16,760 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3290\n",
            "2025-03-24 20:53:24,082 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3509\n",
            "2025-03-24 20:53:31,293 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3099\n",
            "2025-03-24 20:53:38,695 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4029\n",
            "2025-03-24 20:53:44,142 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2395\n",
            "2025-03-24 20:53:44,693 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1114\n",
            "2025-03-24 20:53:44,694 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:54:39,414 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3508, Val Loss: 0.3687, Val Acc: 0.8762\n",
            "2025-03-24 20:54:39,415 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 20:54:44,998 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2527\n",
            "2025-03-24 20:54:52,193 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-24 20:54:59,610 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3169\n",
            "2025-03-24 20:55:06,963 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3217\n",
            "2025-03-24 20:55:14,516 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3157\n",
            "2025-03-24 20:55:21,921 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3031\n",
            "2025-03-24 20:55:29,102 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3413\n",
            "2025-03-24 20:55:36,518 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3147\n",
            "2025-03-24 20:55:43,917 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3382\n",
            "2025-03-24 20:55:51,056 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3227\n",
            "2025-03-24 20:55:58,506 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3033\n",
            "2025-03-24 20:56:05,673 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3521\n",
            "2025-03-24 20:56:13,295 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3142\n",
            "2025-03-24 20:56:20,477 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3378\n",
            "2025-03-24 20:56:27,900 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3432\n",
            "2025-03-24 20:56:35,252 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3193\n",
            "2025-03-24 20:56:42,715 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2937\n",
            "2025-03-24 20:56:49,943 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3249\n",
            "2025-03-24 20:56:57,490 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2887\n",
            "2025-03-24 20:57:04,938 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3422\n",
            "2025-03-24 20:57:12,234 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3177\n",
            "2025-03-24 20:57:19,667 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-24 20:57:26,981 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3243\n",
            "2025-03-24 20:57:34,235 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3129\n",
            "2025-03-24 20:57:41,452 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3328\n",
            "2025-03-24 20:57:48,880 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3088\n",
            "2025-03-24 20:57:56,263 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3174\n",
            "2025-03-24 20:58:03,690 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2794\n",
            "2025-03-24 20:58:11,012 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3470\n",
            "2025-03-24 20:58:18,363 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3286\n",
            "2025-03-24 20:58:25,839 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3087\n",
            "2025-03-24 20:58:33,212 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3658\n",
            "2025-03-24 20:58:40,524 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3378\n",
            "2025-03-24 20:58:46,250 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2334\n",
            "2025-03-24 20:58:46,772 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1290\n",
            "2025-03-24 20:58:46,773 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 20:59:41,994 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3221, Val Loss: 0.3705, Val Acc: 0.8734\n",
            "2025-03-24 20:59:41,995 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 20:59:47,556 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2420\n",
            "2025-03-24 20:59:55,028 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3229\n",
            "2025-03-24 21:00:02,427 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3148\n",
            "2025-03-24 21:00:09,931 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3329\n",
            "2025-03-24 21:00:17,423 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3188\n",
            "2025-03-24 21:00:24,720 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3010\n",
            "2025-03-24 21:00:32,127 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2872\n",
            "2025-03-24 21:00:39,632 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3365\n",
            "2025-03-24 21:00:46,832 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-24 21:00:54,242 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3146\n",
            "2025-03-24 21:01:01,818 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2849\n",
            "2025-03-24 21:01:09,214 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-24 21:01:16,372 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-24 21:01:23,810 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-24 21:01:31,187 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3162\n",
            "2025-03-24 21:01:38,609 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3423\n",
            "2025-03-24 21:01:46,209 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-24 21:01:53,495 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3086\n",
            "2025-03-24 21:02:00,799 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3366\n",
            "2025-03-24 21:02:07,959 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2946\n",
            "2025-03-24 21:02:15,412 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2906\n",
            "2025-03-24 21:02:22,991 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3019\n",
            "2025-03-24 21:02:30,158 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3315\n",
            "2025-03-24 21:02:37,779 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3489\n",
            "2025-03-24 21:02:45,150 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3533\n",
            "2025-03-24 21:02:52,562 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3077\n",
            "2025-03-24 21:02:59,974 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3670\n",
            "2025-03-24 21:03:07,385 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3663\n",
            "2025-03-24 21:03:14,546 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3371\n",
            "2025-03-24 21:03:21,957 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3291\n",
            "2025-03-24 21:03:29,246 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-24 21:03:36,497 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3101\n",
            "2025-03-24 21:03:43,790 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3403\n",
            "2025-03-24 21:03:49,261 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2323\n",
            "2025-03-24 21:03:49,787 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0763\n",
            "2025-03-24 21:03:49,788 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:04:43,799 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3165, Val Loss: 0.3721, Val Acc: 0.8734\n",
            "2025-03-24 21:04:43,800 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 21:04:49,183 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2265\n",
            "2025-03-24 21:04:56,547 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3175\n",
            "2025-03-24 21:05:03,689 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3174\n",
            "2025-03-24 21:05:11,249 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-24 21:05:18,477 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3162\n",
            "2025-03-24 21:05:25,883 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2843\n",
            "2025-03-24 21:05:33,161 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2868\n",
            "2025-03-24 21:05:40,550 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3084\n",
            "2025-03-24 21:05:47,919 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2989\n",
            "2025-03-24 21:05:55,128 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2905\n",
            "2025-03-24 21:06:02,525 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2866\n",
            "2025-03-24 21:06:09,677 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-24 21:06:17,121 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3132\n",
            "2025-03-24 21:06:24,509 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2965\n",
            "2025-03-24 21:06:31,714 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3201\n",
            "2025-03-24 21:06:39,071 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 21:06:46,509 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3124\n",
            "2025-03-24 21:06:53,907 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2894\n",
            "2025-03-24 21:07:01,133 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3015\n",
            "2025-03-24 21:07:08,505 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2863\n",
            "2025-03-24 21:07:15,873 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2977\n",
            "2025-03-24 21:07:23,020 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2954\n",
            "2025-03-24 21:07:30,490 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2986\n",
            "2025-03-24 21:07:37,651 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-24 21:07:45,096 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3450\n",
            "2025-03-24 21:07:52,261 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-24 21:07:59,816 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3054\n",
            "2025-03-24 21:08:07,284 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2773\n",
            "2025-03-24 21:08:14,900 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3034\n",
            "2025-03-24 21:08:22,685 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3168\n",
            "2025-03-24 21:08:30,493 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3172\n",
            "2025-03-24 21:08:38,023 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-24 21:08:45,509 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3042\n",
            "2025-03-24 21:08:51,496 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2000\n",
            "2025-03-24 21:08:52,130 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1660\n",
            "2025-03-24 21:08:52,131 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:09:49,435 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3027, Val Loss: 0.3656, Val Acc: 0.8758\n",
            "2025-03-24 21:09:49,435 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 21:09:55,256 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2267\n",
            "2025-03-24 21:10:02,635 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2697\n",
            "2025-03-24 21:10:10,134 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2885\n",
            "2025-03-24 21:10:17,615 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2837\n",
            "2025-03-24 21:10:25,271 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3132\n",
            "2025-03-24 21:10:33,102 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2968\n",
            "2025-03-24 21:10:40,934 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3046\n",
            "2025-03-24 21:10:49,060 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3065\n",
            "2025-03-24 21:10:57,484 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3228\n",
            "2025-03-24 21:11:05,915 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2657\n",
            "2025-03-24 21:11:14,445 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-24 21:11:23,223 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2669\n",
            "2025-03-24 21:11:31,520 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-24 21:11:40,017 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-24 21:11:48,625 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2912\n",
            "2025-03-24 21:11:57,023 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3008\n",
            "2025-03-24 21:12:05,229 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-24 21:12:13,615 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2818\n",
            "2025-03-24 21:12:22,016 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2527\n",
            "2025-03-24 21:12:30,395 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2864\n",
            "2025-03-24 21:12:38,999 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-24 21:12:47,481 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2631\n",
            "2025-03-24 21:12:56,018 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3052\n",
            "2025-03-24 21:13:04,497 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2789\n",
            "2025-03-24 21:13:12,960 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2697\n",
            "2025-03-24 21:13:21,501 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2563\n",
            "2025-03-24 21:13:30,197 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3129\n",
            "2025-03-24 21:13:38,793 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2953\n",
            "2025-03-24 21:13:47,191 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2816\n",
            "2025-03-24 21:13:55,458 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2721\n",
            "2025-03-24 21:14:03,887 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3064\n",
            "2025-03-24 21:14:12,354 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2837\n",
            "2025-03-24 21:14:20,837 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2711\n",
            "2025-03-24 21:14:27,180 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2208\n",
            "2025-03-24 21:14:27,796 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0945\n",
            "2025-03-24 21:14:27,797 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:15:28,059 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2864, Val Loss: 0.3763, Val Acc: 0.8706\n",
            "2025-03-24 21:15:28,060 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 21:15:34,462 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2226\n",
            "2025-03-24 21:15:42,834 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3049\n",
            "2025-03-24 21:15:51,362 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2784\n",
            "2025-03-24 21:15:59,318 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2720\n",
            "2025-03-24 21:16:07,976 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2697\n",
            "2025-03-24 21:16:16,370 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2913\n",
            "2025-03-24 21:16:24,776 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-24 21:16:33,338 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2576\n",
            "2025-03-24 21:16:41,531 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3014\n",
            "2025-03-24 21:16:49,958 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2937\n",
            "2025-03-24 21:16:58,730 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2713\n",
            "2025-03-24 21:17:07,230 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2590\n",
            "2025-03-24 21:17:15,925 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2719\n",
            "2025-03-24 21:17:24,456 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2541\n",
            "2025-03-24 21:17:32,977 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2611\n",
            "2025-03-24 21:17:41,484 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2793\n",
            "2025-03-24 21:17:49,945 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2833\n",
            "2025-03-24 21:17:58,321 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2846\n",
            "2025-03-24 21:18:06,610 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2828\n",
            "2025-03-24 21:18:14,904 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2600\n",
            "2025-03-24 21:18:23,065 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3228\n",
            "2025-03-24 21:18:31,306 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3014\n",
            "2025-03-24 21:18:39,700 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2698\n",
            "2025-03-24 21:18:48,097 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-24 21:18:56,493 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2728\n",
            "2025-03-24 21:19:04,615 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2649\n",
            "2025-03-24 21:19:12,974 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2800\n",
            "2025-03-24 21:19:21,104 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2927\n",
            "2025-03-24 21:19:29,312 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2808\n",
            "2025-03-24 21:19:37,640 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-24 21:19:45,818 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2954\n",
            "2025-03-24 21:19:54,128 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3152\n",
            "2025-03-24 21:20:02,631 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-24 21:20:09,147 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2181\n",
            "2025-03-24 21:20:09,786 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1182\n",
            "2025-03-24 21:20:09,787 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:21:11,806 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2814, Val Loss: 0.3710, Val Acc: 0.8809\n",
            "2025-03-24 21:21:11,806 - INFO - [TRAIN INFO] Early stopping at epoch 16 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 21:21:11,807 - INFO - [TRAIN INFO] Total Time: 5007.08s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▇▆▅▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇▇█████████</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.28144</td></tr><tr><td>train_val_loss_diff</td><td>-0.08951</td></tr><tr><td>val_accuracy</td><td>0.88087</td></tr><tr><td>val_loss</td><td>0.37095</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_gated_fusion_fold_1</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wft1mavq' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/wft1mavq</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_195743-wft1mavq\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 21:21:14,058 - INFO - [TRAIN INFO] Fold 1 Training Complete at epoch 16. Total Time: 5009.33s\n",
            "2025-03-24 21:21:14,082 - INFO - [K-FOLD INFO] Fold 1 completed in 5012.52 seconds\n",
            "2025-03-24 21:21:14,083 - INFO - [K-FOLD INFO] ============================== Fold 2/5 ==============================\n",
            "2025-03-24 21:21:14,088 - INFO - [K-FOLD INFO] Fold 2:\n",
            "2025-03-24 21:21:14,089 - INFO -    Train Samples: 8594\n",
            "2025-03-24 21:21:14,089 - INFO -    Validation Samples: 2149\n",
            "2025-03-24 21:21:14,089 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 2\n",
            "2025-03-24 21:21:14,092 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 2:\n",
            "2025-03-24 21:21:14,093 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 21:21:14,957 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 2\n",
            "2025-03-24 21:21:14,960 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 2:\n",
            "2025-03-24 21:21:14,960 - INFO - [K-FOLD INFO] Loss function initialized for Fold 2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_gated\\wandb\\run-20250324_212114-k0q01ebw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/k0q01ebw' target=\"_blank\">experiment_multimodal_attention_gated_fusion_fold_2</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/k0q01ebw' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/k0q01ebw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 21:21:17,724 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 21:21:17,724 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 21:21:24,338 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0410\n",
            "2025-03-24 21:21:32,999 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.3934\n",
            "2025-03-24 21:21:41,715 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3195\n",
            "2025-03-24 21:21:50,223 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.2738\n",
            "2025-03-24 21:21:58,648 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3152\n",
            "2025-03-24 21:22:07,436 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2632\n",
            "2025-03-24 21:22:16,050 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.1863\n",
            "2025-03-24 21:22:24,743 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2674\n",
            "2025-03-24 21:22:33,265 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2252\n",
            "2025-03-24 21:22:41,866 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2209\n",
            "2025-03-24 21:22:50,341 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2019\n",
            "2025-03-24 21:22:58,911 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1664\n",
            "2025-03-24 21:23:07,437 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1210\n",
            "2025-03-24 21:23:15,856 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1643\n",
            "2025-03-24 21:23:24,569 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1137\n",
            "2025-03-24 21:23:33,215 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.0513\n",
            "2025-03-24 21:23:41,686 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1129\n",
            "2025-03-24 21:23:50,257 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0467\n",
            "2025-03-24 21:23:59,207 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0550\n",
            "2025-03-24 21:24:08,005 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0458\n",
            "2025-03-24 21:24:16,473 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0508\n",
            "2025-03-24 21:24:25,387 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.9993\n",
            "2025-03-24 21:24:33,898 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0134\n",
            "2025-03-24 21:24:42,565 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0240\n",
            "2025-03-24 21:24:51,326 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 1.0229\n",
            "2025-03-24 21:25:00,037 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9985\n",
            "2025-03-24 21:25:08,978 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9734\n",
            "2025-03-24 21:25:17,487 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9729\n",
            "2025-03-24 21:25:26,281 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9087\n",
            "2025-03-24 21:25:34,857 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9046\n",
            "2025-03-24 21:25:43,332 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9952\n",
            "2025-03-24 21:25:52,154 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8852\n",
            "2025-03-24 21:26:00,834 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9054\n",
            "2025-03-24 21:26:07,377 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7113\n",
            "2025-03-24 21:26:08,044 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2286\n",
            "2025-03-24 21:26:08,045 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:27:09,160 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1016, Val Loss: 0.7869, Val Acc: 0.6915\n",
            "2025-03-24 21:27:09,512 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 21:27:09,512 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 21:27:16,128 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6965\n",
            "2025-03-24 21:27:24,925 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9135\n",
            "2025-03-24 21:27:33,692 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8661\n",
            "2025-03-24 21:27:42,307 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8383\n",
            "2025-03-24 21:27:51,147 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8845\n",
            "2025-03-24 21:27:59,825 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7742\n",
            "2025-03-24 21:28:08,514 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8681\n",
            "2025-03-24 21:28:17,234 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8024\n",
            "2025-03-24 21:28:25,913 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.9503\n",
            "2025-03-24 21:28:34,519 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7947\n",
            "2025-03-24 21:28:43,227 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.9629\n",
            "2025-03-24 21:28:51,874 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8341\n",
            "2025-03-24 21:29:00,714 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8469\n",
            "2025-03-24 21:29:09,449 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7739\n",
            "2025-03-24 21:29:18,085 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7940\n",
            "2025-03-24 21:29:26,348 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8010\n",
            "2025-03-24 21:29:34,559 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8329\n",
            "2025-03-24 21:29:43,127 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7346\n",
            "2025-03-24 21:29:51,532 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.8293\n",
            "2025-03-24 21:29:59,984 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8099\n",
            "2025-03-24 21:30:08,214 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7430\n",
            "2025-03-24 21:30:15,951 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7934\n",
            "2025-03-24 21:30:23,645 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7513\n",
            "2025-03-24 21:30:31,305 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8348\n",
            "2025-03-24 21:30:38,707 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7678\n",
            "2025-03-24 21:30:46,620 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.8028\n",
            "2025-03-24 21:30:54,223 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7640\n",
            "2025-03-24 21:31:02,029 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7877\n",
            "2025-03-24 21:31:10,424 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.8174\n",
            "2025-03-24 21:31:18,191 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7604\n",
            "2025-03-24 21:31:25,983 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7152\n",
            "2025-03-24 21:31:33,593 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8320\n",
            "2025-03-24 21:31:41,213 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6941\n",
            "2025-03-24 21:31:47,173 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5438\n",
            "2025-03-24 21:31:47,788 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1390\n",
            "2025-03-24 21:31:47,788 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:32:44,869 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8105, Val Loss: 0.5927, Val Acc: 0.7655\n",
            "2025-03-24 21:32:45,171 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 21:32:45,173 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 21:32:51,027 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5138\n",
            "2025-03-24 21:32:58,830 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6901\n",
            "2025-03-24 21:33:06,426 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6998\n",
            "2025-03-24 21:33:14,104 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6318\n",
            "2025-03-24 21:33:21,820 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7945\n",
            "2025-03-24 21:33:29,549 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7408\n",
            "2025-03-24 21:33:37,205 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6422\n",
            "2025-03-24 21:33:45,205 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7832\n",
            "2025-03-24 21:33:52,948 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7072\n",
            "2025-03-24 21:34:00,564 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7135\n",
            "2025-03-24 21:34:08,407 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6198\n",
            "2025-03-24 21:34:16,205 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7239\n",
            "2025-03-24 21:34:23,992 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6994\n",
            "2025-03-24 21:34:31,546 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6241\n",
            "2025-03-24 21:34:39,402 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6779\n",
            "2025-03-24 21:34:47,594 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7304\n",
            "2025-03-24 21:34:55,626 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6594\n",
            "2025-03-24 21:35:03,792 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7158\n",
            "2025-03-24 21:35:11,342 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6917\n",
            "2025-03-24 21:35:18,749 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7034\n",
            "2025-03-24 21:35:26,413 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6719\n",
            "2025-03-24 21:35:34,125 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6277\n",
            "2025-03-24 21:35:41,987 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6192\n",
            "2025-03-24 21:35:49,569 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6204\n",
            "2025-03-24 21:35:57,229 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6836\n",
            "2025-03-24 21:36:04,979 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6425\n",
            "2025-03-24 21:36:12,434 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6149\n",
            "2025-03-24 21:36:20,106 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6053\n",
            "2025-03-24 21:36:27,798 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6709\n",
            "2025-03-24 21:36:35,462 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7204\n",
            "2025-03-24 21:36:43,183 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.6920\n",
            "2025-03-24 21:36:50,967 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6022\n",
            "2025-03-24 21:36:59,248 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6519\n",
            "2025-03-24 21:37:05,357 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4467\n",
            "2025-03-24 21:37:05,972 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1368\n",
            "2025-03-24 21:37:05,973 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:38:08,930 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6746, Val Loss: 0.4929, Val Acc: 0.8181\n",
            "2025-03-24 21:38:09,278 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 21:38:09,278 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 21:38:15,736 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3757\n",
            "2025-03-24 21:38:24,247 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5855\n",
            "2025-03-24 21:38:32,724 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6255\n",
            "2025-03-24 21:38:41,124 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5410\n",
            "2025-03-24 21:38:49,529 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6376\n",
            "2025-03-24 21:38:58,130 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6834\n",
            "2025-03-24 21:39:06,908 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5920\n",
            "2025-03-24 21:39:15,676 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5631\n",
            "2025-03-24 21:39:24,316 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6211\n",
            "2025-03-24 21:39:32,880 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5479\n",
            "2025-03-24 21:39:41,288 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5946\n",
            "2025-03-24 21:39:50,082 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5746\n",
            "2025-03-24 21:39:58,523 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5605\n",
            "2025-03-24 21:40:06,907 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6331\n",
            "2025-03-24 21:40:15,296 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6348\n",
            "2025-03-24 21:40:23,696 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6182\n",
            "2025-03-24 21:40:32,267 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6562\n",
            "2025-03-24 21:40:40,507 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5853\n",
            "2025-03-24 21:40:48,760 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5410\n",
            "2025-03-24 21:40:57,483 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6760\n",
            "2025-03-24 21:41:06,229 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5581\n",
            "2025-03-24 21:41:14,866 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5694\n",
            "2025-03-24 21:41:23,285 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6168\n",
            "2025-03-24 21:41:31,890 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5891\n",
            "2025-03-24 21:41:40,485 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5455\n",
            "2025-03-24 21:41:49,068 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5630\n",
            "2025-03-24 21:41:57,436 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5617\n",
            "2025-03-24 21:42:05,897 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5374\n",
            "2025-03-24 21:42:14,334 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6005\n",
            "2025-03-24 21:42:22,843 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6617\n",
            "2025-03-24 21:42:31,450 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5813\n",
            "2025-03-24 21:42:39,855 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5697\n",
            "2025-03-24 21:42:48,075 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5499\n",
            "2025-03-24 21:42:54,275 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4120\n",
            "2025-03-24 21:42:54,923 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1701\n",
            "2025-03-24 21:42:54,924 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:43:55,235 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5906, Val Loss: 0.4393, Val Acc: 0.8395\n",
            "2025-03-24 21:43:55,544 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 21:43:55,544 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 21:44:01,859 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3843\n",
            "2025-03-24 21:44:10,223 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5181\n",
            "2025-03-24 21:44:18,616 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5557\n",
            "2025-03-24 21:44:27,008 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4893\n",
            "2025-03-24 21:44:35,436 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5772\n",
            "2025-03-24 21:44:43,587 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5115\n",
            "2025-03-24 21:44:51,872 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5219\n",
            "2025-03-24 21:45:00,358 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6298\n",
            "2025-03-24 21:45:08,801 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5312\n",
            "2025-03-24 21:45:17,253 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5507\n",
            "2025-03-24 21:45:26,006 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5479\n",
            "2025-03-24 21:45:34,202 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4773\n",
            "2025-03-24 21:45:41,952 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5856\n",
            "2025-03-24 21:45:49,534 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4897\n",
            "2025-03-24 21:45:57,209 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5530\n",
            "2025-03-24 21:46:05,016 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5008\n",
            "2025-03-24 21:46:12,995 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5119\n",
            "2025-03-24 21:46:21,257 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5729\n",
            "2025-03-24 21:46:29,384 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4928\n",
            "2025-03-24 21:46:37,304 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4643\n",
            "2025-03-24 21:46:44,962 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5211\n",
            "2025-03-24 21:46:52,720 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4846\n",
            "2025-03-24 21:47:00,547 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5612\n",
            "2025-03-24 21:47:08,266 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5641\n",
            "2025-03-24 21:47:16,112 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5880\n",
            "2025-03-24 21:47:23,791 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5682\n",
            "2025-03-24 21:47:31,581 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5664\n",
            "2025-03-24 21:47:39,471 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5229\n",
            "2025-03-24 21:47:47,517 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4998\n",
            "2025-03-24 21:47:55,540 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5204\n",
            "2025-03-24 21:48:03,241 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5389\n",
            "2025-03-24 21:48:11,154 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5416\n",
            "2025-03-24 21:48:18,844 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4894\n",
            "2025-03-24 21:48:24,851 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4012\n",
            "2025-03-24 21:48:25,392 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1129\n",
            "2025-03-24 21:48:25,394 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:49:23,645 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5317, Val Loss: 0.4051, Val Acc: 0.8474\n",
            "2025-03-24 21:49:23,977 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 21:49:23,978 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 21:49:30,318 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3546\n",
            "2025-03-24 21:49:38,761 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5347\n",
            "2025-03-24 21:49:46,829 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5129\n",
            "2025-03-24 21:49:54,467 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4623\n",
            "2025-03-24 21:50:02,219 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4520\n",
            "2025-03-24 21:50:10,017 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4838\n",
            "2025-03-24 21:50:18,180 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5152\n",
            "2025-03-24 21:50:26,212 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5222\n",
            "2025-03-24 21:50:34,309 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5436\n",
            "2025-03-24 21:50:42,596 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4948\n",
            "2025-03-24 21:50:51,007 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5171\n",
            "2025-03-24 21:50:59,544 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4682\n",
            "2025-03-24 21:51:08,146 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5187\n",
            "2025-03-24 21:51:16,752 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4922\n",
            "2025-03-24 21:51:25,315 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5254\n",
            "2025-03-24 21:51:33,737 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5192\n",
            "2025-03-24 21:51:42,103 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5203\n",
            "2025-03-24 21:51:50,536 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5364\n",
            "2025-03-24 21:51:58,958 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5059\n",
            "2025-03-24 21:52:07,456 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4660\n",
            "2025-03-24 21:52:15,914 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4421\n",
            "2025-03-24 21:52:24,286 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4879\n",
            "2025-03-24 21:52:32,731 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4888\n",
            "2025-03-24 21:52:41,297 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4820\n",
            "2025-03-24 21:52:49,748 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4486\n",
            "2025-03-24 21:52:58,262 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4998\n",
            "2025-03-24 21:53:06,456 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5467\n",
            "2025-03-24 21:53:14,424 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4688\n",
            "2025-03-24 21:53:22,461 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5178\n",
            "2025-03-24 21:53:30,282 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4762\n",
            "2025-03-24 21:53:38,185 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5034\n",
            "2025-03-24 21:53:46,157 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4977\n",
            "2025-03-24 21:53:54,120 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5001\n",
            "2025-03-24 21:54:00,131 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3971\n",
            "2025-03-24 21:54:00,719 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1513\n",
            "2025-03-24 21:54:00,719 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 21:54:59,952 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4994, Val Loss: 0.3760, Val Acc: 0.8669\n",
            "2025-03-24 21:55:00,294 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 21:55:00,295 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 21:55:06,351 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3572\n",
            "2025-03-24 21:55:14,063 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4319\n",
            "2025-03-24 21:55:21,620 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4795\n",
            "2025-03-24 21:55:29,870 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4383\n",
            "2025-03-24 21:55:38,408 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4481\n",
            "2025-03-24 21:55:46,765 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4444\n",
            "2025-03-24 21:55:55,218 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4670\n",
            "2025-03-24 21:56:03,321 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4308\n",
            "2025-03-24 21:56:10,915 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4384\n",
            "2025-03-24 21:56:18,568 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4382\n",
            "2025-03-24 21:56:26,548 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4743\n",
            "2025-03-24 21:56:34,399 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3998\n",
            "2025-03-24 21:56:42,033 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3944\n",
            "2025-03-24 21:56:49,937 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5228\n",
            "2025-03-24 21:56:57,504 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5187\n",
            "2025-03-24 21:57:05,784 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4859\n",
            "2025-03-24 21:57:13,389 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4640\n",
            "2025-03-24 21:57:20,990 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4537\n",
            "2025-03-24 21:57:28,846 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4311\n",
            "2025-03-24 21:57:36,782 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4658\n",
            "2025-03-24 21:57:44,974 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5439\n",
            "2025-03-24 21:57:53,376 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5161\n",
            "2025-03-24 21:58:01,774 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4798\n",
            "2025-03-24 21:58:10,355 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4774\n",
            "2025-03-24 21:58:18,683 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4823\n",
            "2025-03-24 21:58:27,131 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4787\n",
            "2025-03-24 21:58:35,432 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4777\n",
            "2025-03-24 21:58:43,714 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4514\n",
            "2025-03-24 21:58:52,158 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5072\n",
            "2025-03-24 21:59:00,751 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4388\n",
            "2025-03-24 21:59:08,995 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4521\n",
            "2025-03-24 21:59:17,546 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5013\n",
            "2025-03-24 21:59:25,956 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5082\n",
            "2025-03-24 21:59:32,140 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3059\n",
            "2025-03-24 21:59:32,755 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0805\n",
            "2025-03-24 21:59:32,756 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:00:30,733 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4648, Val Loss: 0.3915, Val Acc: 0.8664\n",
            "2025-03-24 22:00:30,733 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 22:00:36,442 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2954\n",
            "2025-03-24 22:00:44,254 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4481\n",
            "2025-03-24 22:00:51,924 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4008\n",
            "2025-03-24 22:00:59,975 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4397\n",
            "2025-03-24 22:01:08,118 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3845\n",
            "2025-03-24 22:01:15,830 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4257\n",
            "2025-03-24 22:01:23,706 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4028\n",
            "2025-03-24 22:01:31,502 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4724\n",
            "2025-03-24 22:01:39,458 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4120\n",
            "2025-03-24 22:01:46,982 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4261\n",
            "2025-03-24 22:01:54,701 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4256\n",
            "2025-03-24 22:02:02,296 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4081\n",
            "2025-03-24 22:02:09,694 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4389\n",
            "2025-03-24 22:02:17,310 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4911\n",
            "2025-03-24 22:02:24,890 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4226\n",
            "2025-03-24 22:02:32,485 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3901\n",
            "2025-03-24 22:02:39,965 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4478\n",
            "2025-03-24 22:02:47,683 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3657\n",
            "2025-03-24 22:02:55,321 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4762\n",
            "2025-03-24 22:03:02,647 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4170\n",
            "2025-03-24 22:03:10,375 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4533\n",
            "2025-03-24 22:03:17,871 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4014\n",
            "2025-03-24 22:03:25,570 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4139\n",
            "2025-03-24 22:03:33,210 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5037\n",
            "2025-03-24 22:03:40,810 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4206\n",
            "2025-03-24 22:03:48,671 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4316\n",
            "2025-03-24 22:03:56,552 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4629\n",
            "2025-03-24 22:04:04,201 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4012\n",
            "2025-03-24 22:04:11,779 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5026\n",
            "2025-03-24 22:04:19,135 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4564\n",
            "2025-03-24 22:04:26,598 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4482\n",
            "2025-03-24 22:04:34,347 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4665\n",
            "2025-03-24 22:04:41,945 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4989\n",
            "2025-03-24 22:04:47,510 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3301\n",
            "2025-03-24 22:04:48,118 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1103\n",
            "2025-03-24 22:04:48,119 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:05:42,435 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4353, Val Loss: 0.4164, Val Acc: 0.8557\n",
            "2025-03-24 22:05:42,436 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 22:05:48,180 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3092\n",
            "2025-03-24 22:05:55,633 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4001\n",
            "2025-03-24 22:06:03,051 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4923\n",
            "2025-03-24 22:06:10,641 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3724\n",
            "2025-03-24 22:06:18,212 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4474\n",
            "2025-03-24 22:06:25,652 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3748\n",
            "2025-03-24 22:06:32,979 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3490\n",
            "2025-03-24 22:06:40,611 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4718\n",
            "2025-03-24 22:06:47,992 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4731\n",
            "2025-03-24 22:06:55,769 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3717\n",
            "2025-03-24 22:07:03,283 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3816\n",
            "2025-03-24 22:07:10,957 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4362\n",
            "2025-03-24 22:07:18,589 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5136\n",
            "2025-03-24 22:07:26,080 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3931\n",
            "2025-03-24 22:07:33,591 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4070\n",
            "2025-03-24 22:07:41,139 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4208\n",
            "2025-03-24 22:07:49,176 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4190\n",
            "2025-03-24 22:07:56,989 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4060\n",
            "2025-03-24 22:08:04,784 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4355\n",
            "2025-03-24 22:08:12,400 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3899\n",
            "2025-03-24 22:08:19,880 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4201\n",
            "2025-03-24 22:08:27,617 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4214\n",
            "2025-03-24 22:08:35,175 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4118\n",
            "2025-03-24 22:08:42,673 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4552\n",
            "2025-03-24 22:08:49,884 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4510\n",
            "2025-03-24 22:08:57,572 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3987\n",
            "2025-03-24 22:09:04,966 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3977\n",
            "2025-03-24 22:09:12,506 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5198\n",
            "2025-03-24 22:09:19,956 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4577\n",
            "2025-03-24 22:09:27,545 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4660\n",
            "2025-03-24 22:09:35,155 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4521\n",
            "2025-03-24 22:09:42,867 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4257\n",
            "2025-03-24 22:09:50,435 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3748\n",
            "2025-03-24 22:09:56,189 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3457\n",
            "2025-03-24 22:09:56,709 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1386\n",
            "2025-03-24 22:09:56,710 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:10:51,450 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4267, Val Loss: 0.3911, Val Acc: 0.8604\n",
            "2025-03-24 22:10:51,450 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 22:10:57,240 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2542\n",
            "2025-03-24 22:11:04,513 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3783\n",
            "2025-03-24 22:11:12,064 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4499\n",
            "2025-03-24 22:11:20,120 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3725\n",
            "2025-03-24 22:11:27,991 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4012\n",
            "2025-03-24 22:11:35,808 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3492\n",
            "2025-03-24 22:11:43,516 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4173\n",
            "2025-03-24 22:11:50,941 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4145\n",
            "2025-03-24 22:11:58,707 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3542\n",
            "2025-03-24 22:12:06,629 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3856\n",
            "2025-03-24 22:12:14,402 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3901\n",
            "2025-03-24 22:12:21,975 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4020\n",
            "2025-03-24 22:12:29,629 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4192\n",
            "2025-03-24 22:12:37,241 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3625\n",
            "2025-03-24 22:12:44,932 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4267\n",
            "2025-03-24 22:12:52,893 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3974\n",
            "2025-03-24 22:13:00,710 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3956\n",
            "2025-03-24 22:13:08,387 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3663\n",
            "2025-03-24 22:13:16,154 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4080\n",
            "2025-03-24 22:13:24,152 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3772\n",
            "2025-03-24 22:13:32,049 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3739\n",
            "2025-03-24 22:13:39,777 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3884\n",
            "2025-03-24 22:13:47,446 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3442\n",
            "2025-03-24 22:13:55,286 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3880\n",
            "2025-03-24 22:14:03,005 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4093\n",
            "2025-03-24 22:14:10,461 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4166\n",
            "2025-03-24 22:14:18,168 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3834\n",
            "2025-03-24 22:14:25,672 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3941\n",
            "2025-03-24 22:14:33,791 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4010\n",
            "2025-03-24 22:14:41,465 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3913\n",
            "2025-03-24 22:14:49,224 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3894\n",
            "2025-03-24 22:14:57,187 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3844\n",
            "2025-03-24 22:15:04,715 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3664\n",
            "2025-03-24 22:15:10,537 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2918\n",
            "2025-03-24 22:15:11,108 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0769\n",
            "2025-03-24 22:15:11,108 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:16:06,739 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3888, Val Loss: 0.3585, Val Acc: 0.8651\n",
            "2025-03-24 22:16:07,039 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-24 22:16:07,039 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 22:16:12,922 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2701\n",
            "2025-03-24 22:16:20,990 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3732\n",
            "2025-03-24 22:16:28,922 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3562\n",
            "2025-03-24 22:16:36,762 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3516\n",
            "2025-03-24 22:16:44,587 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3341\n",
            "2025-03-24 22:16:52,411 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3379\n",
            "2025-03-24 22:17:00,135 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3639\n",
            "2025-03-24 22:17:07,973 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3624\n",
            "2025-03-24 22:17:15,709 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3362\n",
            "2025-03-24 22:17:23,349 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3744\n",
            "2025-03-24 22:17:31,217 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3293\n",
            "2025-03-24 22:17:38,899 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3468\n",
            "2025-03-24 22:17:46,467 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3403\n",
            "2025-03-24 22:17:54,155 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3496\n",
            "2025-03-24 22:18:02,067 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3761\n",
            "2025-03-24 22:18:10,183 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3920\n",
            "2025-03-24 22:18:18,303 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3536\n",
            "2025-03-24 22:18:26,496 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3437\n",
            "2025-03-24 22:18:34,595 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3928\n",
            "2025-03-24 22:18:42,089 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3589\n",
            "2025-03-24 22:18:50,002 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3818\n",
            "2025-03-24 22:18:58,016 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4085\n",
            "2025-03-24 22:19:05,989 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3395\n",
            "2025-03-24 22:19:13,598 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4043\n",
            "2025-03-24 22:19:21,612 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3612\n",
            "2025-03-24 22:19:29,186 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3274\n",
            "2025-03-24 22:19:37,292 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3600\n",
            "2025-03-24 22:19:44,928 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3683\n",
            "2025-03-24 22:19:52,527 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3847\n",
            "2025-03-24 22:20:00,185 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3745\n",
            "2025-03-24 22:20:08,163 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3739\n",
            "2025-03-24 22:20:15,954 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4115\n",
            "2025-03-24 22:20:23,955 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3589\n",
            "2025-03-24 22:20:29,741 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2725\n",
            "2025-03-24 22:20:30,299 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1107\n",
            "2025-03-24 22:20:30,300 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:21:25,972 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3639, Val Loss: 0.3779, Val Acc: 0.8720\n",
            "2025-03-24 22:21:25,973 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-24 22:21:31,851 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-24 22:21:39,722 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3140\n",
            "2025-03-24 22:21:47,650 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3485\n",
            "2025-03-24 22:21:55,723 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2987\n",
            "2025-03-24 22:22:03,424 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3719\n",
            "2025-03-24 22:22:11,111 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3728\n",
            "2025-03-24 22:22:18,717 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3465\n",
            "2025-03-24 22:22:26,172 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3400\n",
            "2025-03-24 22:22:33,718 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3956\n",
            "2025-03-24 22:22:41,266 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3021\n",
            "2025-03-24 22:22:48,936 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3434\n",
            "2025-03-24 22:22:56,712 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3834\n",
            "2025-03-24 22:23:04,509 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3399\n",
            "2025-03-24 22:23:12,317 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3508\n",
            "2025-03-24 22:23:20,107 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3662\n",
            "2025-03-24 22:23:27,888 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3898\n",
            "2025-03-24 22:23:35,671 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3228\n",
            "2025-03-24 22:23:43,434 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3123\n",
            "2025-03-24 22:23:51,009 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4145\n",
            "2025-03-24 22:23:58,816 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3738\n",
            "2025-03-24 22:24:06,694 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3516\n",
            "2025-03-24 22:24:14,707 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3808\n",
            "2025-03-24 22:24:22,666 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3321\n",
            "2025-03-24 22:24:30,442 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3811\n",
            "2025-03-24 22:24:38,205 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3817\n",
            "2025-03-24 22:24:45,957 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3571\n",
            "2025-03-24 22:24:53,671 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4246\n",
            "2025-03-24 22:25:01,448 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3541\n",
            "2025-03-24 22:25:08,869 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3618\n",
            "2025-03-24 22:25:16,738 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3333\n",
            "2025-03-24 22:25:24,374 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3926\n",
            "2025-03-24 22:25:31,912 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3771\n",
            "2025-03-24 22:25:39,599 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3365\n",
            "2025-03-24 22:25:45,464 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2827\n",
            "2025-03-24 22:25:46,014 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1463\n",
            "2025-03-24 22:25:46,016 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:26:42,814 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3602, Val Loss: 0.3910, Val Acc: 0.8669\n",
            "2025-03-24 22:26:42,815 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-24 22:26:48,665 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2458\n",
            "2025-03-24 22:26:56,352 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3447\n",
            "2025-03-24 22:27:04,126 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3215\n",
            "2025-03-24 22:27:11,741 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3418\n",
            "2025-03-24 22:27:19,268 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3213\n",
            "2025-03-24 22:27:27,376 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3255\n",
            "2025-03-24 22:27:35,218 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2925\n",
            "2025-03-24 22:27:42,921 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3527\n",
            "2025-03-24 22:27:50,621 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3245\n",
            "2025-03-24 22:27:58,609 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4114\n",
            "2025-03-24 22:28:06,162 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3214\n",
            "2025-03-24 22:28:13,679 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3549\n",
            "2025-03-24 22:28:21,302 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3242\n",
            "2025-03-24 22:28:28,957 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3511\n",
            "2025-03-24 22:28:37,003 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3916\n",
            "2025-03-24 22:28:44,969 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3706\n",
            "2025-03-24 22:28:53,396 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3919\n",
            "2025-03-24 22:29:01,352 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3414\n",
            "2025-03-24 22:29:09,412 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3132\n",
            "2025-03-24 22:29:17,086 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3461\n",
            "2025-03-24 22:29:24,718 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3175\n",
            "2025-03-24 22:29:32,123 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3887\n",
            "2025-03-24 22:29:39,536 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3490\n",
            "2025-03-24 22:29:47,065 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3442\n",
            "2025-03-24 22:29:54,414 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3432\n",
            "2025-03-24 22:30:01,749 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3118\n",
            "2025-03-24 22:30:09,374 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3604\n",
            "2025-03-24 22:30:16,725 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3260\n",
            "2025-03-24 22:30:24,372 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3509\n",
            "2025-03-24 22:30:31,816 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3232\n",
            "2025-03-24 22:30:39,364 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3740\n",
            "2025-03-24 22:30:46,901 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3207\n",
            "2025-03-24 22:30:54,429 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3431\n",
            "2025-03-24 22:31:00,078 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-24 22:31:00,655 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0744\n",
            "2025-03-24 22:31:00,655 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:31:55,781 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3434, Val Loss: 0.3915, Val Acc: 0.8664\n",
            "2025-03-24 22:31:55,782 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-24 22:32:01,373 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2105\n",
            "2025-03-24 22:32:08,960 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3657\n",
            "2025-03-24 22:32:16,408 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3389\n",
            "2025-03-24 22:32:23,746 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2836\n",
            "2025-03-24 22:32:31,302 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2719\n",
            "2025-03-24 22:32:38,716 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3511\n",
            "2025-03-24 22:32:46,318 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3772\n",
            "2025-03-24 22:32:53,868 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3106\n",
            "2025-03-24 22:33:01,216 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3099\n",
            "2025-03-24 22:33:08,914 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3314\n",
            "2025-03-24 22:33:16,397 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3287\n",
            "2025-03-24 22:33:23,915 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3279\n",
            "2025-03-24 22:33:31,465 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3139\n",
            "2025-03-24 22:33:39,114 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3208\n",
            "2025-03-24 22:33:46,681 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3256\n",
            "2025-03-24 22:33:54,503 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-24 22:34:02,097 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-24 22:34:09,757 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3348\n",
            "2025-03-24 22:34:17,045 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3559\n",
            "2025-03-24 22:34:24,502 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3062\n",
            "2025-03-24 22:34:32,098 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3405\n",
            "2025-03-24 22:34:39,234 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3081\n",
            "2025-03-24 22:34:46,879 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-24 22:34:54,098 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3375\n",
            "2025-03-24 22:35:01,686 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3338\n",
            "2025-03-24 22:35:09,268 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-24 22:35:16,691 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3431\n",
            "2025-03-24 22:35:24,467 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3437\n",
            "2025-03-24 22:35:31,875 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3475\n",
            "2025-03-24 22:35:39,374 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3276\n",
            "2025-03-24 22:35:46,866 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3052\n",
            "2025-03-24 22:35:54,270 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2956\n",
            "2025-03-24 22:36:01,867 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3062\n",
            "2025-03-24 22:36:07,468 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2523\n",
            "2025-03-24 22:36:07,984 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1573\n",
            "2025-03-24 22:36:07,984 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:37:02,767 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3233, Val Loss: 0.3789, Val Acc: 0.8762\n",
            "2025-03-24 22:37:02,768 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-24 22:37:08,464 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2327\n",
            "2025-03-24 22:37:15,832 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3043\n",
            "2025-03-24 22:37:23,385 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2816\n",
            "2025-03-24 22:37:30,638 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2968\n",
            "2025-03-24 22:37:38,439 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3072\n",
            "2025-03-24 22:37:45,758 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3247\n",
            "2025-03-24 22:37:53,428 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3132\n",
            "2025-03-24 22:38:00,859 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3237\n",
            "2025-03-24 22:38:08,382 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2893\n",
            "2025-03-24 22:38:15,978 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-24 22:38:23,509 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3044\n",
            "2025-03-24 22:38:30,843 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2619\n",
            "2025-03-24 22:38:38,608 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2878\n",
            "2025-03-24 22:38:46,187 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2875\n",
            "2025-03-24 22:38:53,517 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2663\n",
            "2025-03-24 22:39:01,205 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2819\n",
            "2025-03-24 22:39:08,350 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2696\n",
            "2025-03-24 22:39:15,721 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3199\n",
            "2025-03-24 22:39:23,204 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3196\n",
            "2025-03-24 22:39:30,595 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3375\n",
            "2025-03-24 22:39:38,094 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2832\n",
            "2025-03-24 22:39:45,512 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2730\n",
            "2025-03-24 22:39:53,191 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2836\n",
            "2025-03-24 22:40:00,574 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2904\n",
            "2025-03-24 22:40:07,993 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-24 22:40:15,580 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2800\n",
            "2025-03-24 22:40:22,917 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2854\n",
            "2025-03-24 22:40:30,608 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2833\n",
            "2025-03-24 22:40:38,068 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3423\n",
            "2025-03-24 22:40:45,567 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-24 22:40:52,984 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3016\n",
            "2025-03-24 22:41:00,549 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2766\n",
            "2025-03-24 22:41:07,882 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3179\n",
            "2025-03-24 22:41:13,470 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2217\n",
            "2025-03-24 22:41:14,043 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0833\n",
            "2025-03-24 22:41:14,044 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:42:09,060 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2954, Val Loss: 0.3743, Val Acc: 0.8795\n",
            "2025-03-24 22:42:09,061 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-24 22:42:14,923 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1940\n",
            "2025-03-24 22:42:22,297 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2825\n",
            "2025-03-24 22:42:29,899 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2779\n",
            "2025-03-24 22:42:37,392 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2816\n",
            "2025-03-24 22:42:44,726 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-24 22:42:52,336 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2549\n",
            "2025-03-24 22:42:59,845 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2870\n",
            "2025-03-24 22:43:07,237 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2766\n",
            "2025-03-24 22:43:14,625 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2549\n",
            "2025-03-24 22:43:22,125 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2573\n",
            "2025-03-24 22:43:31,914 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-24 22:43:39,732 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3011\n",
            "2025-03-24 22:43:47,324 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3222\n",
            "2025-03-24 22:43:54,927 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2780\n",
            "2025-03-24 22:44:02,771 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-24 22:44:10,715 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2849\n",
            "2025-03-24 22:44:18,207 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2660\n",
            "2025-03-24 22:44:25,735 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2822\n",
            "2025-03-24 22:44:33,444 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3069\n",
            "2025-03-24 22:44:40,958 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2991\n",
            "2025-03-24 22:44:48,487 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-24 22:44:55,807 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2632\n",
            "2025-03-24 22:45:03,504 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2432\n",
            "2025-03-24 22:45:11,037 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2732\n",
            "2025-03-24 22:45:18,394 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2779\n",
            "2025-03-24 22:45:25,850 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2960\n",
            "2025-03-24 22:45:33,273 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2812\n",
            "2025-03-24 22:45:40,885 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2811\n",
            "2025-03-24 22:45:48,269 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2863\n",
            "2025-03-24 22:45:55,873 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2786\n",
            "2025-03-24 22:46:03,258 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2481\n",
            "2025-03-24 22:46:10,877 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-24 22:46:18,217 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2962\n",
            "2025-03-24 22:46:23,757 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2011\n",
            "2025-03-24 22:46:24,321 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0823\n",
            "2025-03-24 22:46:24,322 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:47:19,348 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2790, Val Loss: 0.3782, Val Acc: 0.8725\n",
            "2025-03-24 22:47:19,348 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-24 22:47:24,857 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1873\n",
            "2025-03-24 22:47:32,262 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-24 22:47:39,853 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2857\n",
            "2025-03-24 22:47:47,198 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2652\n",
            "2025-03-24 22:47:54,836 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2594\n",
            "2025-03-24 22:48:02,276 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2580\n",
            "2025-03-24 22:48:09,840 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2745\n",
            "2025-03-24 22:48:17,259 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-24 22:48:24,834 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3040\n",
            "2025-03-24 22:48:32,270 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2880\n",
            "2025-03-24 22:48:39,563 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2562\n",
            "2025-03-24 22:48:47,199 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2581\n",
            "2025-03-24 22:48:54,318 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2522\n",
            "2025-03-24 22:49:01,757 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2673\n",
            "2025-03-24 22:49:09,308 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2563\n",
            "2025-03-24 22:49:16,620 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-24 22:49:24,217 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2582\n",
            "2025-03-24 22:49:31,612 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2617\n",
            "2025-03-24 22:49:38,907 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-24 22:49:46,413 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2703\n",
            "2025-03-24 22:49:53,793 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-24 22:50:01,597 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2817\n",
            "2025-03-24 22:50:08,997 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2903\n",
            "2025-03-24 22:50:16,592 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2713\n",
            "2025-03-24 22:50:24,003 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-24 22:50:31,388 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2848\n",
            "2025-03-24 22:50:38,786 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2876\n",
            "2025-03-24 22:50:46,384 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2857\n",
            "2025-03-24 22:50:53,998 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2637\n",
            "2025-03-24 22:51:01,514 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2819\n",
            "2025-03-24 22:51:09,007 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-24 22:51:16,580 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2525\n",
            "2025-03-24 22:51:24,009 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-24 22:51:29,551 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1968\n",
            "2025-03-24 22:51:30,120 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0658\n",
            "2025-03-24 22:51:30,121 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:52:25,465 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2710, Val Loss: 0.3725, Val Acc: 0.8762\n",
            "2025-03-24 22:52:25,466 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-24 22:52:31,033 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1974\n",
            "2025-03-24 22:52:38,518 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2431\n",
            "2025-03-24 22:52:45,924 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2830\n",
            "2025-03-24 22:52:53,755 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2491\n",
            "2025-03-24 22:53:01,112 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2652\n",
            "2025-03-24 22:53:08,755 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2565\n",
            "2025-03-24 22:53:16,092 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2792\n",
            "2025-03-24 22:53:23,740 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2632\n",
            "2025-03-24 22:53:31,009 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-24 22:53:38,241 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2575\n",
            "2025-03-24 22:53:45,933 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2762\n",
            "2025-03-24 22:53:53,379 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2902\n",
            "2025-03-24 22:54:00,926 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2540\n",
            "2025-03-24 22:54:08,466 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-24 22:54:15,948 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2536\n",
            "2025-03-24 22:54:23,437 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2522\n",
            "2025-03-24 22:54:30,917 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2493\n",
            "2025-03-24 22:54:38,277 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-24 22:54:45,924 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2443\n",
            "2025-03-24 22:54:53,336 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2414\n",
            "2025-03-24 22:55:00,905 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2423\n",
            "2025-03-24 22:55:08,227 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2568\n",
            "2025-03-24 22:55:15,883 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2794\n",
            "2025-03-24 22:55:23,274 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2426\n",
            "2025-03-24 22:55:30,882 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2490\n",
            "2025-03-24 22:55:38,284 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-24 22:55:45,895 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2595\n",
            "2025-03-24 22:55:53,194 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-24 22:56:00,887 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2467\n",
            "2025-03-24 22:56:08,298 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2670\n",
            "2025-03-24 22:56:15,462 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2571\n",
            "2025-03-24 22:56:23,492 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2542\n",
            "2025-03-24 22:56:31,061 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2644\n",
            "2025-03-24 22:56:36,681 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2027\n",
            "2025-03-24 22:56:37,257 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0886\n",
            "2025-03-24 22:56:37,258 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 22:57:32,401 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2608, Val Loss: 0.3789, Val Acc: 0.8799\n",
            "2025-03-24 22:57:32,401 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-24 22:57:38,010 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1937\n",
            "2025-03-24 22:57:45,416 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2650\n",
            "2025-03-24 22:57:52,833 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2586\n",
            "2025-03-24 22:58:00,460 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2524\n",
            "2025-03-24 22:58:07,858 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-24 22:58:15,347 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2349\n",
            "2025-03-24 22:58:22,753 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2639\n",
            "2025-03-24 22:58:30,343 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2417\n",
            "2025-03-24 22:58:37,656 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2358\n",
            "2025-03-24 22:58:45,241 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2536\n",
            "2025-03-24 22:58:52,849 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2626\n",
            "2025-03-24 22:59:00,266 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2558\n",
            "2025-03-24 22:59:07,694 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2675\n",
            "2025-03-24 22:59:15,246 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2472\n",
            "2025-03-24 22:59:22,637 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2467\n",
            "2025-03-24 22:59:30,233 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2465\n",
            "2025-03-24 22:59:37,561 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2517\n",
            "2025-03-24 22:59:45,226 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-24 22:59:52,719 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-24 23:00:00,209 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2528\n",
            "2025-03-24 23:00:07,620 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-24 23:00:15,025 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2405\n",
            "2025-03-24 23:00:22,610 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2403\n",
            "2025-03-24 23:00:29,945 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2449\n",
            "2025-03-24 23:00:37,370 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2400\n",
            "2025-03-24 23:00:44,897 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2549\n",
            "2025-03-24 23:00:52,377 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2436\n",
            "2025-03-24 23:00:59,805 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2993\n",
            "2025-03-24 23:01:07,304 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2648\n",
            "2025-03-24 23:01:14,871 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2353\n",
            "2025-03-24 23:01:22,386 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2862\n",
            "2025-03-24 23:01:29,979 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-24 23:01:37,446 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2511\n",
            "2025-03-24 23:01:42,898 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1928\n",
            "2025-03-24 23:01:43,437 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0556\n",
            "2025-03-24 23:01:43,437 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:02:38,390 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2559, Val Loss: 0.3727, Val Acc: 0.8818\n",
            "2025-03-24 23:02:38,390 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-24 23:02:44,032 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1837\n",
            "2025-03-24 23:02:51,676 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2442\n",
            "2025-03-24 23:02:58,975 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2405\n",
            "2025-03-24 23:03:06,565 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2507\n",
            "2025-03-24 23:03:13,965 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2492\n",
            "2025-03-24 23:03:21,439 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2579\n",
            "2025-03-24 23:03:28,919 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2363\n",
            "2025-03-24 23:03:36,557 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2599\n",
            "2025-03-24 23:03:43,913 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2573\n",
            "2025-03-24 23:03:51,551 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2429\n",
            "2025-03-24 23:03:58,914 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2612\n",
            "2025-03-24 23:04:06,277 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-24 23:04:13,887 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2546\n",
            "2025-03-24 23:04:21,343 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2499\n",
            "2025-03-24 23:04:28,746 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2520\n",
            "2025-03-24 23:04:36,088 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-24 23:04:43,421 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2468\n",
            "2025-03-24 23:04:50,938 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2604\n",
            "2025-03-24 23:04:58,304 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2552\n",
            "2025-03-24 23:05:05,768 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2490\n",
            "2025-03-24 23:05:13,356 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2674\n",
            "2025-03-24 23:05:21,108 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2351\n",
            "2025-03-24 23:05:28,475 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2669\n",
            "2025-03-24 23:05:35,980 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2392\n",
            "2025-03-24 23:05:43,374 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2553\n",
            "2025-03-24 23:05:50,915 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2626\n",
            "2025-03-24 23:05:58,328 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-24 23:06:05,853 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2459\n",
            "2025-03-24 23:06:13,246 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2621\n",
            "2025-03-24 23:06:20,734 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2528\n",
            "2025-03-24 23:06:28,225 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2513\n",
            "2025-03-24 23:06:35,645 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2486\n",
            "2025-03-24 23:06:43,291 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2368\n",
            "2025-03-24 23:06:48,899 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1725\n",
            "2025-03-24 23:06:49,450 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0840\n",
            "2025-03-24 23:06:49,450 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:07:44,401 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2521, Val Loss: 0.3695, Val Acc: 0.8837\n",
            "2025-03-24 23:07:44,402 - INFO - [TRAIN INFO] Early stopping at epoch 20 as validation loss did not improve for 10 epochs.\n",
            "2025-03-24 23:07:44,402 - INFO - [TRAIN INFO] Total Time: 6386.68s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▂▃▃▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▆▅▅▅▄▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇▇▇█▇▇███████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▂▂▂▁▁▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.25207</td></tr><tr><td>train_val_loss_diff</td><td>-0.11739</td></tr><tr><td>val_accuracy</td><td>0.88367</td></tr><tr><td>val_loss</td><td>0.36946</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_gated_fusion_fold_2</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/k0q01ebw' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/k0q01ebw</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_212114-k0q01ebw\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 23:07:46,620 - INFO - [TRAIN INFO] Fold 2 Training Complete at epoch 20. Total Time: 6388.90s\n",
            "2025-03-24 23:07:46,635 - INFO - [K-FOLD INFO] Fold 2 completed in 6392.55 seconds\n",
            "2025-03-24 23:07:46,636 - INFO - [K-FOLD INFO] ============================== Fold 3/5 ==============================\n",
            "2025-03-24 23:07:46,638 - INFO - [K-FOLD INFO] Fold 3:\n",
            "2025-03-24 23:07:46,638 - INFO -    Train Samples: 8594\n",
            "2025-03-24 23:07:46,639 - INFO -    Validation Samples: 2149\n",
            "2025-03-24 23:07:46,640 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 3\n",
            "2025-03-24 23:07:46,641 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 3:\n",
            "2025-03-24 23:07:46,641 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-24 23:07:47,586 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 3\n",
            "2025-03-24 23:07:47,588 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 3:\n",
            "2025-03-24 23:07:47,588 - INFO - [K-FOLD INFO] Loss function initialized for Fold 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_gated\\wandb\\run-20250324_230747-1710y05n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/1710y05n' target=\"_blank\">experiment_multimodal_attention_gated_fusion_fold_3</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/1710y05n' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/1710y05n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-24 23:07:48,303 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-24 23:07:48,304 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-24 23:07:54,011 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.1140\n",
            "2025-03-24 23:08:01,880 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4461\n",
            "2025-03-24 23:08:09,254 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.4215\n",
            "2025-03-24 23:08:16,858 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3724\n",
            "2025-03-24 23:08:24,304 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3157\n",
            "2025-03-24 23:08:31,862 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3267\n",
            "2025-03-24 23:08:39,171 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2969\n",
            "2025-03-24 23:08:46,590 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2318\n",
            "2025-03-24 23:08:54,019 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2022\n",
            "2025-03-24 23:09:01,464 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2077\n",
            "2025-03-24 23:09:09,055 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2174\n",
            "2025-03-24 23:09:16,674 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1741\n",
            "2025-03-24 23:09:24,064 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1702\n",
            "2025-03-24 23:09:31,443 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1377\n",
            "2025-03-24 23:09:38,988 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1642\n",
            "2025-03-24 23:09:46,451 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1192\n",
            "2025-03-24 23:09:53,968 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1323\n",
            "2025-03-24 23:10:01,654 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0701\n",
            "2025-03-24 23:10:09,139 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0950\n",
            "2025-03-24 23:10:16,552 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0492\n",
            "2025-03-24 23:10:23,938 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0647\n",
            "2025-03-24 23:10:31,258 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0192\n",
            "2025-03-24 23:10:38,663 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0973\n",
            "2025-03-24 23:10:46,287 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0766\n",
            "2025-03-24 23:10:53,533 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9455\n",
            "2025-03-24 23:11:01,015 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0096\n",
            "2025-03-24 23:11:08,379 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9555\n",
            "2025-03-24 23:11:15,796 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9444\n",
            "2025-03-24 23:11:23,385 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9780\n",
            "2025-03-24 23:11:30,813 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9437\n",
            "2025-03-24 23:11:38,209 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9640\n",
            "2025-03-24 23:11:45,775 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9135\n",
            "2025-03-24 23:11:53,349 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9430\n",
            "2025-03-24 23:11:58,944 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6611\n",
            "2025-03-24 23:11:59,495 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2431\n",
            "2025-03-24 23:11:59,496 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:12:54,668 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1266, Val Loss: 0.7950, Val Acc: 0.6826\n",
            "2025-03-24 23:12:54,972 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 23:12:54,973 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-24 23:13:00,877 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6838\n",
            "2025-03-24 23:13:08,407 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9336\n",
            "2025-03-24 23:13:15,903 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.7939\n",
            "2025-03-24 23:13:23,108 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9076\n",
            "2025-03-24 23:13:30,619 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8538\n",
            "2025-03-24 23:13:38,372 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.9327\n",
            "2025-03-24 23:13:45,780 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8839\n",
            "2025-03-24 23:13:53,176 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.9018\n",
            "2025-03-24 23:14:00,475 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8930\n",
            "2025-03-24 23:14:07,926 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7978\n",
            "2025-03-24 23:14:15,197 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8339\n",
            "2025-03-24 23:14:22,547 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8746\n",
            "2025-03-24 23:14:29,835 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7358\n",
            "2025-03-24 23:14:37,152 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.9106\n",
            "2025-03-24 23:14:44,486 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.8067\n",
            "2025-03-24 23:14:51,641 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.7465\n",
            "2025-03-24 23:14:58,933 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8230\n",
            "2025-03-24 23:15:06,027 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.8716\n",
            "2025-03-24 23:15:13,240 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.8558\n",
            "2025-03-24 23:15:20,684 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8288\n",
            "2025-03-24 23:15:27,934 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7553\n",
            "2025-03-24 23:15:35,207 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7937\n",
            "2025-03-24 23:15:42,481 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7513\n",
            "2025-03-24 23:15:49,664 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8780\n",
            "2025-03-24 23:15:56,922 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7574\n",
            "2025-03-24 23:16:04,195 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7657\n",
            "2025-03-24 23:16:11,407 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7402\n",
            "2025-03-24 23:16:18,711 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7689\n",
            "2025-03-24 23:16:25,854 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7986\n",
            "2025-03-24 23:16:33,307 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7898\n",
            "2025-03-24 23:16:40,512 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7338\n",
            "2025-03-24 23:16:47,688 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.8012\n",
            "2025-03-24 23:16:54,878 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7568\n",
            "2025-03-24 23:17:00,432 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5154\n",
            "2025-03-24 23:17:00,956 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1356\n",
            "2025-03-24 23:17:00,957 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:17:55,254 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8181, Val Loss: 0.5979, Val Acc: 0.7594\n",
            "2025-03-24 23:17:55,554 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 23:17:55,555 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-24 23:18:01,036 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4780\n",
            "2025-03-24 23:18:08,478 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.7165\n",
            "2025-03-24 23:18:15,661 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.7815\n",
            "2025-03-24 23:18:23,071 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.7396\n",
            "2025-03-24 23:18:30,385 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7282\n",
            "2025-03-24 23:18:37,850 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6706\n",
            "2025-03-24 23:18:45,185 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6960\n",
            "2025-03-24 23:18:52,474 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6656\n",
            "2025-03-24 23:18:59,688 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7610\n",
            "2025-03-24 23:19:06,873 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7148\n",
            "2025-03-24 23:19:14,441 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6644\n",
            "2025-03-24 23:19:21,555 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7201\n",
            "2025-03-24 23:19:28,728 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7067\n",
            "2025-03-24 23:19:36,061 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7273\n",
            "2025-03-24 23:19:43,170 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6785\n",
            "2025-03-24 23:19:50,331 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6995\n",
            "2025-03-24 23:19:57,647 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6370\n",
            "2025-03-24 23:20:04,695 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6059\n",
            "2025-03-24 23:20:11,891 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6599\n",
            "2025-03-24 23:20:19,246 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6761\n",
            "2025-03-24 23:20:26,362 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6634\n",
            "2025-03-24 23:20:33,582 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6830\n",
            "2025-03-24 23:20:40,933 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6064\n",
            "2025-03-24 23:20:48,228 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6377\n",
            "2025-03-24 23:20:55,611 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6993\n",
            "2025-03-24 23:21:02,821 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6545\n",
            "2025-03-24 23:21:09,933 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6714\n",
            "2025-03-24 23:21:17,113 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6496\n",
            "2025-03-24 23:21:24,223 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6506\n",
            "2025-03-24 23:21:31,439 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6870\n",
            "2025-03-24 23:21:38,639 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7062\n",
            "2025-03-24 23:21:45,921 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6935\n",
            "2025-03-24 23:21:52,976 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6374\n",
            "2025-03-24 23:21:58,414 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4230\n",
            "2025-03-24 23:21:58,964 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2611\n",
            "2025-03-24 23:21:58,964 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:22:53,368 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6830, Val Loss: 0.4994, Val Acc: 0.8060\n",
            "2025-03-24 23:22:53,687 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 23:22:53,687 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-24 23:22:59,254 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4318\n",
            "2025-03-24 23:23:06,598 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5795\n",
            "2025-03-24 23:23:13,801 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6555\n",
            "2025-03-24 23:23:20,954 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5720\n",
            "2025-03-24 23:23:28,377 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5470\n",
            "2025-03-24 23:23:35,790 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5810\n",
            "2025-03-24 23:23:43,049 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6409\n",
            "2025-03-24 23:23:50,255 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5703\n",
            "2025-03-24 23:23:57,568 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5690\n",
            "2025-03-24 23:24:04,900 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6081\n",
            "2025-03-24 23:24:12,169 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5598\n",
            "2025-03-24 23:24:19,364 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5350\n",
            "2025-03-24 23:24:26,566 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6174\n",
            "2025-03-24 23:24:33,768 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6317\n",
            "2025-03-24 23:24:41,008 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5550\n",
            "2025-03-24 23:24:48,257 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6021\n",
            "2025-03-24 23:24:55,528 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6375\n",
            "2025-03-24 23:25:02,739 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5447\n",
            "2025-03-24 23:25:09,926 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5950\n",
            "2025-03-24 23:25:17,136 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5768\n",
            "2025-03-24 23:25:24,330 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5650\n",
            "2025-03-24 23:25:31,412 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5704\n",
            "2025-03-24 23:25:38,702 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6046\n",
            "2025-03-24 23:25:45,933 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6148\n",
            "2025-03-24 23:25:53,209 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5499\n",
            "2025-03-24 23:26:00,426 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6193\n",
            "2025-03-24 23:26:07,739 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5813\n",
            "2025-03-24 23:26:14,833 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5614\n",
            "2025-03-24 23:26:21,885 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5558\n",
            "2025-03-24 23:26:29,306 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5866\n",
            "2025-03-24 23:26:36,456 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5871\n",
            "2025-03-24 23:26:43,554 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5325\n",
            "2025-03-24 23:26:50,899 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5871\n",
            "2025-03-24 23:26:56,459 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5053\n",
            "2025-03-24 23:26:56,976 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1877\n",
            "2025-03-24 23:26:56,977 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:27:50,846 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5872, Val Loss: 0.4575, Val Acc: 0.8292\n",
            "2025-03-24 23:27:51,149 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 23:27:51,150 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-24 23:27:56,681 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4693\n",
            "2025-03-24 23:28:04,093 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5325\n",
            "2025-03-24 23:28:11,237 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4984\n",
            "2025-03-24 23:28:18,413 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5502\n",
            "2025-03-24 23:28:25,954 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5745\n",
            "2025-03-24 23:28:33,242 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5842\n",
            "2025-03-24 23:28:40,468 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5832\n",
            "2025-03-24 23:28:47,879 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5590\n",
            "2025-03-24 23:28:55,224 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5323\n",
            "2025-03-24 23:29:02,656 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5168\n",
            "2025-03-24 23:29:09,751 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5109\n",
            "2025-03-24 23:29:16,903 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5492\n",
            "2025-03-24 23:29:24,203 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5277\n",
            "2025-03-24 23:29:31,338 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5441\n",
            "2025-03-24 23:29:38,561 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5342\n",
            "2025-03-24 23:29:45,863 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5403\n",
            "2025-03-24 23:29:53,045 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5560\n",
            "2025-03-24 23:30:00,239 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5920\n",
            "2025-03-24 23:30:07,437 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5647\n",
            "2025-03-24 23:30:14,555 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5391\n",
            "2025-03-24 23:30:21,772 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5920\n",
            "2025-03-24 23:30:28,872 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5592\n",
            "2025-03-24 23:30:36,205 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5259\n",
            "2025-03-24 23:30:43,325 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5267\n",
            "2025-03-24 23:30:50,530 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5656\n",
            "2025-03-24 23:30:57,753 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5313\n",
            "2025-03-24 23:31:05,235 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4839\n",
            "2025-03-24 23:31:12,356 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5215\n",
            "2025-03-24 23:31:19,545 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5357\n",
            "2025-03-24 23:31:26,831 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4894\n",
            "2025-03-24 23:31:33,939 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5010\n",
            "2025-03-24 23:31:41,095 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4826\n",
            "2025-03-24 23:31:48,515 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5206\n",
            "2025-03-24 23:31:53,892 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3686\n",
            "2025-03-24 23:31:54,439 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1912\n",
            "2025-03-24 23:31:54,439 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:32:48,966 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5409, Val Loss: 0.4012, Val Acc: 0.8478\n",
            "2025-03-24 23:32:49,288 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 23:32:49,288 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-24 23:32:54,892 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3722\n",
            "2025-03-24 23:33:02,401 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5067\n",
            "2025-03-24 23:33:09,585 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4983\n",
            "2025-03-24 23:33:16,765 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4807\n",
            "2025-03-24 23:33:24,851 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4754\n",
            "2025-03-24 23:33:32,337 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4649\n",
            "2025-03-24 23:33:39,591 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4719\n",
            "2025-03-24 23:33:46,940 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4685\n",
            "2025-03-24 23:33:54,571 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4286\n",
            "2025-03-24 23:34:01,959 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4846\n",
            "2025-03-24 23:34:09,272 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4987\n",
            "2025-03-24 23:34:16,431 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4746\n",
            "2025-03-24 23:34:23,782 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4818\n",
            "2025-03-24 23:34:30,927 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5013\n",
            "2025-03-24 23:34:38,187 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4911\n",
            "2025-03-24 23:34:45,351 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5107\n",
            "2025-03-24 23:34:52,751 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5352\n",
            "2025-03-24 23:34:59,970 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5134\n",
            "2025-03-24 23:35:07,163 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4360\n",
            "2025-03-24 23:35:14,368 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4907\n",
            "2025-03-24 23:35:21,572 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4682\n",
            "2025-03-24 23:35:28,938 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5321\n",
            "2025-03-24 23:35:36,024 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5263\n",
            "2025-03-24 23:35:43,395 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4995\n",
            "2025-03-24 23:35:50,746 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5050\n",
            "2025-03-24 23:35:57,699 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4951\n",
            "2025-03-24 23:36:04,846 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5491\n",
            "2025-03-24 23:36:12,322 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4788\n",
            "2025-03-24 23:36:19,412 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5263\n",
            "2025-03-24 23:36:26,718 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5913\n",
            "2025-03-24 23:36:34,106 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4613\n",
            "2025-03-24 23:36:41,388 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5083\n",
            "2025-03-24 23:36:48,716 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4798\n",
            "2025-03-24 23:36:54,310 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3773\n",
            "2025-03-24 23:36:54,847 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0941\n",
            "2025-03-24 23:36:54,848 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:37:49,449 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4942, Val Loss: 0.4069, Val Acc: 0.8455\n",
            "2025-03-24 23:37:49,450 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-24 23:37:55,296 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3407\n",
            "2025-03-24 23:38:02,367 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4356\n",
            "2025-03-24 23:38:09,584 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4805\n",
            "2025-03-24 23:38:16,655 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4644\n",
            "2025-03-24 23:38:24,073 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4476\n",
            "2025-03-24 23:38:31,300 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4729\n",
            "2025-03-24 23:38:38,478 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4674\n",
            "2025-03-24 23:38:45,550 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5177\n",
            "2025-03-24 23:38:52,718 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4392\n",
            "2025-03-24 23:38:59,866 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4905\n",
            "2025-03-24 23:39:07,141 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4952\n",
            "2025-03-24 23:39:14,495 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4694\n",
            "2025-03-24 23:39:21,710 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5220\n",
            "2025-03-24 23:39:28,970 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5023\n",
            "2025-03-24 23:39:36,284 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4406\n",
            "2025-03-24 23:39:43,300 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4688\n",
            "2025-03-24 23:39:50,658 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4815\n",
            "2025-03-24 23:39:58,021 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5110\n",
            "2025-03-24 23:40:05,323 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4396\n",
            "2025-03-24 23:40:12,579 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5148\n",
            "2025-03-24 23:40:19,876 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5067\n",
            "2025-03-24 23:40:27,144 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4840\n",
            "2025-03-24 23:40:34,427 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4173\n",
            "2025-03-24 23:40:41,661 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4592\n",
            "2025-03-24 23:40:48,727 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5144\n",
            "2025-03-24 23:40:55,936 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4761\n",
            "2025-03-24 23:41:03,236 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4690\n",
            "2025-03-24 23:41:10,274 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5029\n",
            "2025-03-24 23:41:17,471 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4425\n",
            "2025-03-24 23:41:24,846 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5012\n",
            "2025-03-24 23:41:31,945 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4741\n",
            "2025-03-24 23:41:39,217 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4267\n",
            "2025-03-24 23:41:46,569 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4461\n",
            "2025-03-24 23:41:51,939 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3623\n",
            "2025-03-24 23:41:52,523 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0978\n",
            "2025-03-24 23:41:52,524 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:42:46,818 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4735, Val Loss: 0.3905, Val Acc: 0.8562\n",
            "2025-03-24 23:42:47,123 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 23:42:47,124 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-24 23:42:52,718 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3020\n",
            "2025-03-24 23:43:00,029 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4290\n",
            "2025-03-24 23:43:07,215 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4287\n",
            "2025-03-24 23:43:14,488 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4583\n",
            "2025-03-24 23:43:21,777 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4151\n",
            "2025-03-24 23:43:29,137 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4310\n",
            "2025-03-24 23:43:36,488 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4071\n",
            "2025-03-24 23:43:44,002 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4232\n",
            "2025-03-24 23:43:51,109 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4511\n",
            "2025-03-24 23:43:58,609 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4265\n",
            "2025-03-24 23:44:05,865 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5049\n",
            "2025-03-24 23:44:13,205 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4603\n",
            "2025-03-24 23:44:20,375 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4800\n",
            "2025-03-24 23:44:27,623 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4228\n",
            "2025-03-24 23:44:34,914 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4544\n",
            "2025-03-24 23:44:42,183 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4001\n",
            "2025-03-24 23:44:49,303 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3962\n",
            "2025-03-24 23:44:56,528 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4291\n",
            "2025-03-24 23:45:03,917 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4981\n",
            "2025-03-24 23:45:11,188 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4236\n",
            "2025-03-24 23:45:18,570 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3939\n",
            "2025-03-24 23:45:25,584 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4506\n",
            "2025-03-24 23:45:32,772 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4305\n",
            "2025-03-24 23:45:39,978 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4596\n",
            "2025-03-24 23:45:47,166 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4840\n",
            "2025-03-24 23:45:54,371 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4667\n",
            "2025-03-24 23:46:01,587 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4560\n",
            "2025-03-24 23:46:08,798 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4296\n",
            "2025-03-24 23:46:16,166 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5100\n",
            "2025-03-24 23:46:23,261 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4773\n",
            "2025-03-24 23:46:30,504 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4870\n",
            "2025-03-24 23:46:37,934 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4384\n",
            "2025-03-24 23:46:45,018 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4572\n",
            "2025-03-24 23:46:50,444 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2967\n",
            "2025-03-24 23:46:50,954 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1837\n",
            "2025-03-24 23:46:50,955 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:47:44,850 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4463, Val Loss: 0.3929, Val Acc: 0.8544\n",
            "2025-03-24 23:47:44,851 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-24 23:47:50,386 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2849\n",
            "2025-03-24 23:47:57,723 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4464\n",
            "2025-03-24 23:48:04,859 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4295\n",
            "2025-03-24 23:48:12,089 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4423\n",
            "2025-03-24 23:48:19,507 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3985\n",
            "2025-03-24 23:48:26,669 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3795\n",
            "2025-03-24 23:48:33,817 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4131\n",
            "2025-03-24 23:48:41,111 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3873\n",
            "2025-03-24 23:48:48,322 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3770\n",
            "2025-03-24 23:48:55,514 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4075\n",
            "2025-03-24 23:49:02,714 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3329\n",
            "2025-03-24 23:49:09,905 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4100\n",
            "2025-03-24 23:49:17,166 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3717\n",
            "2025-03-24 23:49:24,405 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4625\n",
            "2025-03-24 23:49:31,581 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4615\n",
            "2025-03-24 23:49:38,810 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4167\n",
            "2025-03-24 23:49:45,917 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4728\n",
            "2025-03-24 23:49:53,435 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4192\n",
            "2025-03-24 23:50:00,605 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4386\n",
            "2025-03-24 23:50:07,844 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4353\n",
            "2025-03-24 23:50:15,214 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3946\n",
            "2025-03-24 23:50:22,529 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3823\n",
            "2025-03-24 23:50:29,819 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4322\n",
            "2025-03-24 23:50:37,051 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4931\n",
            "2025-03-24 23:50:44,087 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4020\n",
            "2025-03-24 23:50:51,451 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4781\n",
            "2025-03-24 23:50:58,638 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4084\n",
            "2025-03-24 23:51:05,846 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3795\n",
            "2025-03-24 23:51:13,262 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4389\n",
            "2025-03-24 23:51:20,369 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3844\n",
            "2025-03-24 23:51:27,593 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4076\n",
            "2025-03-24 23:51:34,754 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4316\n",
            "2025-03-24 23:51:41,863 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4543\n",
            "2025-03-24 23:51:47,441 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-24 23:51:47,976 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1104\n",
            "2025-03-24 23:51:47,977 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:52:42,385 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4170, Val Loss: 0.3929, Val Acc: 0.8581\n",
            "2025-03-24 23:52:42,386 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-24 23:52:47,823 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2655\n",
            "2025-03-24 23:52:55,233 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4202\n",
            "2025-03-24 23:53:02,599 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3645\n",
            "2025-03-24 23:53:09,846 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3784\n",
            "2025-03-24 23:53:17,229 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3861\n",
            "2025-03-24 23:53:24,368 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3781\n",
            "2025-03-24 23:53:31,696 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3623\n",
            "2025-03-24 23:53:39,008 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3823\n",
            "2025-03-24 23:53:46,131 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3989\n",
            "2025-03-24 23:53:53,251 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4122\n",
            "2025-03-24 23:54:00,602 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3755\n",
            "2025-03-24 23:54:07,773 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3675\n",
            "2025-03-24 23:54:15,218 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4339\n",
            "2025-03-24 23:54:22,419 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3695\n",
            "2025-03-24 23:54:29,744 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3895\n",
            "2025-03-24 23:54:36,816 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3454\n",
            "2025-03-24 23:54:44,004 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3748\n",
            "2025-03-24 23:54:51,361 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3746\n",
            "2025-03-24 23:54:58,582 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4405\n",
            "2025-03-24 23:55:05,689 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3769\n",
            "2025-03-24 23:55:12,918 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3462\n",
            "2025-03-24 23:55:20,092 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4190\n",
            "2025-03-24 23:55:27,195 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3881\n",
            "2025-03-24 23:55:34,427 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3702\n",
            "2025-03-24 23:55:41,698 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4566\n",
            "2025-03-24 23:55:48,981 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3702\n",
            "2025-03-24 23:55:56,176 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3908\n",
            "2025-03-24 23:56:03,360 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3863\n",
            "2025-03-24 23:56:10,573 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3550\n",
            "2025-03-24 23:56:17,897 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3724\n",
            "2025-03-24 23:56:25,172 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4197\n",
            "2025-03-24 23:56:32,259 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4058\n",
            "2025-03-24 23:56:39,370 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3932\n",
            "2025-03-24 23:56:44,780 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3101\n",
            "2025-03-24 23:56:45,336 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1049\n",
            "2025-03-24 23:56:45,337 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-24 23:57:39,539 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3877, Val Loss: 0.3722, Val Acc: 0.8669\n",
            "2025-03-24 23:57:39,828 - INFO - [TRAIN INFO] Best Model Saved for Fold 3\n",
            "2025-03-24 23:57:39,829 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-24 23:57:45,455 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2408\n",
            "2025-03-24 23:57:52,754 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3665\n",
            "2025-03-24 23:57:59,830 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3712\n",
            "2025-03-24 23:58:07,337 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3634\n",
            "2025-03-24 23:58:14,710 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3461\n",
            "2025-03-24 23:58:21,846 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3364\n",
            "2025-03-24 23:58:29,039 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3259\n",
            "2025-03-24 23:58:36,234 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4031\n",
            "2025-03-24 23:58:43,538 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3926\n",
            "2025-03-24 23:58:50,662 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4073\n",
            "2025-03-24 23:58:58,010 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3886\n",
            "2025-03-24 23:59:05,445 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3567\n",
            "2025-03-24 23:59:12,886 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3851\n",
            "2025-03-24 23:59:20,045 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4457\n",
            "2025-03-24 23:59:27,213 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3893\n",
            "2025-03-24 23:59:34,404 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3952\n",
            "2025-03-24 23:59:41,494 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3542\n",
            "2025-03-24 23:59:48,641 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3391\n",
            "2025-03-24 23:59:55,838 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3695\n",
            "2025-03-25 00:00:03,097 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3846\n",
            "2025-03-25 00:00:10,387 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3242\n",
            "2025-03-25 00:00:17,501 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3889\n",
            "2025-03-25 00:00:24,833 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3221\n",
            "2025-03-25 00:00:32,092 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4153\n",
            "2025-03-25 00:00:39,235 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3298\n",
            "2025-03-25 00:00:46,514 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3696\n",
            "2025-03-25 00:00:53,896 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4093\n",
            "2025-03-25 00:01:01,026 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4076\n",
            "2025-03-25 00:01:08,240 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3275\n",
            "2025-03-25 00:01:15,580 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3657\n",
            "2025-03-25 00:01:22,880 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3489\n",
            "2025-03-25 00:01:30,061 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3516\n",
            "2025-03-25 00:01:37,454 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3812\n",
            "2025-03-25 00:01:42,695 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2695\n",
            "2025-03-25 00:01:43,239 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1105\n",
            "2025-03-25 00:01:43,239 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:02:37,851 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3699, Val Loss: 0.4117, Val Acc: 0.8567\n",
            "2025-03-25 00:02:37,851 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 00:02:43,339 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-25 00:02:50,547 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3486\n",
            "2025-03-25 00:02:57,848 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3450\n",
            "2025-03-25 00:03:05,046 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3427\n",
            "2025-03-25 00:03:12,227 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3324\n",
            "2025-03-25 00:03:19,453 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3398\n",
            "2025-03-25 00:03:26,567 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3410\n",
            "2025-03-25 00:03:33,973 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3334\n",
            "2025-03-25 00:03:41,247 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3387\n",
            "2025-03-25 00:03:48,322 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-25 00:03:55,464 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3600\n",
            "2025-03-25 00:04:02,829 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3838\n",
            "2025-03-25 00:04:09,991 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3356\n",
            "2025-03-25 00:04:17,108 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3203\n",
            "2025-03-25 00:04:24,613 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3426\n",
            "2025-03-25 00:04:31,968 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3516\n",
            "2025-03-25 00:04:39,135 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3453\n",
            "2025-03-25 00:04:46,240 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3416\n",
            "2025-03-25 00:04:53,289 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3393\n",
            "2025-03-25 00:05:00,611 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3487\n",
            "2025-03-25 00:05:07,817 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3486\n",
            "2025-03-25 00:05:15,019 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3352\n",
            "2025-03-25 00:05:22,202 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3532\n",
            "2025-03-25 00:05:29,552 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3596\n",
            "2025-03-25 00:05:36,611 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3710\n",
            "2025-03-25 00:05:43,883 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4166\n",
            "2025-03-25 00:05:50,974 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3356\n",
            "2025-03-25 00:05:58,405 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3658\n",
            "2025-03-25 00:06:05,404 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3332\n",
            "2025-03-25 00:06:12,589 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3210\n",
            "2025-03-25 00:06:19,845 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3291\n",
            "2025-03-25 00:06:27,093 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3134\n",
            "2025-03-25 00:06:34,392 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3166\n",
            "2025-03-25 00:06:39,765 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3057\n",
            "2025-03-25 00:06:40,315 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0699\n",
            "2025-03-25 00:06:40,316 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:07:34,967 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3453, Val Loss: 0.3921, Val Acc: 0.8646\n",
            "2025-03-25 00:07:34,967 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 00:07:40,436 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2441\n",
            "2025-03-25 00:07:47,759 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3149\n",
            "2025-03-25 00:07:54,971 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3304\n",
            "2025-03-25 00:08:02,363 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3382\n",
            "2025-03-25 00:08:09,624 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3387\n",
            "2025-03-25 00:08:16,955 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3430\n",
            "2025-03-25 00:08:24,065 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3346\n",
            "2025-03-25 00:08:31,314 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3425\n",
            "2025-03-25 00:08:38,412 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3688\n",
            "2025-03-25 00:08:45,649 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3687\n",
            "2025-03-25 00:08:52,866 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2974\n",
            "2025-03-25 00:08:59,848 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3939\n",
            "2025-03-25 00:09:07,073 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3419\n",
            "2025-03-25 00:09:14,407 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3489\n",
            "2025-03-25 00:09:21,640 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3386\n",
            "2025-03-25 00:09:28,940 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3083\n",
            "2025-03-25 00:09:36,012 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3041\n",
            "2025-03-25 00:09:43,119 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3414\n",
            "2025-03-25 00:09:50,444 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3208\n",
            "2025-03-25 00:09:57,704 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3992\n",
            "2025-03-25 00:10:04,805 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3365\n",
            "2025-03-25 00:10:12,048 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3652\n",
            "2025-03-25 00:10:19,094 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4020\n",
            "2025-03-25 00:10:26,226 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3173\n",
            "2025-03-25 00:10:33,498 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3794\n",
            "2025-03-25 00:10:40,705 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3412\n",
            "2025-03-25 00:10:48,057 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3768\n",
            "2025-03-25 00:10:55,297 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3980\n",
            "2025-03-25 00:11:02,392 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3233\n",
            "2025-03-25 00:11:09,613 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3244\n",
            "2025-03-25 00:11:17,090 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3055\n",
            "2025-03-25 00:11:24,185 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3728\n",
            "2025-03-25 00:11:31,324 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3368\n",
            "2025-03-25 00:11:36,777 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2585\n",
            "2025-03-25 00:11:37,357 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0765\n",
            "2025-03-25 00:11:37,358 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:12:31,421 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3447, Val Loss: 0.3878, Val Acc: 0.8688\n",
            "2025-03-25 00:12:31,421 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 00:12:37,076 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2164\n",
            "2025-03-25 00:12:44,435 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3146\n",
            "2025-03-25 00:12:51,651 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2951\n",
            "2025-03-25 00:12:58,818 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3154\n",
            "2025-03-25 00:13:06,074 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2981\n",
            "2025-03-25 00:13:13,467 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3302\n",
            "2025-03-25 00:13:20,574 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3282\n",
            "2025-03-25 00:13:27,775 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3253\n",
            "2025-03-25 00:13:35,033 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3297\n",
            "2025-03-25 00:13:42,147 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3355\n",
            "2025-03-25 00:13:49,327 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3090\n",
            "2025-03-25 00:13:56,656 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2728\n",
            "2025-03-25 00:14:03,746 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2961\n",
            "2025-03-25 00:14:10,924 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3471\n",
            "2025-03-25 00:14:18,240 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3005\n",
            "2025-03-25 00:14:25,397 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3661\n",
            "2025-03-25 00:14:32,836 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3107\n",
            "2025-03-25 00:14:40,187 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3217\n",
            "2025-03-25 00:14:47,364 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3543\n",
            "2025-03-25 00:14:54,685 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3792\n",
            "2025-03-25 00:15:02,022 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3692\n",
            "2025-03-25 00:15:09,158 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3171\n",
            "2025-03-25 00:15:16,357 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3542\n",
            "2025-03-25 00:15:23,602 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3027\n",
            "2025-03-25 00:15:30,713 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2924\n",
            "2025-03-25 00:15:37,910 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3643\n",
            "2025-03-25 00:15:45,221 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2932\n",
            "2025-03-25 00:15:52,298 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3440\n",
            "2025-03-25 00:15:59,388 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3275\n",
            "2025-03-25 00:16:06,606 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3359\n",
            "2025-03-25 00:16:13,846 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3393\n",
            "2025-03-25 00:16:21,102 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3518\n",
            "2025-03-25 00:16:28,534 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3157\n",
            "2025-03-25 00:16:33,873 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2522\n",
            "2025-03-25 00:16:34,408 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1249\n",
            "2025-03-25 00:16:34,409 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:17:28,574 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3268, Val Loss: 0.3872, Val Acc: 0.8646\n",
            "2025-03-25 00:17:28,574 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 00:17:34,027 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2227\n",
            "2025-03-25 00:17:41,348 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3054\n",
            "2025-03-25 00:17:48,587 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3209\n",
            "2025-03-25 00:17:55,750 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3250\n",
            "2025-03-25 00:18:03,180 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-25 00:18:10,487 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3000\n",
            "2025-03-25 00:18:17,916 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3079\n",
            "2025-03-25 00:18:25,171 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2730\n",
            "2025-03-25 00:18:32,477 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2868\n",
            "2025-03-25 00:18:39,666 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-25 00:18:46,942 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3299\n",
            "2025-03-25 00:18:54,057 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-25 00:19:01,235 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2568\n",
            "2025-03-25 00:19:08,548 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2885\n",
            "2025-03-25 00:19:15,652 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2699\n",
            "2025-03-25 00:19:22,854 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2858\n",
            "2025-03-25 00:19:30,336 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3031\n",
            "2025-03-25 00:19:37,513 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2749\n",
            "2025-03-25 00:19:44,648 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2933\n",
            "2025-03-25 00:19:51,935 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3008\n",
            "2025-03-25 00:19:59,145 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-25 00:20:06,331 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-25 00:20:13,573 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-25 00:20:20,778 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3251\n",
            "2025-03-25 00:20:28,070 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-25 00:20:35,324 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-25 00:20:42,424 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3343\n",
            "2025-03-25 00:20:49,774 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2796\n",
            "2025-03-25 00:20:57,094 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2546\n",
            "2025-03-25 00:21:04,232 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2779\n",
            "2025-03-25 00:21:11,465 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2965\n",
            "2025-03-25 00:21:18,967 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2916\n",
            "2025-03-25 00:21:26,159 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2486\n",
            "2025-03-25 00:21:31,626 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2184\n",
            "2025-03-25 00:21:32,164 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0599\n",
            "2025-03-25 00:21:32,165 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:22:26,449 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2885, Val Loss: 0.3777, Val Acc: 0.8702\n",
            "2025-03-25 00:22:26,450 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 00:22:31,795 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1973\n",
            "2025-03-25 00:22:39,083 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2974\n",
            "2025-03-25 00:22:46,344 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3055\n",
            "2025-03-25 00:22:53,694 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2685\n",
            "2025-03-25 00:23:00,887 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2905\n",
            "2025-03-25 00:23:08,234 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2719\n",
            "2025-03-25 00:23:15,609 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2607\n",
            "2025-03-25 00:23:23,271 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2558\n",
            "2025-03-25 00:23:30,621 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-25 00:23:38,309 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-25 00:23:45,876 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2837\n",
            "2025-03-25 00:23:53,277 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2722\n",
            "2025-03-25 00:24:00,840 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-25 00:24:08,208 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-25 00:24:15,860 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-25 00:24:23,179 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2748\n",
            "2025-03-25 00:24:30,843 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2757\n",
            "2025-03-25 00:24:38,255 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2737\n",
            "2025-03-25 00:24:45,855 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2797\n",
            "2025-03-25 00:24:53,181 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-25 00:25:00,651 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2748\n",
            "2025-03-25 00:25:07,962 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2970\n",
            "2025-03-25 00:25:15,357 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2836\n",
            "2025-03-25 00:25:22,829 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2634\n",
            "2025-03-25 00:25:30,167 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2980\n",
            "2025-03-25 00:25:37,656 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2668\n",
            "2025-03-25 00:25:45,227 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3161\n",
            "2025-03-25 00:25:52,577 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-25 00:26:00,406 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2551\n",
            "2025-03-25 00:26:07,539 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2619\n",
            "2025-03-25 00:26:14,971 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2628\n",
            "2025-03-25 00:26:22,667 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2584\n",
            "2025-03-25 00:26:29,936 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2801\n",
            "2025-03-25 00:26:35,336 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2130\n",
            "2025-03-25 00:26:35,888 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0587\n",
            "2025-03-25 00:26:35,889 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:27:30,416 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2776, Val Loss: 0.3830, Val Acc: 0.8730\n",
            "2025-03-25 00:27:30,416 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-25 00:27:35,851 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1908\n",
            "2025-03-25 00:27:43,182 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2804\n",
            "2025-03-25 00:27:50,289 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2751\n",
            "2025-03-25 00:27:57,440 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-25 00:28:04,932 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2554\n",
            "2025-03-25 00:28:12,193 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-25 00:28:19,382 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2863\n",
            "2025-03-25 00:28:26,589 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2741\n",
            "2025-03-25 00:28:33,750 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2616\n",
            "2025-03-25 00:28:41,175 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2628\n",
            "2025-03-25 00:28:48,369 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2551\n",
            "2025-03-25 00:28:55,691 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2641\n",
            "2025-03-25 00:29:03,313 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-25 00:29:10,722 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-25 00:29:18,334 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2863\n",
            "2025-03-25 00:29:25,527 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2832\n",
            "2025-03-25 00:29:32,780 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2930\n",
            "2025-03-25 00:29:40,342 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2490\n",
            "2025-03-25 00:29:47,725 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2464\n",
            "2025-03-25 00:29:55,351 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2633\n",
            "2025-03-25 00:30:02,672 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-25 00:30:10,154 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2756\n",
            "2025-03-25 00:30:17,486 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-25 00:30:24,924 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2889\n",
            "2025-03-25 00:30:32,346 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2747\n",
            "2025-03-25 00:30:39,714 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2692\n",
            "2025-03-25 00:30:47,315 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2705\n",
            "2025-03-25 00:30:54,649 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2702\n",
            "2025-03-25 00:31:02,116 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-25 00:31:09,707 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2693\n",
            "2025-03-25 00:31:17,126 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2677\n",
            "2025-03-25 00:31:24,724 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2623\n",
            "2025-03-25 00:31:32,103 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-25 00:31:37,665 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1980\n",
            "2025-03-25 00:31:38,230 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0737\n",
            "2025-03-25 00:31:38,231 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:32:32,418 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2698, Val Loss: 0.3938, Val Acc: 0.8660\n",
            "2025-03-25 00:32:32,418 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-25 00:32:37,910 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1907\n",
            "2025-03-25 00:32:45,299 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2558\n",
            "2025-03-25 00:32:52,581 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-25 00:32:59,916 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2543\n",
            "2025-03-25 00:33:07,414 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2647\n",
            "2025-03-25 00:33:14,665 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2795\n",
            "2025-03-25 00:33:22,293 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-25 00:33:29,706 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2838\n",
            "2025-03-25 00:33:37,266 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 00:33:44,662 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2621\n",
            "2025-03-25 00:33:51,986 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2676\n",
            "2025-03-25 00:33:59,658 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-25 00:34:07,004 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2653\n",
            "2025-03-25 00:34:14,648 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2531\n",
            "2025-03-25 00:34:22,020 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2561\n",
            "2025-03-25 00:34:29,652 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2438\n",
            "2025-03-25 00:34:37,000 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2736\n",
            "2025-03-25 00:34:44,647 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2598\n",
            "2025-03-25 00:34:51,999 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2550\n",
            "2025-03-25 00:34:59,623 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2650\n",
            "2025-03-25 00:35:06,797 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2542\n",
            "2025-03-25 00:35:14,168 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-25 00:35:21,575 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2634\n",
            "2025-03-25 00:35:29,013 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-25 00:35:36,657 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2779\n",
            "2025-03-25 00:35:43,997 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2531\n",
            "2025-03-25 00:35:51,655 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-25 00:35:59,040 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2658\n",
            "2025-03-25 00:36:06,587 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-25 00:36:14,010 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2390\n",
            "2025-03-25 00:36:21,335 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2672\n",
            "2025-03-25 00:36:29,130 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2525\n",
            "2025-03-25 00:36:36,729 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2787\n",
            "2025-03-25 00:36:42,204 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1848\n",
            "2025-03-25 00:36:42,775 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1203\n",
            "2025-03-25 00:36:42,776 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:37:36,981 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2640, Val Loss: 0.3924, Val Acc: 0.8688\n",
            "2025-03-25 00:37:36,981 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-25 00:37:42,615 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1883\n",
            "2025-03-25 00:37:49,774 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2608\n",
            "2025-03-25 00:37:57,077 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2556\n",
            "2025-03-25 00:38:04,317 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2337\n",
            "2025-03-25 00:38:11,533 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2413\n",
            "2025-03-25 00:38:18,600 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-25 00:38:25,994 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2465\n",
            "2025-03-25 00:38:33,222 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-25 00:38:40,269 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2506\n",
            "2025-03-25 00:38:47,547 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2540\n",
            "2025-03-25 00:38:54,594 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2356\n",
            "2025-03-25 00:39:01,965 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2570\n",
            "2025-03-25 00:39:09,119 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2603\n",
            "2025-03-25 00:39:16,246 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2455\n",
            "2025-03-25 00:39:23,460 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2608\n",
            "2025-03-25 00:39:30,592 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2630\n",
            "2025-03-25 00:39:37,902 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2632\n",
            "2025-03-25 00:39:45,073 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2583\n",
            "2025-03-25 00:39:52,160 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2516\n",
            "2025-03-25 00:39:59,423 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-25 00:40:06,583 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2596\n",
            "2025-03-25 00:40:13,786 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2512\n",
            "2025-03-25 00:40:21,159 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2411\n",
            "2025-03-25 00:40:28,254 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2610\n",
            "2025-03-25 00:40:35,438 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-25 00:40:42,662 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-25 00:40:49,923 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2703\n",
            "2025-03-25 00:40:57,354 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-25 00:41:04,641 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2462\n",
            "2025-03-25 00:41:11,949 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2662\n",
            "2025-03-25 00:41:19,287 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-25 00:41:26,522 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2593\n",
            "2025-03-25 00:41:33,548 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2581\n",
            "2025-03-25 00:41:39,014 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1815\n",
            "2025-03-25 00:41:39,527 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0587\n",
            "2025-03-25 00:41:39,527 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:42:33,703 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2569, Val Loss: 0.3965, Val Acc: 0.8688\n",
            "2025-03-25 00:42:33,703 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-25 00:42:39,212 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1948\n",
            "2025-03-25 00:42:46,495 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2719\n",
            "2025-03-25 00:42:53,718 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2350\n",
            "2025-03-25 00:43:01,109 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2415\n",
            "2025-03-25 00:43:08,318 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2470\n",
            "2025-03-25 00:43:15,516 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2487\n",
            "2025-03-25 00:43:23,315 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2350\n",
            "2025-03-25 00:43:30,676 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2631\n",
            "2025-03-25 00:43:37,976 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2581\n",
            "2025-03-25 00:43:45,164 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2411\n",
            "2025-03-25 00:43:52,677 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2555\n",
            "2025-03-25 00:44:00,072 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2528\n",
            "2025-03-25 00:44:07,098 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2487\n",
            "2025-03-25 00:44:14,286 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-25 00:44:21,485 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2877\n",
            "2025-03-25 00:44:28,695 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2553\n",
            "2025-03-25 00:44:35,939 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2622\n",
            "2025-03-25 00:44:43,164 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2413\n",
            "2025-03-25 00:44:50,487 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2639\n",
            "2025-03-25 00:44:57,604 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2516\n",
            "2025-03-25 00:45:04,727 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2566\n",
            "2025-03-25 00:45:12,078 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2775\n",
            "2025-03-25 00:45:19,156 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2572\n",
            "2025-03-25 00:45:26,301 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2530\n",
            "2025-03-25 00:45:33,664 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2489\n",
            "2025-03-25 00:45:40,860 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2554\n",
            "2025-03-25 00:45:48,058 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2627\n",
            "2025-03-25 00:45:55,147 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2425\n",
            "2025-03-25 00:46:02,332 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2484\n",
            "2025-03-25 00:46:09,630 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2395\n",
            "2025-03-25 00:46:16,763 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2746\n",
            "2025-03-25 00:46:23,926 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2286\n",
            "2025-03-25 00:46:31,122 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2538\n",
            "2025-03-25 00:46:36,537 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1858\n",
            "2025-03-25 00:46:37,068 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0742\n",
            "2025-03-25 00:46:37,068 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:47:30,993 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2544, Val Loss: 0.3883, Val Acc: 0.8734\n",
            "2025-03-25 00:47:30,993 - INFO - [TRAIN INFO] Early stopping at epoch 20 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 00:47:30,994 - INFO - [TRAIN INFO] Total Time: 5982.69s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▂▁▂▃▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇██████▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▆▅▅▄▄▄▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇▇█▇█████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.25436</td></tr><tr><td>train_val_loss_diff</td><td>-0.13398</td></tr><tr><td>val_accuracy</td><td>0.87343</td></tr><tr><td>val_loss</td><td>0.38835</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_gated_fusion_fold_3</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/1710y05n' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/1710y05n</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250324_230747-1710y05n\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 00:47:33,213 - INFO - [TRAIN INFO] Fold 3 Training Complete at epoch 20. Total Time: 5984.91s\n",
            "2025-03-25 00:47:33,228 - INFO - [K-FOLD INFO] Fold 3 completed in 5986.59 seconds\n",
            "2025-03-25 00:47:33,229 - INFO - [K-FOLD INFO] ============================== Fold 4/5 ==============================\n",
            "2025-03-25 00:47:33,230 - INFO - [K-FOLD INFO] Fold 4:\n",
            "2025-03-25 00:47:33,231 - INFO -    Train Samples: 8595\n",
            "2025-03-25 00:47:33,231 - INFO -    Validation Samples: 2148\n",
            "2025-03-25 00:47:33,232 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 4\n",
            "2025-03-25 00:47:33,233 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 4:\n",
            "2025-03-25 00:47:33,234 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 00:47:33,805 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 4\n",
            "2025-03-25 00:47:33,808 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 4:\n",
            "2025-03-25 00:47:33,809 - INFO - [K-FOLD INFO] Loss function initialized for Fold 4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_gated\\wandb\\run-20250325_004733-cuu7sjxc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/cuu7sjxc' target=\"_blank\">experiment_multimodal_attention_gated_fusion_fold_4</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/cuu7sjxc' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/cuu7sjxc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 00:47:34,508 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 00:47:34,509 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-25 00:47:40,191 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.1237\n",
            "2025-03-25 00:47:47,648 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4624\n",
            "2025-03-25 00:47:55,213 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.4373\n",
            "2025-03-25 00:48:02,588 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3927\n",
            "2025-03-25 00:48:10,000 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3509\n",
            "2025-03-25 00:48:17,224 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3250\n",
            "2025-03-25 00:48:24,416 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2809\n",
            "2025-03-25 00:48:31,768 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.3060\n",
            "2025-03-25 00:48:39,380 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2215\n",
            "2025-03-25 00:48:46,614 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.1757\n",
            "2025-03-25 00:48:53,980 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1823\n",
            "2025-03-25 00:49:01,198 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1800\n",
            "2025-03-25 00:49:08,507 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1325\n",
            "2025-03-25 00:49:15,791 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1847\n",
            "2025-03-25 00:49:22,924 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1393\n",
            "2025-03-25 00:49:30,548 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1340\n",
            "2025-03-25 00:49:37,705 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1785\n",
            "2025-03-25 00:49:44,925 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.1231\n",
            "2025-03-25 00:49:52,069 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.1623\n",
            "2025-03-25 00:49:59,296 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0715\n",
            "2025-03-25 00:50:06,990 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0157\n",
            "2025-03-25 00:50:14,343 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0607\n",
            "2025-03-25 00:50:21,982 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.1177\n",
            "2025-03-25 00:50:29,504 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0108\n",
            "2025-03-25 00:50:37,267 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 1.0635\n",
            "2025-03-25 00:50:44,777 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0839\n",
            "2025-03-25 00:50:52,147 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 1.0828\n",
            "2025-03-25 00:50:59,750 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9549\n",
            "2025-03-25 00:51:07,542 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 1.0413\n",
            "2025-03-25 00:51:14,939 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9890\n",
            "2025-03-25 00:51:22,556 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9299\n",
            "2025-03-25 00:51:29,956 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9858\n",
            "2025-03-25 00:51:37,557 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9498\n",
            "2025-03-25 00:51:43,136 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6969\n",
            "2025-03-25 00:51:43,807 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1859\n",
            "2025-03-25 00:51:43,807 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:52:40,012 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1476, Val Loss: 0.8204, Val Acc: 0.6811\n",
            "2025-03-25 00:52:40,298 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 00:52:40,298 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 00:52:46,312 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6926\n",
            "2025-03-25 00:52:53,641 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9442\n",
            "2025-03-25 00:53:01,136 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.9248\n",
            "2025-03-25 00:53:08,760 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8618\n",
            "2025-03-25 00:53:16,697 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8162\n",
            "2025-03-25 00:53:24,017 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.9150\n",
            "2025-03-25 00:53:31,525 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8625\n",
            "2025-03-25 00:53:38,913 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.9155\n",
            "2025-03-25 00:53:46,060 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8375\n",
            "2025-03-25 00:53:53,214 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8231\n",
            "2025-03-25 00:54:00,693 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.9633\n",
            "2025-03-25 00:54:08,010 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8520\n",
            "2025-03-25 00:54:15,498 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8013\n",
            "2025-03-25 00:54:22,698 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7764\n",
            "2025-03-25 00:54:29,976 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.8733\n",
            "2025-03-25 00:54:37,126 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8265\n",
            "2025-03-25 00:54:44,641 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8268\n",
            "2025-03-25 00:54:51,896 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7798\n",
            "2025-03-25 00:54:59,040 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7937\n",
            "2025-03-25 00:55:06,384 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8094\n",
            "2025-03-25 00:55:13,481 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7953\n",
            "2025-03-25 00:55:21,030 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.7312\n",
            "2025-03-25 00:55:28,281 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.8214\n",
            "2025-03-25 00:55:35,679 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8406\n",
            "2025-03-25 00:55:42,886 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7538\n",
            "2025-03-25 00:55:50,081 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7277\n",
            "2025-03-25 00:55:57,270 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7101\n",
            "2025-03-25 00:56:04,519 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7975\n",
            "2025-03-25 00:56:12,045 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7967\n",
            "2025-03-25 00:56:19,180 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.8512\n",
            "2025-03-25 00:56:26,357 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7710\n",
            "2025-03-25 00:56:33,659 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7687\n",
            "2025-03-25 00:56:40,801 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6871\n",
            "2025-03-25 00:56:46,179 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5355\n",
            "2025-03-25 00:56:46,718 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1511\n",
            "2025-03-25 00:56:46,719 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 00:57:40,492 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8188, Val Loss: 0.5944, Val Acc: 0.7709\n",
            "2025-03-25 00:57:40,787 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 00:57:40,788 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 00:57:46,371 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5437\n",
            "2025-03-25 00:57:53,636 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6802\n",
            "2025-03-25 00:58:00,914 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6915\n",
            "2025-03-25 00:58:08,430 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6380\n",
            "2025-03-25 00:58:15,797 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6864\n",
            "2025-03-25 00:58:23,233 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6353\n",
            "2025-03-25 00:58:30,360 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6907\n",
            "2025-03-25 00:58:37,612 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6315\n",
            "2025-03-25 00:58:45,020 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7124\n",
            "2025-03-25 00:58:52,152 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6904\n",
            "2025-03-25 00:58:59,730 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6613\n",
            "2025-03-25 00:59:07,107 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6823\n",
            "2025-03-25 00:59:14,303 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6978\n",
            "2025-03-25 00:59:21,777 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6195\n",
            "2025-03-25 00:59:29,015 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7138\n",
            "2025-03-25 00:59:36,318 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6716\n",
            "2025-03-25 00:59:43,585 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6092\n",
            "2025-03-25 00:59:50,742 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7273\n",
            "2025-03-25 00:59:57,877 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7046\n",
            "2025-03-25 01:00:05,176 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6855\n",
            "2025-03-25 01:00:12,396 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6555\n",
            "2025-03-25 01:00:19,573 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6262\n",
            "2025-03-25 01:00:26,735 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.8420\n",
            "2025-03-25 01:00:33,814 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7111\n",
            "2025-03-25 01:00:41,020 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6209\n",
            "2025-03-25 01:00:48,392 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6655\n",
            "2025-03-25 01:00:55,520 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6595\n",
            "2025-03-25 01:01:02,892 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7632\n",
            "2025-03-25 01:01:10,321 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6395\n",
            "2025-03-25 01:01:17,567 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6604\n",
            "2025-03-25 01:01:24,860 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5755\n",
            "2025-03-25 01:01:32,338 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6101\n",
            "2025-03-25 01:01:39,476 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6546\n",
            "2025-03-25 01:01:44,907 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4671\n",
            "2025-03-25 01:01:45,478 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0982\n",
            "2025-03-25 01:01:45,479 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:02:39,308 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6703, Val Loss: 0.4762, Val Acc: 0.8259\n",
            "2025-03-25 01:02:39,621 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 01:02:39,622 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 01:02:45,351 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4162\n",
            "2025-03-25 01:02:52,538 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5941\n",
            "2025-03-25 01:02:59,686 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5727\n",
            "2025-03-25 01:03:07,073 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5890\n",
            "2025-03-25 01:03:14,243 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5730\n",
            "2025-03-25 01:03:21,733 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5744\n",
            "2025-03-25 01:03:29,136 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5656\n",
            "2025-03-25 01:03:36,413 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5933\n",
            "2025-03-25 01:03:43,908 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6050\n",
            "2025-03-25 01:03:51,310 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6183\n",
            "2025-03-25 01:03:58,576 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6052\n",
            "2025-03-25 01:04:05,854 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5914\n",
            "2025-03-25 01:04:13,290 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5428\n",
            "2025-03-25 01:04:20,725 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6043\n",
            "2025-03-25 01:04:27,726 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5688\n",
            "2025-03-25 01:04:35,086 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6173\n",
            "2025-03-25 01:04:42,327 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5512\n",
            "2025-03-25 01:04:49,570 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6280\n",
            "2025-03-25 01:04:56,906 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6155\n",
            "2025-03-25 01:05:04,007 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5571\n",
            "2025-03-25 01:05:11,082 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5369\n",
            "2025-03-25 01:05:18,503 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5486\n",
            "2025-03-25 01:05:25,602 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5537\n",
            "2025-03-25 01:05:32,752 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5990\n",
            "2025-03-25 01:05:40,095 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5959\n",
            "2025-03-25 01:05:47,238 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6111\n",
            "2025-03-25 01:05:54,353 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6511\n",
            "2025-03-25 01:06:01,694 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5696\n",
            "2025-03-25 01:06:08,813 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5716\n",
            "2025-03-25 01:06:16,281 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5671\n",
            "2025-03-25 01:06:23,636 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5529\n",
            "2025-03-25 01:06:30,872 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7020\n",
            "2025-03-25 01:06:38,140 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5701\n",
            "2025-03-25 01:06:43,852 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3948\n",
            "2025-03-25 01:06:44,410 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1395\n",
            "2025-03-25 01:06:44,411 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:07:38,223 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5851, Val Loss: 0.4444, Val Acc: 0.8352\n",
            "2025-03-25 01:07:38,540 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 01:07:38,540 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 01:07:44,045 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4258\n",
            "2025-03-25 01:07:51,595 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5607\n",
            "2025-03-25 01:07:59,006 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5004\n",
            "2025-03-25 01:08:06,151 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5209\n",
            "2025-03-25 01:08:13,349 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4897\n",
            "2025-03-25 01:08:20,795 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5266\n",
            "2025-03-25 01:08:27,913 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5319\n",
            "2025-03-25 01:08:35,155 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5059\n",
            "2025-03-25 01:08:42,435 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5688\n",
            "2025-03-25 01:08:49,567 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5426\n",
            "2025-03-25 01:08:57,040 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5001\n",
            "2025-03-25 01:09:04,427 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5664\n",
            "2025-03-25 01:09:11,599 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5956\n",
            "2025-03-25 01:09:18,969 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4449\n",
            "2025-03-25 01:09:26,305 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5491\n",
            "2025-03-25 01:09:33,785 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5258\n",
            "2025-03-25 01:09:41,023 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4511\n",
            "2025-03-25 01:09:48,213 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5441\n",
            "2025-03-25 01:09:55,307 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5243\n",
            "2025-03-25 01:10:02,610 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5709\n",
            "2025-03-25 01:10:09,812 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4906\n",
            "2025-03-25 01:10:16,961 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5412\n",
            "2025-03-25 01:10:24,127 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4500\n",
            "2025-03-25 01:10:31,326 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5398\n",
            "2025-03-25 01:10:38,583 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4772\n",
            "2025-03-25 01:10:46,005 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5669\n",
            "2025-03-25 01:10:53,128 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5393\n",
            "2025-03-25 01:11:00,370 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5756\n",
            "2025-03-25 01:11:07,663 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5800\n",
            "2025-03-25 01:11:15,166 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5413\n",
            "2025-03-25 01:11:22,384 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5321\n",
            "2025-03-25 01:11:29,583 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5013\n",
            "2025-03-25 01:11:36,884 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6009\n",
            "2025-03-25 01:11:42,262 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4512\n",
            "2025-03-25 01:11:42,808 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1033\n",
            "2025-03-25 01:11:42,808 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:12:36,352 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5314, Val Loss: 0.4263, Val Acc: 0.8478\n",
            "2025-03-25 01:12:36,669 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 01:12:36,669 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 01:12:42,089 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3752\n",
            "2025-03-25 01:12:49,570 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4888\n",
            "2025-03-25 01:12:56,698 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4751\n",
            "2025-03-25 01:13:04,034 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4506\n",
            "2025-03-25 01:13:11,402 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4406\n",
            "2025-03-25 01:13:18,892 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5281\n",
            "2025-03-25 01:13:26,103 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4411\n",
            "2025-03-25 01:13:33,546 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4784\n",
            "2025-03-25 01:13:40,752 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5330\n",
            "2025-03-25 01:13:47,945 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4487\n",
            "2025-03-25 01:13:55,348 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5050\n",
            "2025-03-25 01:14:02,466 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4530\n",
            "2025-03-25 01:14:09,650 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4960\n",
            "2025-03-25 01:14:17,117 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4101\n",
            "2025-03-25 01:14:24,397 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4795\n",
            "2025-03-25 01:14:31,727 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4915\n",
            "2025-03-25 01:14:38,991 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5124\n",
            "2025-03-25 01:14:46,114 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4742\n",
            "2025-03-25 01:14:53,517 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5718\n",
            "2025-03-25 01:15:00,672 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4570\n",
            "2025-03-25 01:15:07,976 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5200\n",
            "2025-03-25 01:15:15,311 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5561\n",
            "2025-03-25 01:15:22,442 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5428\n",
            "2025-03-25 01:15:29,909 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5107\n",
            "2025-03-25 01:15:37,168 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4928\n",
            "2025-03-25 01:15:44,487 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4661\n",
            "2025-03-25 01:15:51,506 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5236\n",
            "2025-03-25 01:15:58,880 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5151\n",
            "2025-03-25 01:16:05,916 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4886\n",
            "2025-03-25 01:16:13,105 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4452\n",
            "2025-03-25 01:16:20,492 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4531\n",
            "2025-03-25 01:16:27,616 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5225\n",
            "2025-03-25 01:16:34,669 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5637\n",
            "2025-03-25 01:16:40,199 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3229\n",
            "2025-03-25 01:16:40,767 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1520\n",
            "2025-03-25 01:16:40,768 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:17:35,065 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4914, Val Loss: 0.3780, Val Acc: 0.8575\n",
            "2025-03-25 01:17:35,388 - INFO - [TRAIN INFO] Best Model Saved for Fold 4\n",
            "2025-03-25 01:17:35,389 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 01:17:41,128 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3343\n",
            "2025-03-25 01:17:48,610 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4371\n",
            "2025-03-25 01:17:55,767 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4667\n",
            "2025-03-25 01:18:03,209 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4381\n",
            "2025-03-25 01:18:10,486 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5187\n",
            "2025-03-25 01:18:17,857 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4613\n",
            "2025-03-25 01:18:25,204 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4299\n",
            "2025-03-25 01:18:32,441 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4761\n",
            "2025-03-25 01:18:39,636 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4487\n",
            "2025-03-25 01:18:46,750 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4377\n",
            "2025-03-25 01:18:54,015 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4435\n",
            "2025-03-25 01:19:01,188 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4526\n",
            "2025-03-25 01:19:08,650 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4298\n",
            "2025-03-25 01:19:16,000 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4002\n",
            "2025-03-25 01:19:23,264 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4704\n",
            "2025-03-25 01:19:30,546 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4457\n",
            "2025-03-25 01:19:38,018 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4733\n",
            "2025-03-25 01:19:45,144 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4884\n",
            "2025-03-25 01:19:52,379 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5349\n",
            "2025-03-25 01:19:59,589 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4451\n",
            "2025-03-25 01:20:07,008 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4602\n",
            "2025-03-25 01:20:14,284 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5148\n",
            "2025-03-25 01:20:21,414 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4230\n",
            "2025-03-25 01:20:28,737 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4664\n",
            "2025-03-25 01:20:36,024 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4568\n",
            "2025-03-25 01:20:43,200 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4086\n",
            "2025-03-25 01:20:50,343 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4474\n",
            "2025-03-25 01:20:57,504 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4929\n",
            "2025-03-25 01:21:04,807 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5431\n",
            "2025-03-25 01:21:11,986 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4234\n",
            "2025-03-25 01:21:19,056 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4843\n",
            "2025-03-25 01:21:26,288 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4513\n",
            "2025-03-25 01:21:33,453 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4920\n",
            "2025-03-25 01:21:38,875 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3084\n",
            "2025-03-25 01:21:39,430 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1275\n",
            "2025-03-25 01:21:39,431 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:22:33,176 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4602, Val Loss: 0.3826, Val Acc: 0.8678\n",
            "2025-03-25 01:22:33,176 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-25 01:22:38,692 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2843\n",
            "2025-03-25 01:22:45,985 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4028\n",
            "2025-03-25 01:22:53,178 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4365\n",
            "2025-03-25 01:23:00,318 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4370\n",
            "2025-03-25 01:23:07,549 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4410\n",
            "2025-03-25 01:23:14,890 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3684\n",
            "2025-03-25 01:23:22,284 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4345\n",
            "2025-03-25 01:23:29,482 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4878\n",
            "2025-03-25 01:23:36,770 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4769\n",
            "2025-03-25 01:23:43,839 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4223\n",
            "2025-03-25 01:23:51,068 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4236\n",
            "2025-03-25 01:23:58,351 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4136\n",
            "2025-03-25 01:24:05,445 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4735\n",
            "2025-03-25 01:24:12,796 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4000\n",
            "2025-03-25 01:24:20,132 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4354\n",
            "2025-03-25 01:24:27,527 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4405\n",
            "2025-03-25 01:24:34,827 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4726\n",
            "2025-03-25 01:24:42,033 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4887\n",
            "2025-03-25 01:24:49,339 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4221\n",
            "2025-03-25 01:24:56,474 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3962\n",
            "2025-03-25 01:25:03,732 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4376\n",
            "2025-03-25 01:25:11,016 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4334\n",
            "2025-03-25 01:25:18,339 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4549\n",
            "2025-03-25 01:25:25,708 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4054\n",
            "2025-03-25 01:25:32,928 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4696\n",
            "2025-03-25 01:25:40,122 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4611\n",
            "2025-03-25 01:25:47,346 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4435\n",
            "2025-03-25 01:25:54,613 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4072\n",
            "2025-03-25 01:26:02,091 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4314\n",
            "2025-03-25 01:26:09,323 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5029\n",
            "2025-03-25 01:26:16,658 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4619\n",
            "2025-03-25 01:26:23,840 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4824\n",
            "2025-03-25 01:26:31,098 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4301\n",
            "2025-03-25 01:26:36,614 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3620\n",
            "2025-03-25 01:26:37,160 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1230\n",
            "2025-03-25 01:26:37,161 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:27:31,018 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4404, Val Loss: 0.3818, Val Acc: 0.8636\n",
            "2025-03-25 01:27:31,019 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-25 01:27:36,363 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2599\n",
            "2025-03-25 01:27:43,540 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3772\n",
            "2025-03-25 01:27:50,796 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4461\n",
            "2025-03-25 01:27:58,087 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4109\n",
            "2025-03-25 01:28:05,236 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3972\n",
            "2025-03-25 01:28:12,426 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3699\n",
            "2025-03-25 01:28:19,499 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4281\n",
            "2025-03-25 01:28:26,882 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4196\n",
            "2025-03-25 01:28:33,977 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3921\n",
            "2025-03-25 01:28:41,167 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4015\n",
            "2025-03-25 01:28:48,316 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3651\n",
            "2025-03-25 01:28:55,870 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4404\n",
            "2025-03-25 01:29:03,072 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3679\n",
            "2025-03-25 01:29:10,637 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3945\n",
            "2025-03-25 01:29:17,802 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3657\n",
            "2025-03-25 01:29:25,112 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4042\n",
            "2025-03-25 01:29:32,450 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4227\n",
            "2025-03-25 01:29:39,707 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3656\n",
            "2025-03-25 01:29:47,031 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4381\n",
            "2025-03-25 01:29:54,245 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4655\n",
            "2025-03-25 01:30:01,590 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4199\n",
            "2025-03-25 01:30:08,830 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4915\n",
            "2025-03-25 01:30:16,162 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4181\n",
            "2025-03-25 01:30:23,448 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4008\n",
            "2025-03-25 01:30:30,517 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3904\n",
            "2025-03-25 01:30:37,967 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3808\n",
            "2025-03-25 01:30:45,229 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3998\n",
            "2025-03-25 01:30:52,439 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3735\n",
            "2025-03-25 01:30:59,634 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3812\n",
            "2025-03-25 01:31:06,833 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4380\n",
            "2025-03-25 01:31:14,158 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4346\n",
            "2025-03-25 01:31:21,615 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3535\n",
            "2025-03-25 01:31:28,700 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3831\n",
            "2025-03-25 01:31:34,106 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3016\n",
            "2025-03-25 01:31:34,659 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0865\n",
            "2025-03-25 01:31:34,660 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:32:28,472 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4025, Val Loss: 0.3965, Val Acc: 0.8613\n",
            "2025-03-25 01:32:28,472 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-25 01:32:33,946 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2482\n",
            "2025-03-25 01:32:41,087 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3299\n",
            "2025-03-25 01:32:48,262 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4073\n",
            "2025-03-25 01:32:55,386 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3863\n",
            "2025-03-25 01:33:02,784 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4096\n",
            "2025-03-25 01:33:10,056 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4179\n",
            "2025-03-25 01:33:17,264 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3405\n",
            "2025-03-25 01:33:24,582 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3829\n",
            "2025-03-25 01:33:31,908 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3700\n",
            "2025-03-25 01:33:39,353 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4309\n",
            "2025-03-25 01:33:46,300 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4010\n",
            "2025-03-25 01:33:53,749 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4489\n",
            "2025-03-25 01:34:01,109 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3819\n",
            "2025-03-25 01:34:08,285 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3487\n",
            "2025-03-25 01:34:15,561 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3557\n",
            "2025-03-25 01:34:22,825 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4037\n",
            "2025-03-25 01:34:30,168 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3483\n",
            "2025-03-25 01:34:37,359 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3602\n",
            "2025-03-25 01:34:44,554 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3760\n",
            "2025-03-25 01:34:51,758 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3622\n",
            "2025-03-25 01:34:58,928 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3992\n",
            "2025-03-25 01:35:06,148 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3701\n",
            "2025-03-25 01:35:13,346 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3875\n",
            "2025-03-25 01:35:20,726 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4047\n",
            "2025-03-25 01:35:27,924 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3998\n",
            "2025-03-25 01:35:35,059 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4497\n",
            "2025-03-25 01:35:42,308 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4036\n",
            "2025-03-25 01:35:49,545 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3543\n",
            "2025-03-25 01:35:56,713 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4088\n",
            "2025-03-25 01:36:03,824 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3730\n",
            "2025-03-25 01:36:10,975 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3810\n",
            "2025-03-25 01:36:18,239 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3502\n",
            "2025-03-25 01:36:25,721 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3864\n",
            "2025-03-25 01:36:31,124 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3176\n",
            "2025-03-25 01:36:31,706 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0881\n",
            "2025-03-25 01:36:31,707 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:37:25,570 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3847, Val Loss: 0.4090, Val Acc: 0.8659\n",
            "2025-03-25 01:37:25,571 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-25 01:37:31,062 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2601\n",
            "2025-03-25 01:37:38,499 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3551\n",
            "2025-03-25 01:37:45,698 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3367\n",
            "2025-03-25 01:37:53,062 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3560\n",
            "2025-03-25 01:38:00,157 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3498\n",
            "2025-03-25 01:38:07,437 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3620\n",
            "2025-03-25 01:38:15,072 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3221\n",
            "2025-03-25 01:38:22,287 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3394\n",
            "2025-03-25 01:38:29,493 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3415\n",
            "2025-03-25 01:38:36,683 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3006\n",
            "2025-03-25 01:38:43,954 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3408\n",
            "2025-03-25 01:38:51,289 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3372\n",
            "2025-03-25 01:38:58,349 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3748\n",
            "2025-03-25 01:39:05,586 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3696\n",
            "2025-03-25 01:39:12,693 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3451\n",
            "2025-03-25 01:39:19,925 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3160\n",
            "2025-03-25 01:39:27,057 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3219\n",
            "2025-03-25 01:39:34,446 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3602\n",
            "2025-03-25 01:39:41,595 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3382\n",
            "2025-03-25 01:39:48,810 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3247\n",
            "2025-03-25 01:39:56,416 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3380\n",
            "2025-03-25 01:40:03,668 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3612\n",
            "2025-03-25 01:40:10,851 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-25 01:40:18,234 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3432\n",
            "2025-03-25 01:40:25,371 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3246\n",
            "2025-03-25 01:40:32,608 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3264\n",
            "2025-03-25 01:40:39,716 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3590\n",
            "2025-03-25 01:40:46,939 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2944\n",
            "2025-03-25 01:40:54,438 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3245\n",
            "2025-03-25 01:41:01,567 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3896\n",
            "2025-03-25 01:41:08,723 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3497\n",
            "2025-03-25 01:41:16,035 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3331\n",
            "2025-03-25 01:41:23,127 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3512\n",
            "2025-03-25 01:41:28,539 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2442\n",
            "2025-03-25 01:41:29,114 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0659\n",
            "2025-03-25 01:41:29,115 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:42:22,969 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3391, Val Loss: 0.3815, Val Acc: 0.8687\n",
            "2025-03-25 01:42:22,970 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 01:42:28,670 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2187\n",
            "2025-03-25 01:42:35,972 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2988\n",
            "2025-03-25 01:42:43,125 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3486\n",
            "2025-03-25 01:42:50,602 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2959\n",
            "2025-03-25 01:42:57,769 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3346\n",
            "2025-03-25 01:43:04,744 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3025\n",
            "2025-03-25 01:43:12,181 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2930\n",
            "2025-03-25 01:43:19,596 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3255\n",
            "2025-03-25 01:43:26,796 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3433\n",
            "2025-03-25 01:43:34,141 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3249\n",
            "2025-03-25 01:43:41,394 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3065\n",
            "2025-03-25 01:43:48,705 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3222\n",
            "2025-03-25 01:43:56,184 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3063\n",
            "2025-03-25 01:44:03,302 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2965\n",
            "2025-03-25 01:44:10,463 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3308\n",
            "2025-03-25 01:44:17,777 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3039\n",
            "2025-03-25 01:44:24,917 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2942\n",
            "2025-03-25 01:44:32,380 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-25 01:44:39,533 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3192\n",
            "2025-03-25 01:44:46,772 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3113\n",
            "2025-03-25 01:44:54,082 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3456\n",
            "2025-03-25 01:45:01,329 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3040\n",
            "2025-03-25 01:45:08,406 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2728\n",
            "2025-03-25 01:45:15,618 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3089\n",
            "2025-03-25 01:45:22,866 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3290\n",
            "2025-03-25 01:45:30,156 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3570\n",
            "2025-03-25 01:45:37,351 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3320\n",
            "2025-03-25 01:45:44,557 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-25 01:45:51,796 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3356\n",
            "2025-03-25 01:45:59,068 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3148\n",
            "2025-03-25 01:46:06,354 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3590\n",
            "2025-03-25 01:46:13,496 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-25 01:46:20,946 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2803\n",
            "2025-03-25 01:46:26,288 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-25 01:46:26,813 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0763\n",
            "2025-03-25 01:46:26,814 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:47:21,123 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3160, Val Loss: 0.3896, Val Acc: 0.8682\n",
            "2025-03-25 01:47:21,123 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 01:47:26,430 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2208\n",
            "2025-03-25 01:47:33,606 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-25 01:47:40,867 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3324\n",
            "2025-03-25 01:47:48,311 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2902\n",
            "2025-03-25 01:47:55,411 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3196\n",
            "2025-03-25 01:48:02,695 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3103\n",
            "2025-03-25 01:48:10,096 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2640\n",
            "2025-03-25 01:48:17,526 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3004\n",
            "2025-03-25 01:48:24,712 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2980\n",
            "2025-03-25 01:48:32,112 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2722\n",
            "2025-03-25 01:48:39,230 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-25 01:48:46,551 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3576\n",
            "2025-03-25 01:48:53,877 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-25 01:49:01,091 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3199\n",
            "2025-03-25 01:49:08,451 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2847\n",
            "2025-03-25 01:49:15,696 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3352\n",
            "2025-03-25 01:49:23,003 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2700\n",
            "2025-03-25 01:49:30,210 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3032\n",
            "2025-03-25 01:49:37,490 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2894\n",
            "2025-03-25 01:49:44,584 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2843\n",
            "2025-03-25 01:49:51,819 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3282\n",
            "2025-03-25 01:49:59,232 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3060\n",
            "2025-03-25 01:50:06,466 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3138\n",
            "2025-03-25 01:50:13,668 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3116\n",
            "2025-03-25 01:50:20,869 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2937\n",
            "2025-03-25 01:50:28,062 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3635\n",
            "2025-03-25 01:50:35,451 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2925\n",
            "2025-03-25 01:50:42,659 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3177\n",
            "2025-03-25 01:50:49,856 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3309\n",
            "2025-03-25 01:50:57,137 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3155\n",
            "2025-03-25 01:51:04,282 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3087\n",
            "2025-03-25 01:51:11,661 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2877\n",
            "2025-03-25 01:51:18,932 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3488\n",
            "2025-03-25 01:51:24,365 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2215\n",
            "2025-03-25 01:51:24,936 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1366\n",
            "2025-03-25 01:51:24,937 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:52:18,794 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3083, Val Loss: 0.3978, Val Acc: 0.8664\n",
            "2025-03-25 01:52:18,794 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 01:52:24,428 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2168\n",
            "2025-03-25 01:52:31,776 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-25 01:52:38,912 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2773\n",
            "2025-03-25 01:52:46,157 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2955\n",
            "2025-03-25 01:52:53,599 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-25 01:53:00,737 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2766\n",
            "2025-03-25 01:53:08,227 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3020\n",
            "2025-03-25 01:53:15,493 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2718\n",
            "2025-03-25 01:53:22,699 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3213\n",
            "2025-03-25 01:53:30,160 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3313\n",
            "2025-03-25 01:53:37,329 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3222\n",
            "2025-03-25 01:53:45,211 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-25 01:53:52,521 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3055\n",
            "2025-03-25 01:53:59,973 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2696\n",
            "2025-03-25 01:54:07,577 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-25 01:54:14,864 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2896\n",
            "2025-03-25 01:54:22,368 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-25 01:54:29,501 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2940\n",
            "2025-03-25 01:54:36,958 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2890\n",
            "2025-03-25 01:54:44,356 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3204\n",
            "2025-03-25 01:54:51,396 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3396\n",
            "2025-03-25 01:54:58,592 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3151\n",
            "2025-03-25 01:55:05,880 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3001\n",
            "2025-03-25 01:55:13,182 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3153\n",
            "2025-03-25 01:55:20,282 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3103\n",
            "2025-03-25 01:55:27,540 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3060\n",
            "2025-03-25 01:55:34,974 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2694\n",
            "2025-03-25 01:55:42,079 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2783\n",
            "2025-03-25 01:55:49,243 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2645\n",
            "2025-03-25 01:55:56,566 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2739\n",
            "2025-03-25 01:56:03,714 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2907\n",
            "2025-03-25 01:56:11,159 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3106\n",
            "2025-03-25 01:56:18,363 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2717\n",
            "2025-03-25 01:56:23,573 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2312\n",
            "2025-03-25 01:56:24,114 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0710\n",
            "2025-03-25 01:56:24,114 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 01:57:17,902 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.2949, Val Loss: 0.3989, Val Acc: 0.8710\n",
            "2025-03-25 01:57:17,903 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 01:57:23,267 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1960\n",
            "2025-03-25 01:57:30,568 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2933\n",
            "2025-03-25 01:57:37,940 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2638\n",
            "2025-03-25 01:57:45,083 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2582\n",
            "2025-03-25 01:57:52,536 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2777\n",
            "2025-03-25 01:57:59,735 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2797\n",
            "2025-03-25 01:58:07,011 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2769\n",
            "2025-03-25 01:58:14,329 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2930\n",
            "2025-03-25 01:58:21,463 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2919\n",
            "2025-03-25 01:58:28,779 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2630\n",
            "2025-03-25 01:58:36,124 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2660\n",
            "2025-03-25 01:58:43,195 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2884\n",
            "2025-03-25 01:58:50,394 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2620\n",
            "2025-03-25 01:58:57,890 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3064\n",
            "2025-03-25 01:59:05,193 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2720\n",
            "2025-03-25 01:59:12,345 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-25 01:59:19,705 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2731\n",
            "2025-03-25 01:59:26,883 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2767\n",
            "2025-03-25 01:59:34,074 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2827\n",
            "2025-03-25 01:59:41,217 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2774\n",
            "2025-03-25 01:59:48,267 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2929\n",
            "2025-03-25 01:59:55,520 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3007\n",
            "2025-03-25 02:00:02,862 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2871\n",
            "2025-03-25 02:00:10,002 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-25 02:00:17,160 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2813\n",
            "2025-03-25 02:00:24,370 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2972\n",
            "2025-03-25 02:00:31,480 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2892\n",
            "2025-03-25 02:00:39,044 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2665\n",
            "2025-03-25 02:00:46,188 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2686\n",
            "2025-03-25 02:00:53,344 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2969\n",
            "2025-03-25 02:01:00,678 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2785\n",
            "2025-03-25 02:01:07,789 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2644\n",
            "2025-03-25 02:01:15,009 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3012\n",
            "2025-03-25 02:01:20,274 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1912\n",
            "2025-03-25 02:01:20,814 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0716\n",
            "2025-03-25 02:01:20,814 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:02:14,719 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2806, Val Loss: 0.3972, Val Acc: 0.8738\n",
            "2025-03-25 02:02:14,720 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 02:02:20,120 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2203\n",
            "2025-03-25 02:02:27,327 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2590\n",
            "2025-03-25 02:02:34,571 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-25 02:02:42,038 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2639\n",
            "2025-03-25 02:02:49,287 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2840\n",
            "2025-03-25 02:02:56,641 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2441\n",
            "2025-03-25 02:03:03,999 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2790\n",
            "2025-03-25 02:03:11,209 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2828\n",
            "2025-03-25 02:03:18,427 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2550\n",
            "2025-03-25 02:03:26,419 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2761\n",
            "2025-03-25 02:03:33,955 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2941\n",
            "2025-03-25 02:03:41,367 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2964\n",
            "2025-03-25 02:03:49,024 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2707\n",
            "2025-03-25 02:03:56,429 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2885\n",
            "2025-03-25 02:04:03,992 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2803\n",
            "2025-03-25 02:04:11,421 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2826\n",
            "2025-03-25 02:04:18,618 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-25 02:04:26,125 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2559\n",
            "2025-03-25 02:04:33,562 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2959\n",
            "2025-03-25 02:04:41,122 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-25 02:04:48,327 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-25 02:04:55,805 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2868\n",
            "2025-03-25 02:05:02,946 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2686\n",
            "2025-03-25 02:05:10,190 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2555\n",
            "2025-03-25 02:05:17,323 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2654\n",
            "2025-03-25 02:05:24,493 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2760\n",
            "2025-03-25 02:05:31,949 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2668\n",
            "2025-03-25 02:05:39,247 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3069\n",
            "2025-03-25 02:05:46,435 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-25 02:05:53,694 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2724\n",
            "2025-03-25 02:06:00,982 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-25 02:06:08,087 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3089\n",
            "2025-03-25 02:06:15,184 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2698\n",
            "2025-03-25 02:06:20,702 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1926\n",
            "2025-03-25 02:06:21,275 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0701\n",
            "2025-03-25 02:06:21,276 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:07:17,237 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2752, Val Loss: 0.4013, Val Acc: 0.8734\n",
            "2025-03-25 02:07:17,238 - INFO - [TRAIN INFO] Early stopping at epoch 16 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 02:07:17,239 - INFO - [TRAIN INFO] Total Time: 4782.73s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇██▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▆▅▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇██████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.27522</td></tr><tr><td>train_val_loss_diff</td><td>-0.12603</td></tr><tr><td>val_accuracy</td><td>0.87337</td></tr><tr><td>val_loss</td><td>0.40126</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_gated_fusion_fold_4</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/cuu7sjxc' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/cuu7sjxc</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250325_004733-cuu7sjxc\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 02:07:19,523 - INFO - [TRAIN INFO] Fold 4 Training Complete at epoch 16. Total Time: 4785.01s\n",
            "2025-03-25 02:07:19,538 - INFO - [K-FOLD INFO] Fold 4 completed in 4786.31 seconds\n",
            "2025-03-25 02:07:19,539 - INFO - [K-FOLD INFO] ============================== Fold 5/5 ==============================\n",
            "2025-03-25 02:07:19,541 - INFO - [K-FOLD INFO] Fold 5:\n",
            "2025-03-25 02:07:19,541 - INFO -    Train Samples: 8595\n",
            "2025-03-25 02:07:19,542 - INFO -    Validation Samples: 2148\n",
            "2025-03-25 02:07:19,542 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 5\n",
            "2025-03-25 02:07:19,543 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 5:\n",
            "2025-03-25 02:07:19,544 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 02:07:20,159 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 5\n",
            "2025-03-25 02:07:20,161 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 5:\n",
            "2025-03-25 02:07:20,161 - INFO - [K-FOLD INFO] Loss function initialized for Fold 5\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_attention_gated\\wandb\\run-20250325_020720-nl640e8f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/nl640e8f' target=\"_blank\">experiment_multimodal_attention_gated_fusion_fold_5</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/nl640e8f' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/nl640e8f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 02:07:20,879 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 02:07:20,880 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-25 02:07:26,767 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0443\n",
            "2025-03-25 02:07:34,367 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.3688\n",
            "2025-03-25 02:07:42,133 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3850\n",
            "2025-03-25 02:07:49,641 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3485\n",
            "2025-03-25 02:07:57,141 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3298\n",
            "2025-03-25 02:08:04,538 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.3146\n",
            "2025-03-25 02:08:11,882 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2727\n",
            "2025-03-25 02:08:19,000 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2139\n",
            "2025-03-25 02:08:26,337 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2421\n",
            "2025-03-25 02:08:33,447 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2096\n",
            "2025-03-25 02:08:40,602 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1652\n",
            "2025-03-25 02:08:48,112 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.2000\n",
            "2025-03-25 02:08:55,496 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.2423\n",
            "2025-03-25 02:09:03,317 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1717\n",
            "2025-03-25 02:09:10,859 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1797\n",
            "2025-03-25 02:09:18,371 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1394\n",
            "2025-03-25 02:09:25,922 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1014\n",
            "2025-03-25 02:09:33,692 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.1324\n",
            "2025-03-25 02:09:41,336 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0301\n",
            "2025-03-25 02:09:48,840 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0985\n",
            "2025-03-25 02:09:56,184 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0257\n",
            "2025-03-25 02:10:03,922 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.9688\n",
            "2025-03-25 02:10:11,698 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.9597\n",
            "2025-03-25 02:10:19,085 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0682\n",
            "2025-03-25 02:10:26,679 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9743\n",
            "2025-03-25 02:10:34,105 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9760\n",
            "2025-03-25 02:10:41,692 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9963\n",
            "2025-03-25 02:10:49,257 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9920\n",
            "2025-03-25 02:10:56,755 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 1.0437\n",
            "2025-03-25 02:11:04,477 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 1.0600\n",
            "2025-03-25 02:11:11,952 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9328\n",
            "2025-03-25 02:11:19,372 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9404\n",
            "2025-03-25 02:11:27,076 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9301\n",
            "2025-03-25 02:11:32,819 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7087\n",
            "2025-03-25 02:11:33,391 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2480\n",
            "2025-03-25 02:11:33,391 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:12:27,470 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1264, Val Loss: 0.7846, Val Acc: 0.7076\n",
            "2025-03-25 02:12:27,762 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:12:27,762 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 02:12:33,584 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6738\n",
            "2025-03-25 02:12:41,254 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9413\n",
            "2025-03-25 02:12:48,580 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.9544\n",
            "2025-03-25 02:12:55,859 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9271\n",
            "2025-03-25 02:13:03,237 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8694\n",
            "2025-03-25 02:13:10,826 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.9357\n",
            "2025-03-25 02:13:18,241 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8973\n",
            "2025-03-25 02:13:25,536 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8506\n",
            "2025-03-25 02:13:33,048 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8724\n",
            "2025-03-25 02:13:40,389 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8920\n",
            "2025-03-25 02:13:47,637 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7978\n",
            "2025-03-25 02:13:54,854 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8621\n",
            "2025-03-25 02:14:02,112 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8609\n",
            "2025-03-25 02:14:09,573 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7775\n",
            "2025-03-25 02:14:16,832 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7858\n",
            "2025-03-25 02:14:24,029 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8158\n",
            "2025-03-25 02:14:31,234 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.8870\n",
            "2025-03-25 02:14:38,575 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.8406\n",
            "2025-03-25 02:14:46,013 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7932\n",
            "2025-03-25 02:14:53,091 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7144\n",
            "2025-03-25 02:15:00,319 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7355\n",
            "2025-03-25 02:15:07,607 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.8434\n",
            "2025-03-25 02:15:14,738 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.8587\n",
            "2025-03-25 02:15:21,863 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7914\n",
            "2025-03-25 02:15:29,360 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6870\n",
            "2025-03-25 02:15:36,767 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7787\n",
            "2025-03-25 02:15:43,990 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7982\n",
            "2025-03-25 02:15:51,131 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7265\n",
            "2025-03-25 02:15:58,489 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.8321\n",
            "2025-03-25 02:16:05,792 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7885\n",
            "2025-03-25 02:16:12,918 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7405\n",
            "2025-03-25 02:16:20,165 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6853\n",
            "2025-03-25 02:16:27,533 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.8017\n",
            "2025-03-25 02:16:32,887 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6086\n",
            "2025-03-25 02:16:33,461 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2658\n",
            "2025-03-25 02:16:33,462 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:17:27,617 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8264, Val Loss: 0.5900, Val Acc: 0.7831\n",
            "2025-03-25 02:17:27,911 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:17:27,912 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 02:17:33,501 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6117\n",
            "2025-03-25 02:17:40,913 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.7166\n",
            "2025-03-25 02:17:48,156 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.7342\n",
            "2025-03-25 02:17:55,505 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6050\n",
            "2025-03-25 02:18:02,954 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.7002\n",
            "2025-03-25 02:18:10,158 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6862\n",
            "2025-03-25 02:18:17,445 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.7185\n",
            "2025-03-25 02:18:24,948 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6557\n",
            "2025-03-25 02:18:32,279 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6466\n",
            "2025-03-25 02:18:39,602 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7397\n",
            "2025-03-25 02:18:46,827 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6507\n",
            "2025-03-25 02:18:54,002 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.6669\n",
            "2025-03-25 02:19:01,538 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7147\n",
            "2025-03-25 02:19:08,686 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6716\n",
            "2025-03-25 02:19:15,860 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6853\n",
            "2025-03-25 02:19:23,211 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6991\n",
            "2025-03-25 02:19:30,672 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6637\n",
            "2025-03-25 02:19:37,886 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6230\n",
            "2025-03-25 02:19:45,114 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6632\n",
            "2025-03-25 02:19:52,526 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6518\n",
            "2025-03-25 02:19:59,894 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.6408\n",
            "2025-03-25 02:20:07,062 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6842\n",
            "2025-03-25 02:20:14,326 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6666\n",
            "2025-03-25 02:20:21,476 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6826\n",
            "2025-03-25 02:20:28,718 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6126\n",
            "2025-03-25 02:20:35,964 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6440\n",
            "2025-03-25 02:20:43,303 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7239\n",
            "2025-03-25 02:20:50,452 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6614\n",
            "2025-03-25 02:20:57,635 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6492\n",
            "2025-03-25 02:21:05,100 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6244\n",
            "2025-03-25 02:21:12,237 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7091\n",
            "2025-03-25 02:21:19,330 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6763\n",
            "2025-03-25 02:21:26,859 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6799\n",
            "2025-03-25 02:21:32,359 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5108\n",
            "2025-03-25 02:21:32,916 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1600\n",
            "2025-03-25 02:21:32,917 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:22:26,916 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6764, Val Loss: 0.4782, Val Acc: 0.8217\n",
            "2025-03-25 02:22:27,221 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:22:27,221 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 02:22:32,797 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4590\n",
            "2025-03-25 02:22:40,079 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6033\n",
            "2025-03-25 02:22:47,276 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5649\n",
            "2025-03-25 02:22:54,473 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5501\n",
            "2025-03-25 02:23:01,664 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6189\n",
            "2025-03-25 02:23:09,062 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6518\n",
            "2025-03-25 02:23:16,181 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5605\n",
            "2025-03-25 02:23:23,525 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5906\n",
            "2025-03-25 02:23:30,860 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5540\n",
            "2025-03-25 02:23:37,961 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6241\n",
            "2025-03-25 02:23:45,244 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6421\n",
            "2025-03-25 02:23:52,562 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5906\n",
            "2025-03-25 02:23:59,848 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5918\n",
            "2025-03-25 02:24:07,248 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6048\n",
            "2025-03-25 02:24:14,647 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6201\n",
            "2025-03-25 02:24:21,683 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6115\n",
            "2025-03-25 02:24:28,824 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5630\n",
            "2025-03-25 02:24:36,233 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6649\n",
            "2025-03-25 02:24:43,373 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5866\n",
            "2025-03-25 02:24:50,558 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5805\n",
            "2025-03-25 02:24:57,829 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5586\n",
            "2025-03-25 02:25:05,178 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6433\n",
            "2025-03-25 02:25:12,513 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5441\n",
            "2025-03-25 02:25:19,831 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6354\n",
            "2025-03-25 02:25:26,966 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6171\n",
            "2025-03-25 02:25:34,164 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5347\n",
            "2025-03-25 02:25:41,594 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6052\n",
            "2025-03-25 02:25:48,763 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5414\n",
            "2025-03-25 02:25:56,195 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6106\n",
            "2025-03-25 02:26:03,413 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6052\n",
            "2025-03-25 02:26:10,683 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5750\n",
            "2025-03-25 02:26:18,010 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5479\n",
            "2025-03-25 02:26:25,176 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5748\n",
            "2025-03-25 02:26:30,512 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4458\n",
            "2025-03-25 02:26:31,071 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1718\n",
            "2025-03-25 02:26:31,072 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:27:25,018 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5939, Val Loss: 0.4258, Val Acc: 0.8426\n",
            "2025-03-25 02:27:25,304 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:27:25,304 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 02:27:30,887 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3819\n",
            "2025-03-25 02:27:38,187 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5238\n",
            "2025-03-25 02:27:45,525 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5765\n",
            "2025-03-25 02:27:52,973 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5205\n",
            "2025-03-25 02:28:00,129 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5221\n",
            "2025-03-25 02:28:07,579 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5182\n",
            "2025-03-25 02:28:14,799 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5678\n",
            "2025-03-25 02:28:22,174 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5674\n",
            "2025-03-25 02:28:29,322 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5433\n",
            "2025-03-25 02:28:36,412 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4783\n",
            "2025-03-25 02:28:43,763 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5020\n",
            "2025-03-25 02:28:50,902 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5109\n",
            "2025-03-25 02:28:58,100 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5017\n",
            "2025-03-25 02:29:05,407 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5339\n",
            "2025-03-25 02:29:12,624 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5385\n",
            "2025-03-25 02:29:19,954 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5240\n",
            "2025-03-25 02:29:27,063 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6012\n",
            "2025-03-25 02:29:34,352 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5908\n",
            "2025-03-25 02:29:41,450 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5411\n",
            "2025-03-25 02:29:48,709 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5649\n",
            "2025-03-25 02:29:56,031 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4895\n",
            "2025-03-25 02:30:03,333 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5658\n",
            "2025-03-25 02:30:10,530 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5178\n",
            "2025-03-25 02:30:17,892 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5231\n",
            "2025-03-25 02:30:25,046 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5657\n",
            "2025-03-25 02:30:32,299 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5480\n",
            "2025-03-25 02:30:39,619 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5350\n",
            "2025-03-25 02:30:46,813 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5326\n",
            "2025-03-25 02:30:54,285 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5532\n",
            "2025-03-25 02:31:01,520 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5989\n",
            "2025-03-25 02:31:08,714 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5495\n",
            "2025-03-25 02:31:16,062 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5152\n",
            "2025-03-25 02:31:23,255 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5722\n",
            "2025-03-25 02:31:28,657 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3789\n",
            "2025-03-25 02:31:29,208 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1561\n",
            "2025-03-25 02:31:29,209 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:32:23,256 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5396, Val Loss: 0.4057, Val Acc: 0.8520\n",
            "2025-03-25 02:32:23,573 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:32:23,574 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 02:32:29,284 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3980\n",
            "2025-03-25 02:32:36,613 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5577\n",
            "2025-03-25 02:32:43,812 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4824\n",
            "2025-03-25 02:32:51,092 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4795\n",
            "2025-03-25 02:32:58,374 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4584\n",
            "2025-03-25 02:33:05,676 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5447\n",
            "2025-03-25 02:33:12,887 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4563\n",
            "2025-03-25 02:33:20,084 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4342\n",
            "2025-03-25 02:33:27,413 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5284\n",
            "2025-03-25 02:33:34,872 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4773\n",
            "2025-03-25 02:33:41,992 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4674\n",
            "2025-03-25 02:33:49,155 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5119\n",
            "2025-03-25 02:33:56,460 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4502\n",
            "2025-03-25 02:34:03,605 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4862\n",
            "2025-03-25 02:34:11,045 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4356\n",
            "2025-03-25 02:34:18,255 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5104\n",
            "2025-03-25 02:34:25,461 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4643\n",
            "2025-03-25 02:34:32,839 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4523\n",
            "2025-03-25 02:34:39,986 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5497\n",
            "2025-03-25 02:34:47,196 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5082\n",
            "2025-03-25 02:34:54,616 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5797\n",
            "2025-03-25 02:35:01,701 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4994\n",
            "2025-03-25 02:35:08,947 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4692\n",
            "2025-03-25 02:35:16,236 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5266\n",
            "2025-03-25 02:35:23,357 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5713\n",
            "2025-03-25 02:35:30,481 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5281\n",
            "2025-03-25 02:35:37,840 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4870\n",
            "2025-03-25 02:35:45,131 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4548\n",
            "2025-03-25 02:35:52,504 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5275\n",
            "2025-03-25 02:35:59,836 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4812\n",
            "2025-03-25 02:36:07,001 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5580\n",
            "2025-03-25 02:36:14,061 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4974\n",
            "2025-03-25 02:36:21,229 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5176\n",
            "2025-03-25 02:36:26,634 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3601\n",
            "2025-03-25 02:36:27,206 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1394\n",
            "2025-03-25 02:36:27,206 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:37:21,619 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4993, Val Loss: 0.3922, Val Acc: 0.8589\n",
            "2025-03-25 02:37:21,938 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:37:21,939 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 02:37:27,558 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3561\n",
            "2025-03-25 02:37:34,870 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4338\n",
            "2025-03-25 02:37:42,355 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5012\n",
            "2025-03-25 02:37:49,579 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4626\n",
            "2025-03-25 02:37:56,892 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4214\n",
            "2025-03-25 02:38:04,150 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4003\n",
            "2025-03-25 02:38:11,575 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4807\n",
            "2025-03-25 02:38:18,981 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4323\n",
            "2025-03-25 02:38:26,254 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4947\n",
            "2025-03-25 02:38:33,497 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4699\n",
            "2025-03-25 02:38:40,902 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4391\n",
            "2025-03-25 02:38:48,170 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4200\n",
            "2025-03-25 02:38:55,371 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4768\n",
            "2025-03-25 02:39:02,570 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4925\n",
            "2025-03-25 02:39:09,774 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4447\n",
            "2025-03-25 02:39:16,962 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5048\n",
            "2025-03-25 02:39:24,279 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4178\n",
            "2025-03-25 02:39:31,565 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4235\n",
            "2025-03-25 02:39:38,685 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4794\n",
            "2025-03-25 02:39:45,834 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5464\n",
            "2025-03-25 02:39:53,115 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4395\n",
            "2025-03-25 02:40:00,550 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4354\n",
            "2025-03-25 02:40:07,643 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4863\n",
            "2025-03-25 02:40:14,794 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5158\n",
            "2025-03-25 02:40:22,143 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4481\n",
            "2025-03-25 02:40:29,246 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4557\n",
            "2025-03-25 02:40:36,933 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5112\n",
            "2025-03-25 02:40:43,936 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5143\n",
            "2025-03-25 02:40:51,133 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5020\n",
            "2025-03-25 02:40:58,338 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4402\n",
            "2025-03-25 02:41:05,700 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4966\n",
            "2025-03-25 02:41:12,877 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4648\n",
            "2025-03-25 02:41:20,083 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4837\n",
            "2025-03-25 02:41:25,642 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3717\n",
            "2025-03-25 02:41:26,195 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0728\n",
            "2025-03-25 02:41:26,196 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:42:20,239 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4663, Val Loss: 0.3875, Val Acc: 0.8561\n",
            "2025-03-25 02:42:20,531 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:42:20,531 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-25 02:42:26,099 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3348\n",
            "2025-03-25 02:42:33,487 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4484\n",
            "2025-03-25 02:42:40,660 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4565\n",
            "2025-03-25 02:42:48,098 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4842\n",
            "2025-03-25 02:42:55,305 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4101\n",
            "2025-03-25 02:43:02,636 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4662\n",
            "2025-03-25 02:43:09,827 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4193\n",
            "2025-03-25 02:43:17,070 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4593\n",
            "2025-03-25 02:43:24,206 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4658\n",
            "2025-03-25 02:43:31,441 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4158\n",
            "2025-03-25 02:43:38,666 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4569\n",
            "2025-03-25 02:43:45,985 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4276\n",
            "2025-03-25 02:43:53,248 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4444\n",
            "2025-03-25 02:44:00,896 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4020\n",
            "2025-03-25 02:44:08,183 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4224\n",
            "2025-03-25 02:44:15,343 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4404\n",
            "2025-03-25 02:44:22,545 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4539\n",
            "2025-03-25 02:44:29,860 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4241\n",
            "2025-03-25 02:44:37,004 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4234\n",
            "2025-03-25 02:44:44,130 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4666\n",
            "2025-03-25 02:44:51,462 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4441\n",
            "2025-03-25 02:44:58,572 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4943\n",
            "2025-03-25 02:45:05,770 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4033\n",
            "2025-03-25 02:45:13,146 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4093\n",
            "2025-03-25 02:45:20,459 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4138\n",
            "2025-03-25 02:45:27,641 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4619\n",
            "2025-03-25 02:45:35,055 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5175\n",
            "2025-03-25 02:45:42,449 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4322\n",
            "2025-03-25 02:45:49,704 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4057\n",
            "2025-03-25 02:45:57,052 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4223\n",
            "2025-03-25 02:46:04,247 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4559\n",
            "2025-03-25 02:46:11,443 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4605\n",
            "2025-03-25 02:46:18,646 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4457\n",
            "2025-03-25 02:46:24,059 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3316\n",
            "2025-03-25 02:46:24,670 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1148\n",
            "2025-03-25 02:46:24,671 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:47:18,646 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4425, Val Loss: 0.3847, Val Acc: 0.8589\n",
            "2025-03-25 02:47:18,978 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:47:18,979 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-25 02:47:24,563 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3156\n",
            "2025-03-25 02:47:31,818 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4680\n",
            "2025-03-25 02:47:39,204 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4446\n",
            "2025-03-25 02:47:46,610 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3813\n",
            "2025-03-25 02:47:53,687 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4474\n",
            "2025-03-25 02:48:00,861 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4094\n",
            "2025-03-25 02:48:08,382 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4186\n",
            "2025-03-25 02:48:15,780 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4183\n",
            "2025-03-25 02:48:23,016 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4459\n",
            "2025-03-25 02:48:30,317 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4463\n",
            "2025-03-25 02:48:37,680 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4371\n",
            "2025-03-25 02:48:45,174 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4091\n",
            "2025-03-25 02:48:52,309 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4808\n",
            "2025-03-25 02:48:59,374 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3937\n",
            "2025-03-25 02:49:06,788 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4264\n",
            "2025-03-25 02:49:14,093 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4353\n",
            "2025-03-25 02:49:21,261 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3997\n",
            "2025-03-25 02:49:28,573 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4169\n",
            "2025-03-25 02:49:35,629 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4079\n",
            "2025-03-25 02:49:42,926 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3948\n",
            "2025-03-25 02:49:50,275 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3912\n",
            "2025-03-25 02:49:57,571 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4117\n",
            "2025-03-25 02:50:04,764 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3314\n",
            "2025-03-25 02:50:11,969 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4235\n",
            "2025-03-25 02:50:19,160 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5262\n",
            "2025-03-25 02:50:26,473 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4291\n",
            "2025-03-25 02:50:33,551 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4418\n",
            "2025-03-25 02:50:40,959 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4862\n",
            "2025-03-25 02:50:48,252 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3658\n",
            "2025-03-25 02:50:55,464 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5099\n",
            "2025-03-25 02:51:02,744 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3923\n",
            "2025-03-25 02:51:09,758 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4820\n",
            "2025-03-25 02:51:17,019 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3837\n",
            "2025-03-25 02:51:22,350 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2805\n",
            "2025-03-25 02:51:22,908 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1119\n",
            "2025-03-25 02:51:22,909 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:52:16,644 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4256, Val Loss: 0.3617, Val Acc: 0.8743\n",
            "2025-03-25 02:52:16,941 - INFO - [TRAIN INFO] Best Model Saved for Fold 5\n",
            "2025-03-25 02:52:16,942 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-25 02:52:22,466 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2559\n",
            "2025-03-25 02:52:29,874 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4196\n",
            "2025-03-25 02:52:37,122 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3888\n",
            "2025-03-25 02:52:44,313 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3607\n",
            "2025-03-25 02:52:51,576 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3976\n",
            "2025-03-25 02:52:58,932 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3830\n",
            "2025-03-25 02:53:06,200 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3957\n",
            "2025-03-25 02:53:13,511 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3954\n",
            "2025-03-25 02:53:20,739 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4349\n",
            "2025-03-25 02:53:28,113 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3519\n",
            "2025-03-25 02:53:35,338 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3814\n",
            "2025-03-25 02:53:42,874 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4086\n",
            "2025-03-25 02:53:50,051 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3773\n",
            "2025-03-25 02:53:57,503 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3742\n",
            "2025-03-25 02:54:04,700 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3803\n",
            "2025-03-25 02:54:11,894 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4326\n",
            "2025-03-25 02:54:19,109 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4071\n",
            "2025-03-25 02:54:26,473 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3319\n",
            "2025-03-25 02:54:33,501 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3666\n",
            "2025-03-25 02:54:40,771 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4011\n",
            "2025-03-25 02:54:47,932 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3717\n",
            "2025-03-25 02:54:55,282 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3768\n",
            "2025-03-25 02:55:02,606 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4460\n",
            "2025-03-25 02:55:09,750 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4182\n",
            "2025-03-25 02:55:17,052 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3894\n",
            "2025-03-25 02:55:24,232 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3833\n",
            "2025-03-25 02:55:31,673 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4413\n",
            "2025-03-25 02:55:38,864 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4218\n",
            "2025-03-25 02:55:46,067 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3683\n",
            "2025-03-25 02:55:53,415 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4510\n",
            "2025-03-25 02:56:00,635 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4114\n",
            "2025-03-25 02:56:07,765 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4649\n",
            "2025-03-25 02:56:14,982 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4151\n",
            "2025-03-25 02:56:20,531 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2991\n",
            "2025-03-25 02:56:21,075 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1030\n",
            "2025-03-25 02:56:21,075 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 02:57:15,015 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3972, Val Loss: 0.3675, Val Acc: 0.8748\n",
            "2025-03-25 02:57:15,016 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-25 02:57:20,460 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2932\n",
            "2025-03-25 02:57:27,994 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3361\n",
            "2025-03-25 02:57:35,226 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3602\n",
            "2025-03-25 02:57:42,431 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3465\n",
            "2025-03-25 02:57:49,628 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3900\n",
            "2025-03-25 02:57:56,969 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3272\n",
            "2025-03-25 02:58:04,088 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3733\n",
            "2025-03-25 02:58:11,359 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3634\n",
            "2025-03-25 02:58:18,443 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3623\n",
            "2025-03-25 02:58:25,818 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3630\n",
            "2025-03-25 02:58:32,993 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3289\n",
            "2025-03-25 02:58:40,077 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3291\n",
            "2025-03-25 02:58:47,169 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3400\n",
            "2025-03-25 02:58:54,616 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3201\n",
            "2025-03-25 02:59:01,891 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3233\n",
            "2025-03-25 02:59:09,080 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3508\n",
            "2025-03-25 02:59:16,393 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3429\n",
            "2025-03-25 02:59:23,535 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3938\n",
            "2025-03-25 02:59:30,664 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3448\n",
            "2025-03-25 02:59:37,994 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3579\n",
            "2025-03-25 02:59:45,124 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3370\n",
            "2025-03-25 02:59:52,589 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3783\n",
            "2025-03-25 02:59:59,779 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3729\n",
            "2025-03-25 03:00:07,123 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3956\n",
            "2025-03-25 03:00:14,553 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3984\n",
            "2025-03-25 03:00:21,930 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4162\n",
            "2025-03-25 03:00:29,210 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3840\n",
            "2025-03-25 03:00:36,381 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3873\n",
            "2025-03-25 03:00:43,564 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3543\n",
            "2025-03-25 03:00:50,815 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3993\n",
            "2025-03-25 03:00:58,084 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3924\n",
            "2025-03-25 03:01:05,564 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3441\n",
            "2025-03-25 03:01:12,866 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3737\n",
            "2025-03-25 03:01:18,253 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3177\n",
            "2025-03-25 03:01:18,806 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1057\n",
            "2025-03-25 03:01:18,806 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:02:12,763 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3646, Val Loss: 0.4137, Val Acc: 0.8673\n",
            "2025-03-25 03:02:12,764 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 03:02:18,209 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2917\n",
            "2025-03-25 03:02:25,536 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3627\n",
            "2025-03-25 03:02:32,742 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3455\n",
            "2025-03-25 03:02:40,080 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3764\n",
            "2025-03-25 03:02:47,243 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3492\n",
            "2025-03-25 03:02:54,465 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3840\n",
            "2025-03-25 03:03:01,543 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3621\n",
            "2025-03-25 03:03:09,080 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-25 03:03:16,269 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3187\n",
            "2025-03-25 03:03:23,520 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3318\n",
            "2025-03-25 03:03:30,882 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3583\n",
            "2025-03-25 03:03:38,494 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3846\n",
            "2025-03-25 03:03:45,635 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3429\n",
            "2025-03-25 03:03:52,821 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3385\n",
            "2025-03-25 03:04:00,278 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3189\n",
            "2025-03-25 03:04:07,507 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3412\n",
            "2025-03-25 03:04:14,727 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3572\n",
            "2025-03-25 03:04:21,918 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3738\n",
            "2025-03-25 03:04:29,171 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3877\n",
            "2025-03-25 03:04:36,403 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3336\n",
            "2025-03-25 03:04:43,876 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3757\n",
            "2025-03-25 03:04:51,070 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3925\n",
            "2025-03-25 03:04:58,494 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3817\n",
            "2025-03-25 03:05:05,751 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3524\n",
            "2025-03-25 03:05:13,267 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3775\n",
            "2025-03-25 03:05:20,399 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3339\n",
            "2025-03-25 03:05:27,599 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3623\n",
            "2025-03-25 03:05:34,881 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3530\n",
            "2025-03-25 03:05:41,968 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3336\n",
            "2025-03-25 03:05:49,142 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3877\n",
            "2025-03-25 03:05:56,467 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3640\n",
            "2025-03-25 03:06:03,541 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3626\n",
            "2025-03-25 03:06:10,777 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3127\n",
            "2025-03-25 03:06:16,460 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3222\n",
            "2025-03-25 03:06:17,019 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1293\n",
            "2025-03-25 03:06:17,019 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:07:11,615 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3587, Val Loss: 0.3857, Val Acc: 0.8692\n",
            "2025-03-25 03:07:11,615 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 03:07:17,175 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-25 03:07:24,628 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3250\n",
            "2025-03-25 03:07:31,706 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3323\n",
            "2025-03-25 03:07:38,972 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3732\n",
            "2025-03-25 03:07:46,183 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3178\n",
            "2025-03-25 03:07:53,387 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3043\n",
            "2025-03-25 03:08:00,576 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3322\n",
            "2025-03-25 03:08:07,720 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2760\n",
            "2025-03-25 03:08:15,208 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3074\n",
            "2025-03-25 03:08:22,433 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3941\n",
            "2025-03-25 03:08:29,659 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3925\n",
            "2025-03-25 03:08:36,868 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3651\n",
            "2025-03-25 03:08:44,153 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3699\n",
            "2025-03-25 03:08:51,612 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3753\n",
            "2025-03-25 03:08:58,769 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3311\n",
            "2025-03-25 03:09:06,212 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3172\n",
            "2025-03-25 03:09:13,423 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3311\n",
            "2025-03-25 03:09:20,620 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3089\n",
            "2025-03-25 03:09:27,836 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3169\n",
            "2025-03-25 03:09:35,184 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3324\n",
            "2025-03-25 03:09:42,461 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3214\n",
            "2025-03-25 03:09:49,742 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2901\n",
            "2025-03-25 03:09:56,805 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3364\n",
            "2025-03-25 03:10:04,121 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3296\n",
            "2025-03-25 03:10:11,200 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3468\n",
            "2025-03-25 03:10:18,604 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3118\n",
            "2025-03-25 03:10:25,845 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3730\n",
            "2025-03-25 03:10:33,169 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3295\n",
            "2025-03-25 03:10:40,545 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3103\n",
            "2025-03-25 03:10:47,577 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-25 03:10:54,943 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3108\n",
            "2025-03-25 03:11:02,132 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3559\n",
            "2025-03-25 03:11:09,228 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3598\n",
            "2025-03-25 03:11:14,709 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2576\n",
            "2025-03-25 03:11:15,309 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1569\n",
            "2025-03-25 03:11:15,309 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:12:09,182 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3361, Val Loss: 0.4079, Val Acc: 0.8673\n",
            "2025-03-25 03:12:09,182 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 03:12:14,649 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2279\n",
            "2025-03-25 03:12:21,869 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-25 03:12:29,209 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3231\n",
            "2025-03-25 03:12:36,547 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-25 03:12:43,705 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3168\n",
            "2025-03-25 03:12:50,899 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3483\n",
            "2025-03-25 03:12:58,264 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3476\n",
            "2025-03-25 03:13:05,662 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3211\n",
            "2025-03-25 03:13:13,318 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3037\n",
            "2025-03-25 03:13:20,531 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3290\n",
            "2025-03-25 03:13:27,742 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3147\n",
            "2025-03-25 03:13:34,940 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2851\n",
            "2025-03-25 03:13:42,128 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3183\n",
            "2025-03-25 03:13:49,333 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2974\n",
            "2025-03-25 03:13:56,705 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2966\n",
            "2025-03-25 03:14:03,886 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2842\n",
            "2025-03-25 03:14:10,996 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3045\n",
            "2025-03-25 03:14:18,204 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3011\n",
            "2025-03-25 03:14:25,390 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2603\n",
            "2025-03-25 03:14:32,537 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3041\n",
            "2025-03-25 03:14:40,072 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2887\n",
            "2025-03-25 03:14:47,188 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3195\n",
            "2025-03-25 03:14:54,357 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3025\n",
            "2025-03-25 03:15:01,712 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3267\n",
            "2025-03-25 03:15:08,833 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-25 03:15:16,014 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3213\n",
            "2025-03-25 03:15:23,291 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3180\n",
            "2025-03-25 03:15:30,609 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2923\n",
            "2025-03-25 03:15:37,891 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-25 03:15:45,079 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3309\n",
            "2025-03-25 03:15:52,477 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-25 03:15:59,648 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3113\n",
            "2025-03-25 03:16:06,821 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3292\n",
            "2025-03-25 03:16:12,270 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2372\n",
            "2025-03-25 03:16:12,815 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0772\n",
            "2025-03-25 03:16:12,816 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:17:06,498 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3082, Val Loss: 0.3687, Val Acc: 0.8818\n",
            "2025-03-25 03:17:06,498 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 03:17:11,955 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2268\n",
            "2025-03-25 03:17:19,267 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3386\n",
            "2025-03-25 03:17:26,459 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3134\n",
            "2025-03-25 03:17:33,838 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2822\n",
            "2025-03-25 03:17:41,008 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2833\n",
            "2025-03-25 03:17:48,269 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 03:17:55,797 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2926\n",
            "2025-03-25 03:18:03,205 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2907\n",
            "2025-03-25 03:18:10,426 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2687\n",
            "2025-03-25 03:18:17,620 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3172\n",
            "2025-03-25 03:18:24,764 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2908\n",
            "2025-03-25 03:18:31,987 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2834\n",
            "2025-03-25 03:18:39,134 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2899\n",
            "2025-03-25 03:18:46,588 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2665\n",
            "2025-03-25 03:18:53,719 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2814\n",
            "2025-03-25 03:19:00,919 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2890\n",
            "2025-03-25 03:19:08,406 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2691\n",
            "2025-03-25 03:19:15,626 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2765\n",
            "2025-03-25 03:19:22,833 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2907\n",
            "2025-03-25 03:19:30,055 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2914\n",
            "2025-03-25 03:19:37,280 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2748\n",
            "2025-03-25 03:19:44,626 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2943\n",
            "2025-03-25 03:19:51,718 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2840\n",
            "2025-03-25 03:19:58,922 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2783\n",
            "2025-03-25 03:20:06,092 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-25 03:20:13,417 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2710\n",
            "2025-03-25 03:20:20,673 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2820\n",
            "2025-03-25 03:20:28,049 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3267\n",
            "2025-03-25 03:20:35,359 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2802\n",
            "2025-03-25 03:20:42,605 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3008\n",
            "2025-03-25 03:20:49,794 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2808\n",
            "2025-03-25 03:20:56,976 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2643\n",
            "2025-03-25 03:21:04,264 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3056\n",
            "2025-03-25 03:21:09,713 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2198\n",
            "2025-03-25 03:21:10,268 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0703\n",
            "2025-03-25 03:21:10,269 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:22:04,107 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2878, Val Loss: 0.3795, Val Acc: 0.8790\n",
            "2025-03-25 03:22:04,108 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 03:22:09,685 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1742\n",
            "2025-03-25 03:22:16,737 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-25 03:22:24,175 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3076\n",
            "2025-03-25 03:22:31,240 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2658\n",
            "2025-03-25 03:22:38,486 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2773\n",
            "2025-03-25 03:22:45,619 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2642\n",
            "2025-03-25 03:22:52,859 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2567\n",
            "2025-03-25 03:23:00,018 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2900\n",
            "2025-03-25 03:23:07,136 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2799\n",
            "2025-03-25 03:23:14,355 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3134\n",
            "2025-03-25 03:23:21,611 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3022\n",
            "2025-03-25 03:23:28,955 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2635\n",
            "2025-03-25 03:23:36,289 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2669\n",
            "2025-03-25 03:23:43,412 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2706\n",
            "2025-03-25 03:23:50,741 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3073\n",
            "2025-03-25 03:23:57,850 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2834\n",
            "2025-03-25 03:24:05,056 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2706\n",
            "2025-03-25 03:24:12,340 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2605\n",
            "2025-03-25 03:24:19,470 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3023\n",
            "2025-03-25 03:24:26,588 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-25 03:24:33,936 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2870\n",
            "2025-03-25 03:24:40,986 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2833\n",
            "2025-03-25 03:24:48,218 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2795\n",
            "2025-03-25 03:24:55,515 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2826\n",
            "2025-03-25 03:25:02,604 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2829\n",
            "2025-03-25 03:25:09,724 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2873\n",
            "2025-03-25 03:25:17,118 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2816\n",
            "2025-03-25 03:25:24,248 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3207\n",
            "2025-03-25 03:25:31,383 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2515\n",
            "2025-03-25 03:25:38,719 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-25 03:25:45,824 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3060\n",
            "2025-03-25 03:25:52,952 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2735\n",
            "2025-03-25 03:26:00,297 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2524\n",
            "2025-03-25 03:26:05,869 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1987\n",
            "2025-03-25 03:26:06,441 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0673\n",
            "2025-03-25 03:26:06,442 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:27:00,306 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2792, Val Loss: 0.3741, Val Acc: 0.8785\n",
            "2025-03-25 03:27:00,307 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-25 03:27:05,778 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1864\n",
            "2025-03-25 03:27:13,061 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2491\n",
            "2025-03-25 03:27:20,266 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2708\n",
            "2025-03-25 03:27:27,472 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2484\n",
            "2025-03-25 03:27:34,843 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3127\n",
            "2025-03-25 03:27:42,051 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3029\n",
            "2025-03-25 03:27:49,195 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2606\n",
            "2025-03-25 03:27:56,318 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2538\n",
            "2025-03-25 03:28:03,582 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2597\n",
            "2025-03-25 03:28:11,040 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2662\n",
            "2025-03-25 03:28:18,220 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-25 03:28:25,655 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-25 03:28:33,020 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2604\n",
            "2025-03-25 03:28:40,298 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-25 03:28:47,531 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2427\n",
            "2025-03-25 03:28:55,057 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2775\n",
            "2025-03-25 03:29:02,401 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2660\n",
            "2025-03-25 03:29:09,841 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2687\n",
            "2025-03-25 03:29:16,942 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-25 03:29:24,173 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-25 03:29:31,561 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2794\n",
            "2025-03-25 03:29:38,844 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-25 03:29:46,039 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2757\n",
            "2025-03-25 03:29:53,372 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-25 03:30:00,510 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2529\n",
            "2025-03-25 03:30:07,804 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2745\n",
            "2025-03-25 03:30:15,029 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2825\n",
            "2025-03-25 03:30:22,124 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2601\n",
            "2025-03-25 03:30:29,221 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3045\n",
            "2025-03-25 03:30:36,596 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-25 03:30:43,761 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2686\n",
            "2025-03-25 03:30:50,996 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2803\n",
            "2025-03-25 03:30:58,407 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2651\n",
            "2025-03-25 03:31:03,818 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2180\n",
            "2025-03-25 03:31:04,374 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0746\n",
            "2025-03-25 03:31:04,375 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:31:58,263 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2705, Val Loss: 0.3814, Val Acc: 0.8804\n",
            "2025-03-25 03:31:58,263 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-25 03:32:03,967 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2024\n",
            "2025-03-25 03:32:11,187 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2554\n",
            "2025-03-25 03:32:18,567 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2700\n",
            "2025-03-25 03:32:25,779 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2807\n",
            "2025-03-25 03:32:32,960 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-25 03:32:40,063 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2605\n",
            "2025-03-25 03:32:47,257 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2533\n",
            "2025-03-25 03:32:54,509 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-25 03:33:01,627 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2406\n",
            "2025-03-25 03:33:08,759 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2586\n",
            "2025-03-25 03:33:16,052 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2810\n",
            "2025-03-25 03:33:23,266 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2663\n",
            "2025-03-25 03:33:30,514 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2776\n",
            "2025-03-25 03:33:37,968 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2921\n",
            "2025-03-25 03:33:45,096 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2562\n",
            "2025-03-25 03:33:52,274 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2993\n",
            "2025-03-25 03:33:59,612 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2741\n",
            "2025-03-25 03:34:06,956 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2740\n",
            "2025-03-25 03:34:14,155 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2711\n",
            "2025-03-25 03:34:21,382 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2866\n",
            "2025-03-25 03:34:28,740 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2383\n",
            "2025-03-25 03:34:35,842 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2572\n",
            "2025-03-25 03:34:43,034 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2578\n",
            "2025-03-25 03:34:50,218 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2351\n",
            "2025-03-25 03:34:57,437 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2697\n",
            "2025-03-25 03:35:04,730 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 03:35:12,076 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2464\n",
            "2025-03-25 03:35:19,240 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2867\n",
            "2025-03-25 03:35:26,524 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2429\n",
            "2025-03-25 03:35:33,645 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2506\n",
            "2025-03-25 03:35:40,831 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2685\n",
            "2025-03-25 03:35:48,518 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2672\n",
            "2025-03-25 03:35:55,839 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2548\n",
            "2025-03-25 03:36:01,200 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1900\n",
            "2025-03-25 03:36:01,754 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0695\n",
            "2025-03-25 03:36:01,755 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:36:55,554 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2644, Val Loss: 0.3832, Val Acc: 0.8808\n",
            "2025-03-25 03:36:55,554 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-25 03:37:00,977 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1777\n",
            "2025-03-25 03:37:08,163 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2492\n",
            "2025-03-25 03:37:15,654 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2505\n",
            "2025-03-25 03:37:22,963 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2485\n",
            "2025-03-25 03:37:30,286 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2599\n",
            "2025-03-25 03:37:37,506 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2461\n",
            "2025-03-25 03:37:44,808 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2510\n",
            "2025-03-25 03:37:52,280 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2565\n",
            "2025-03-25 03:37:59,378 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2537\n",
            "2025-03-25 03:38:06,601 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2667\n",
            "2025-03-25 03:38:13,785 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2610\n",
            "2025-03-25 03:38:20,908 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2515\n",
            "2025-03-25 03:38:28,253 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2743\n",
            "2025-03-25 03:38:35,514 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2552\n",
            "2025-03-25 03:38:42,864 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2564\n",
            "2025-03-25 03:38:50,068 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2620\n",
            "2025-03-25 03:38:57,301 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2637\n",
            "2025-03-25 03:39:04,670 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2394\n",
            "2025-03-25 03:39:11,734 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2476\n",
            "2025-03-25 03:39:18,960 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2513\n",
            "2025-03-25 03:39:26,182 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2479\n",
            "2025-03-25 03:39:33,364 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2782\n",
            "2025-03-25 03:39:40,841 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2609\n",
            "2025-03-25 03:39:48,038 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2694\n",
            "2025-03-25 03:39:55,454 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2671\n",
            "2025-03-25 03:40:02,643 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2544\n",
            "2025-03-25 03:40:09,849 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2545\n",
            "2025-03-25 03:40:17,037 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2571\n",
            "2025-03-25 03:40:24,217 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2522\n",
            "2025-03-25 03:40:31,432 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2677\n",
            "2025-03-25 03:40:38,636 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2574\n",
            "2025-03-25 03:40:45,884 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2581\n",
            "2025-03-25 03:40:53,409 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2779\n",
            "2025-03-25 03:40:58,981 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2184\n",
            "2025-03-25 03:40:59,551 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0955\n",
            "2025-03-25 03:40:59,551 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 03:41:53,257 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2589, Val Loss: 0.3856, Val Acc: 0.8766\n",
            "2025-03-25 03:41:53,257 - INFO - [TRAIN INFO] Early stopping at epoch 19 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 03:41:53,258 - INFO - [TRAIN INFO] Total Time: 5672.38s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>▁▁▁▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>learning_rate_classifier</td><td>▁▂▃▄▅▆▇█████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_fusion</td><td>▁▂▃▄▅▆▇█████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_image</td><td>▁▂▃▄▅▆▇█████▃▃▃▃▁▁▁</td></tr><tr><td>learning_rate_text</td><td>▁▂▃▄▅▆▇█████▃▃▃▃▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_val_loss_diff</td><td>█▆▆▅▅▄▄▄▄▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇██▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.25893</td></tr><tr><td>train_val_loss_diff</td><td>-0.12665</td></tr><tr><td>val_accuracy</td><td>0.87663</td></tr><tr><td>val_loss</td><td>0.38557</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_attention_gated_fusion_fold_5</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/nl640e8f' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/nl640e8f</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250325_020720-nl640e8f\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 03:41:55,463 - INFO - [TRAIN INFO] Fold 5 Training Complete at epoch 19. Total Time: 5674.58s\n",
            "2025-03-25 03:41:55,477 - INFO - [K-FOLD INFO] Fold 5 completed in 5675.94 seconds\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "logging.info(\"[K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "\n",
        "    \n",
        "    fold_start_time = time.time()  # Start timing for this fold\n",
        "    logging.info(f\"[K-FOLD INFO] ============================== Fold {fold+1}/{K_FOLDS} ==============================\")\n",
        "\n",
        "    # Get train and validation subsets\n",
        "    train_texts_fold = train_texts[train_idx]\n",
        "    val_texts_fold = train_texts[val_idx]\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "    train_image_paths_fold = train_image_paths[train_idx]\n",
        "    val_image_paths_fold = train_image_paths[val_idx]\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train Samples: {len(train_texts_fold)}\")\n",
        "    logging.info(f\"   Validation Samples: {len(val_texts_fold)}\")\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_image_dataset = ImageDataset(train_image_paths_fold, train_labels_fold, transform[\"train\"])\n",
        "    val_image_dataset = ImageDataset(val_image_paths_fold, val_labels_fold, transform[\"val\"])\n",
        "    \n",
        "    train_text_dataset = CustomTextDataset(train_texts_fold, train_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "    val_text_dataset = CustomTextDataset(val_texts_fold, val_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # Create multimodal datasets\n",
        "    train_multimodal_dataset = MultimodalDataset(train_image_dataset, train_text_dataset)\n",
        "    val_multimodal_dataset = MultimodalDataset(val_image_dataset, val_text_dataset)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Created multimodal datasets for Fold {fold+1}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    dataloaders = {\n",
        "        \"train_loader\": DataLoader(train_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=True),\n",
        "        \"val_loader\": DataLoader(val_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    }\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] DataLoaders initialized for Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train batches: {len(dataloaders['train_loader'])}, Validation batches: {len(dataloaders['val_loader'])}\")\n",
        "\n",
        "    # Initialize model, optimizer, and criterion\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Model initialized on {device} for Fold {fold+1}\")\n",
        "\n",
        "    # Define Optimizer using AdamW\n",
        "    optimizer = optim.AdamW([\n",
        "        {\"params\": model.image_model.features[-3:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_IMAGE, \"weight_decay\": WEIGHT_DECAY_IMAGE},  # Unfrozen EfficientNet layer\n",
        "        {\"params\": model.text_model.transformer.layer[-2:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_TEXT, \"weight_decay\": WEIGHT_DECAY_TEXT},  # Unfrozen DistilBERT layer\n",
        "        {\"params\": model.image_fc.parameters(), \"lr\": LEARNING_RATE_IMAGE, \"weight_decay\": 0}, \n",
        "        {\"params\": model.text_fc.parameters(), \"lr\": LEARNING_RATE_TEXT, \"weight_decay\": 0},\n",
        "        {\"params\": model.fusion_fc.parameters(), \"lr\": LEARNING_RATE_FUSION, \"weight_decay\": WEIGHT_DECAY_FUSION},  \n",
        "        {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER, \"weight_decay\": WEIGHT_DECAY_CLASSIFIER}  \n",
        "    ], betas=(0.9, 0.999), eps=1e-8)  # Default AdamW betas and eps\n",
        "\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Optimizer initialized for Fold {fold+1}:\")\n",
        "    # Define Loss Function\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING_PREDICTION) \n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Loss function initialized for Fold {fold+1}\")\n",
        "\n",
        "    # Train model for this fold\n",
        "    train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True)\n",
        "\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Measure Fold Time\n",
        "    fold_time = time.time() - fold_start_time\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1} completed in {fold_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for fold in range(K_FOLDS):\n",
        "#     logging.info(f\"\\n[TEST INFO] Evaluating Fold {fold + 1} on Test Set...\")\n",
        "\n",
        "#     # Load best model for the fold\n",
        "#     model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "#     model_path = f\"best_model_fold_{fold + 1}.pth\"\n",
        "    \n",
        "#     try:\n",
        "#         model.load_state_dict(torch.load(model_path))\n",
        "#         logging.info(f\"[TEST INFO] Loaded best model for Fold {fold + 1} from {model_path}\")\n",
        "#     except FileNotFoundError:\n",
        "#         logging.error(f\"[ERROR] Model file {model_path} not found! Skipping Fold {fold + 1} evaluation.\")\n",
        "#         continue  # Skip to the next fold if model file is missing\n",
        "\n",
        "#     model.eval()  # Set to evaluation mode\n",
        "\n",
        "#     # Evaluate model on test data\n",
        "#     test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "\n",
        "#     # Log test set performance for the fold\n",
        "#     logging.info(f\"[TEST INFO] Fold {fold + 1} Test Performance:\")\n",
        "#     logging.info(f\"   Test Loss: {test_loss:.4f}\")\n",
        "#     logging.info(f\"   Test Accuracy: {test_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "enel645_torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
