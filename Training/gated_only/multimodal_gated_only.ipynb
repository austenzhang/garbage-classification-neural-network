{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLOOi0GQM5B"
      },
      "source": [
        "## Garbage Classification Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
        "from torchvision.models.efficientnet import EfficientNet_B0_Weights\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import wandb\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import time\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "NOTES = '''\n",
        "'''\n",
        "\n",
        "# ========================================= GLOBAL CONFIGURATION ================================================\n",
        "# Data Directories\n",
        "DATA_DIR = r\"C:\\NN Data\\garbage_data\\kfold_garbage_data\"\n",
        "CLASSES = [\"Black\", \"Blue\", \"Green\", \"TTR\"]\n",
        "\n",
        "# ========================================= Experiment Settings =========================================\n",
        "WANDB_RUN_NAME = \"experiment_multimodal_gated_only\"\n",
        "MODEL_NAME = \"experiment_multimodal_gated_only\"\n",
        "\n",
        "# ========================================= Data Settings =========================================\n",
        "IMAGE_SIZE = (224, 224)  # Input image size for EfficientNetV2-S\n",
        "NUM_CLASSES = 4  # Number of output classes for classification\n",
        "MAX_LEN = 40  # Maximum token length for DistilBERT tokenizer\n",
        "TEST_SIZE = 0.2  # Test dataset size split\n",
        "K_FOLDS = 5  # Number of folds for stratified k-fold cross-validation\n",
        "\n",
        "# ========================================= Training Hyperparameters =========================================\n",
        "BATCH_SIZE = 64  # Number of samples per batch\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "EPOCHS = 50  # Maximum number of training epochs\n",
        "DROPOUT_IMAGE = 0.2 # Reduce from 0.3\n",
        "DROPOUT_TEXT = 0.1 # Reduce from 0.2\n",
        "DROPOUT_FUSION = 0.2 \n",
        "DROPOUT_CLASSIFIER = 0.1\n",
        "PATIENCE = 10  # Number of epochs to wait before early stopping\n",
        "CONVERGENCE_THRESHOLD = 0.001  # Minimum improvement in validation loss to continue training\n",
        "\n",
        "# ========================================= Optimization Settings =========================================\n",
        "OPTIMIZER = \"AdamW\"\n",
        "LR_SCHEDULING_FACTOR = 0.3\n",
        "LEARNING_RATE_UNFREEZE_IMAGE = 1e-5\n",
        "LEARNING_RATE_UNFREEZE_TEXT = 1e-5\n",
        "LEARNING_RATE_FUSION = 1e-3\n",
        "LEARNING_RATE_CLASSIFIER = 5e-3\n",
        "LEARNING_RATE_IMAGE = 0.001 # # EfficientNetB0\n",
        "LEARNING_RATE_TEXT = 0.00002 # DistilBERT Uncased\n",
        "WEIGHT_DECAY_TEXT = 1e-3  # Reduce from 1e-2\n",
        "WEIGHT_DECAY_IMAGE = 1e-4  # Reduce from 1e-3\n",
        "WEIGHT_DECAY_FUSION = 4e-4 \n",
        "WEIGHT_DECAY_CLASSIFIER = 1e-3  # Reduce from 1e-4\n",
        "LABEL_SMOOTHING_PREDICTION = 0.05 # Reduce from 0.1\n",
        "\n",
        "# ========================================= System Settings =========================================\n",
        "NUM_WORKERS = 4  # Dataloader parallelization\n",
        "\n",
        "# Wandb Configuration\n",
        "WANDB_CONFIG = {\n",
        "    \"entity\": \"shcau-university-of-calgary-in-alberta\",\n",
        "    \"project\": \"transfer_learning_garbage\",\n",
        "    \"name\": WANDB_RUN_NAME,\n",
        "    \"tags\": [\"distilBERT\", \"efficientnet\", \"CVPR_2024_dataset\"],\n",
        "    \"notes\": NOTES,\n",
        "    \"config\": {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"dataset\": \"CVPR_2024_dataset\",\n",
        "        \"image_size\": IMAGE_SIZE,\n",
        "        \"num_workers\": NUM_WORKERS,\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"max_len\": MAX_LEN,\n",
        "        \"learning_rate_image\": LEARNING_RATE_IMAGE,\n",
        "        \"learning_rate_text\": LEARNING_RATE_TEXT,\n",
        "        \"learning_rate_fusion\": LEARNING_RATE_FUSION,\n",
        "        \"learning_rate_classifier\": LEARNING_RATE_CLASSIFIER,\n",
        "        \"learning_rate_unfreeze_image\": LEARNING_RATE_UNFREEZE_IMAGE, # learning rate for unfrozen EfficientNet layers\n",
        "        \"learning_rate_unfreeze_text\": LEARNING_RATE_UNFREEZE_TEXT, # learning rate for unfrozen DistilBERT layers\n",
        "        \"dropout_image\": DROPOUT_IMAGE,\n",
        "        \"dropout_text\": DROPOUT_TEXT,\n",
        "        \"dropout_classifier\": DROPOUT_CLASSIFIER,\n",
        "        \"convergence_threshold\": CONVERGENCE_THRESHOLD,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"weight_decay_text\": WEIGHT_DECAY_TEXT,\n",
        "        \"weight_decay_image\": WEIGHT_DECAY_IMAGE,\n",
        "        \"weight_decay_classifier\": WEIGHT_DECAY_CLASSIFIER,\n",
        "        \"label_smoothing_prediction\": LABEL_SMOOTHING_PREDICTION,\n",
        "        \"optimizer\": OPTIMIZER \n",
        "    },\n",
        "    \"job_type\": \"train\",\n",
        "    \"resume\": \"allow\",\n",
        "}\n",
        "\n",
        "# Normalization Stats\n",
        "NORMALIZATION_STATS = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOG_FILE = \"experiment_multimodal_gated_only.txt\"  # Log file name\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Log everything (INFO and above)\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(LOG_FILE, mode='w'),  # Overwrite log file on each run\n",
        "        logging.StreamHandler(sys.stdout)  # Print log messages to console too\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:16:31,862 - INFO - [CONFIG] ============================== Experiment Configuration ==============================\n",
            "2025-03-25 19:16:31,862 - INFO - [CONFIG] Experiment Name: experiment_multimodal_gated_only\n",
            "2025-03-25 19:16:31,863 - INFO - [CONFIG] Entity: shcau-university-of-calgary-in-alberta\n",
            "2025-03-25 19:16:31,864 - INFO - [CONFIG] Project: transfer_learning_garbage\n",
            "2025-03-25 19:16:31,865 - INFO - [CONFIG] Tags: distilBERT, efficientnet, CVPR_2024_dataset\n",
            "2025-03-25 19:16:31,865 - INFO - [CONFIG] Notes: \n",
            "\n",
            "2025-03-25 19:16:31,866 - INFO - [CONFIG] Job Type: train\n",
            "2025-03-25 19:16:31,867 - INFO - [CONFIG] Resume: allow\n",
            "2025-03-25 19:16:31,868 - INFO - [CONFIG] ------------------------------ Hyperparameters ------------------------------\n",
            "2025-03-25 19:16:31,868 - INFO - [CONFIG] epochs: 50\n",
            "2025-03-25 19:16:31,869 - INFO - [CONFIG] batch_size: 64\n",
            "2025-03-25 19:16:31,870 - INFO - [CONFIG] dataset: CVPR_2024_dataset\n",
            "2025-03-25 19:16:31,870 - INFO - [CONFIG] image_size: (224, 224)\n",
            "2025-03-25 19:16:31,871 - INFO - [CONFIG] num_workers: 4\n",
            "2025-03-25 19:16:31,872 - INFO - [CONFIG] num_classes: 4\n",
            "2025-03-25 19:16:31,873 - INFO - [CONFIG] max_len: 40\n",
            "2025-03-25 19:16:31,873 - INFO - [CONFIG] learning_rate_image: 0.001\n",
            "2025-03-25 19:16:31,874 - INFO - [CONFIG] learning_rate_text: 2e-05\n",
            "2025-03-25 19:16:31,874 - INFO - [CONFIG] learning_rate_fusion: 0.001\n",
            "2025-03-25 19:16:31,875 - INFO - [CONFIG] learning_rate_classifier: 0.005\n",
            "2025-03-25 19:16:31,876 - INFO - [CONFIG] learning_rate_unfreeze_image: 1e-05\n",
            "2025-03-25 19:16:31,877 - INFO - [CONFIG] learning_rate_unfreeze_text: 1e-05\n",
            "2025-03-25 19:16:31,877 - INFO - [CONFIG] dropout_image: 0.2\n",
            "2025-03-25 19:16:31,878 - INFO - [CONFIG] dropout_text: 0.1\n",
            "2025-03-25 19:16:31,879 - INFO - [CONFIG] dropout_classifier: 0.1\n",
            "2025-03-25 19:16:31,879 - INFO - [CONFIG] convergence_threshold: 0.001\n",
            "2025-03-25 19:16:31,880 - INFO - [CONFIG] patience: 10\n",
            "2025-03-25 19:16:31,880 - INFO - [CONFIG] weight_decay_text: 0.001\n",
            "2025-03-25 19:16:31,881 - INFO - [CONFIG] weight_decay_image: 0.0001\n",
            "2025-03-25 19:16:31,882 - INFO - [CONFIG] weight_decay_classifier: 0.001\n",
            "2025-03-25 19:16:31,882 - INFO - [CONFIG] label_smoothing_prediction: 0.05\n",
            "2025-03-25 19:16:31,883 - INFO - [CONFIG] optimizer: AdamW\n",
            "2025-03-25 19:16:31,884 - INFO - [CONFIG] =============================================================================\n"
          ]
        }
      ],
      "source": [
        "# Log the configuration\n",
        "logging.info(\"[CONFIG] ============================== Experiment Configuration ==============================\")\n",
        "\n",
        "# Log top-level keys\n",
        "logging.info(f\"[CONFIG] Experiment Name: {WANDB_CONFIG['name']}\")\n",
        "logging.info(f\"[CONFIG] Entity: {WANDB_CONFIG['entity']}\")\n",
        "logging.info(f\"[CONFIG] Project: {WANDB_CONFIG['project']}\")\n",
        "logging.info(f\"[CONFIG] Tags: {', '.join(WANDB_CONFIG['tags'])}\")\n",
        "logging.info(f\"[CONFIG] Notes: {WANDB_CONFIG['notes']}\")\n",
        "logging.info(f\"[CONFIG] Job Type: {WANDB_CONFIG['job_type']}\")\n",
        "logging.info(f\"[CONFIG] Resume: {WANDB_CONFIG['resume']}\")\n",
        "\n",
        "# Log nested configuration (under 'config')\n",
        "logging.info(\"[CONFIG] ------------------------------ Hyperparameters ------------------------------\")\n",
        "for key, value in WANDB_CONFIG[\"config\"].items():\n",
        "    logging.info(f\"[CONFIG] {key}: {value}\")\n",
        "\n",
        "logging.info(\"[CONFIG] =============================================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_wandb(fold):\n",
        "    \"\"\"Initialize wandb for each fold with a unique run name.\"\"\"\n",
        "    wandb.init(\n",
        "        entity=WANDB_CONFIG[\"entity\"],\n",
        "        project=WANDB_CONFIG[\"project\"],\n",
        "        name=f\"{WANDB_RUN_NAME}_fold_{fold + 1}\",\n",
        "        tags=WANDB_CONFIG[\"tags\"],\n",
        "        notes=WANDB_CONFIG[\"notes\"],\n",
        "        config=WANDB_CONFIG[\"config\"],\n",
        "        job_type=WANDB_CONFIG[\"job_type\"],\n",
        "        resume=WANDB_CONFIG[\"resume\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SpaCy for lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load NLTK stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Standardize text, remove stopwords, and apply lemmatization.\"\"\"\n",
        "    # 1. Standardize text (lowercasing & trimming spaces)\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # 2. Remove stopwords\n",
        "    text_tokens = text.split()\n",
        "    text = \" \".join([word for word in text_tokens if word not in stop_words])\n",
        "\n",
        "    # 3. Lemmatization\n",
        "    doc = nlp(text)\n",
        "    text = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "    return text\n",
        "\n",
        "def read_text_files_with_labels_and_image_paths(path):\n",
        "    \"\"\"Extract text from file names, apply preprocessing, and return labels with image paths.\"\"\"\n",
        "    texts, labels, image_paths = [], [], []\n",
        "    class_folders = sorted(os.listdir(path))\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = sorted(os.listdir(class_path))  # Sort to ensure order consistency\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    # Extract filename without extension\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "\n",
        "                    # Replace underscores with spaces\n",
        "                    text = file_name_no_ext.replace(\"_\", \" \")\n",
        "\n",
        "                    # Remove numbers\n",
        "                    text_without_digits = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "                    # Apply preprocessing\n",
        "                    preprocessed_text = preprocess_text(text_without_digits)\n",
        "\n",
        "                    texts.append(preprocessed_text)\n",
        "                    labels.append(label_map[class_name])\n",
        "                    image_paths.append(file_path)\n",
        "\n",
        "    return np.array(texts), np.array(labels), np.array(image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    \"\"\"Dataset class for text data.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "    \n",
        "# Custom dataset class for images\n",
        "class ImageDataset(Dataset):\n",
        "    \"\"\"Dataset class for image data.\"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    \"\"\"Dataset class for multimodal data (image + text).\"\"\"\n",
        "    def __init__(self, image_dataset, text_dataset):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_dataset), len(self.text_dataset))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.image_dataset[idx]\n",
        "        text_data = self.text_dataset[idx]\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": text_data[\"input_ids\"],\n",
        "            \"attention_mask\": text_data[\"attention_mask\"],\n",
        "            \"label\": label\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================== Gated Fusion ========================\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(GatedFusion, self).__init__()\n",
        "        self.gate = nn.Linear(2 * feature_dim, feature_dim)  # Learnable gate\n",
        "        self.sigmoid = nn.Sigmoid()  # Activation\n",
        "\n",
        "    def forward(self, text_feat, image_feat):\n",
        "        combined_feat = torch.cat((text_feat, image_feat), dim=1)\n",
        "        gate_value = self.sigmoid(self.gate(combined_feat))  # Value between 0-1\n",
        "        fused_feat = (gate_value * text_feat) + ((1 - gate_value) * image_feat)  # Weighted fusion\n",
        "        return fused_feat\n",
        "\n",
        "# ======================== Multimodal Classifier (Last Feature Extractor Layer Unfrozen) ========================\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    \"\"\"Multimodal model combining EfficientNetB0 and DistilBERT with partial fine-tuning.\"\"\"\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # ----------- Image Feature Extractor (EfficientNetB0) -----------\n",
        "        self.image_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        \n",
        "        # Freeze all layers except the last one\n",
        "        for param in self.image_model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.image_model.features[-3:].parameters():  # Unfreeze last feature layer\n",
        "            param.requires_grad = True\n",
        "\n",
        "        num_ftrs = self.image_model.classifier[1].in_features\n",
        "        self.image_model.classifier = nn.Identity()  # Remove classifier\n",
        "        self.image_fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_IMAGE)\n",
        "        )\n",
        "\n",
        "        # ----------- Text Feature Extractor (DistilBERT) -----------\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # Freeze all layers except the last transformer layer\n",
        "        for param in self.text_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.text_model.transformer.layer[-2:].parameters():  # Unfreeze last transformer layer\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(self.text_model.config.hidden_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_TEXT)\n",
        "        )\n",
        "\n",
        "        # ----------- Normalize Features -----------\n",
        "        self.text_norm = nn.LayerNorm(512)\n",
        "        self.image_norm = nn.LayerNorm(512)\n",
        "\n",
        "        # ----------- Gated Fusion -----------\n",
        "        self.gated_fusion = GatedFusion(feature_dim=512)\n",
        "\n",
        "        # ----------- Fully Connected Fusion & Classification -----------\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512, 512),  # Increase dimension\n",
        "            nn.BatchNorm1d(512),  # Add batch normalization\n",
        "            nn.ReLU(),            # Use GELU activation\n",
        "\n",
        "            nn.Linear(512, 256),  # Intermediate layer\n",
        "            nn.BatchNorm1d(256),  # Batch normalization\n",
        "            nn.ReLU(),            # GELU activation\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT_CLASSIFIER)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, image_inputs):\n",
        "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(text_output.last_hidden_state[:, 0, :])\n",
        "        text_features = self.text_norm(text_features)\n",
        "        image_features = self.image_fc(self.image_model(image_inputs))\n",
        "        image_features = self.image_norm(image_features)\n",
        "        gated_feat = self.gated_fusion(text_features, image_features)\n",
        "        fused_features = self.fusion_fc(gated_feat)\n",
        "        output = self.classifier(self.dropout(fused_features))\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:17:00,674 - INFO - First 4 samples of dataset:\n",
            "\n",
            "2025-03-25 19:17:00,674 - INFO - Texts: ['aero bar wrapper' 'break glass' 'break rubber' 'butter paper']\n",
            "2025-03-25 19:17:00,675 - INFO - Labels: [0 0 0 0]\n",
            "2025-03-25 19:17:00,676 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Aero_bar_wrapper_1.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_Glass_5291.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Broken_rubber_7263.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\Butter_Paper_9976.png']\n",
            "2025-03-25 19:17:00,676 - INFO - \n",
            "Last 4 samples of dataset:\n",
            "\n",
            "2025-03-25 19:17:00,677 - INFO - Texts: ['wristwatch' 'xbox controller' 'xbox one controller' 'zipper file bag']\n",
            "2025-03-25 19:17:00,677 - INFO - Labels: [3 3 3 3]\n",
            "2025-03-25 19:17:00,678 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\wristwatch_3782.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_controller_2047.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\xbox_one_controller_2048.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\zipper_file_bag_2049.png']\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "texts, labels, image_paths = read_text_files_with_labels_and_image_paths(DATA_DIR)\n",
        "\n",
        "# Log first and last 4 samples\n",
        "logging.info(\"First 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[:4]}\")\n",
        "logging.info(f\"Labels: {labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of dataset:\\n\")\n",
        "logging.info(f\"Texts: {texts[-4:]}\")\n",
        "logging.info(f\"Labels: {labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split into test set and development set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:17:00,698 - INFO - First 4 samples of test set:\n",
            "\n",
            "2025-03-25 19:17:00,698 - INFO - Texts: ['ballast light' 'old phone' 'milk jug lid tab' 'dirty dish sponge']\n",
            "2025-03-25 19:17:00,699 - INFO - Labels: [3 3 0 0]\n",
            "2025-03-25 19:17:00,700 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\ballast_light_286.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\Old_Phones_7828.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\milk_jug_lid_tab_1137.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\dirty_dish_sponge_437.png']\n",
            "2025-03-25 19:17:00,700 - INFO - \n",
            "Last 4 samples of test set:\n",
            "\n",
            "2025-03-25 19:17:00,701 - INFO - Texts: ['empty glass jar' 'non - stretchy plastic' 'backpack' 'piece break glass']\n",
            "2025-03-25 19:17:00,702 - INFO - Labels: [1 0 3 0]\n",
            "2025-03-25 19:17:00,702 - INFO - Image Paths: ['C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Blue\\\\empty_glass_jar_1609.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\non-stretchy plastic.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\TTR\\\\backpack_216.png'\n",
            " 'C:\\\\NN Data\\\\garbage_data\\\\kfold_garbage_data\\\\Black\\\\piece_of_broken_glass_1315.png']\n"
          ]
        }
      ],
      "source": [
        "# Split into a test set and development set\n",
        "train_texts, test_texts, train_labels, test_labels, train_image_paths, test_image_paths = train_test_split(\n",
        "    texts, labels, image_paths, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Log first 4 samples of test set\n",
        "logging.info(\"First 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[:4]}\")\n",
        "logging.info(f\"Labels: {test_labels[:4]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[:4]}\")\n",
        "\n",
        "logging.info(\"\\nLast 4 samples of test set:\\n\")\n",
        "logging.info(f\"Texts: {test_texts[-4:]}\")\n",
        "logging.info(f\"Labels: {test_labels[-4:]}\")\n",
        "logging.info(f\"Image Paths: {test_image_paths[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE), \n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Apply correct normalization\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Only resize + normalize\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_STATS.mean, std=NORMALIZATION_STATS.std)  # Only resize + normalize\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Tokenizer for DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoader for test set\n",
        "\n",
        "Create the dataloader for the test set and set aside for model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test dataset\n",
        "test_image_dataset = ImageDataset(test_image_paths, test_labels, transform[\"test\"])\n",
        "test_text_dataset = CustomTextDataset(test_texts, test_labels, tokenizer, max_len=MAX_LEN)  # Ensure tokenizer is defined\n",
        "test_multimodal_dataset = MultimodalDataset(test_image_dataset, test_text_dataset)\n",
        "\n",
        "# DataLoader for test set\n",
        "test_loader = DataLoader(test_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a peek at a batch in the test set to verify that data has been correctly organized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:17:02,735 - INFO - [INFO] One Batch Sample Inspection:\n",
            "2025-03-25 19:17:02,736 - INFO -    Images Shape: torch.Size([64, 3, 224, 224])\n",
            "2025-03-25 19:17:02,737 - INFO -    Input IDs Shape: torch.Size([64, 40])\n",
            "2025-03-25 19:17:02,738 - INFO -    Attention Mask Shape: torch.Size([64, 40])\n",
            "2025-03-25 19:17:02,739 - INFO -    Labels Shape: torch.Size([64])\n",
            "2025-03-25 19:17:02,739 - INFO - \n",
            "[INFO] First Sample:\n",
            "2025-03-25 19:17:02,771 - INFO -    Image Tensor: tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         ...,\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
            "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
            "\n",
            "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         ...,\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
            "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
            "\n",
            "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         ...,\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
            "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]])\n",
            "2025-03-25 19:17:02,772 - INFO -    Input IDs: tensor([  101, 28030,  2422,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "2025-03-25 19:17:02,773 - INFO -    Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "2025-03-25 19:17:02,774 - INFO -    Label: 3\n"
          ]
        }
      ],
      "source": [
        "# Get one batch\n",
        "for batch in test_loader:\n",
        "    images = batch[\"image\"]  # Image tensor\n",
        "    input_ids = batch[\"input_ids\"]  # Tokenized text tensor\n",
        "    attention_mask = batch[\"attention_mask\"]  # Attention mask\n",
        "    labels = batch[\"label\"]  # Labels tensor\n",
        "\n",
        "    # Log shapes of tensors\n",
        "    logging.info(\"[INFO] One Batch Sample Inspection:\")\n",
        "    logging.info(f\"   Images Shape: {images.shape}\")\n",
        "    logging.info(f\"   Input IDs Shape: {input_ids.shape}\")\n",
        "    logging.info(f\"   Attention Mask Shape: {attention_mask.shape}\")\n",
        "    logging.info(f\"   Labels Shape: {labels.shape}\")\n",
        "\n",
        "    # Log first sample details\n",
        "    logging.info(\"\\n[INFO] First Sample:\")\n",
        "    logging.info(f\"   Image Tensor: {images[0]}\")\n",
        "    logging.info(f\"   Input IDs: {input_ids[0]}\")\n",
        "    logging.info(f\"   Attention Mask: {attention_mask[0]}\")\n",
        "    logging.info(f\"   Label: {labels[0]}\")\n",
        "\n",
        "    break  # Stop after inspecting one batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Stratified K-Fold on the development set to split into train/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:17:02,785 - INFO - [INFO] Fold 1/5\n",
            "2025-03-25 19:17:02,786 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 19:17:02,787 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-25 19:17:02,788 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-25 19:17:02,788 - INFO - [INFO] Fold 2/5\n",
            "2025-03-25 19:17:02,789 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 19:17:02,790 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-25 19:17:02,791 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-25 19:17:02,792 - INFO - [INFO] Fold 3/5\n",
            "2025-03-25 19:17:02,793 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 19:17:02,794 - INFO -    Train Class Distribution: Counter({np.int64(1): 3591, np.int64(0): 1753, np.int64(2): 1708, np.int64(3): 1542})\n",
            "2025-03-25 19:17:02,795 - INFO -    Validation Class Distribution: Counter({np.int64(1): 897, np.int64(0): 439, np.int64(2): 427, np.int64(3): 386})\n",
            "2025-03-25 19:17:02,796 - INFO - [INFO] Fold 4/5\n",
            "2025-03-25 19:17:02,797 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 19:17:02,798 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-25 19:17:02,799 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n",
            "2025-03-25 19:17:02,800 - INFO - [INFO] Fold 5/5\n",
            "2025-03-25 19:17:02,800 - INFO - [INFO] Class Distributions:\n",
            "2025-03-25 19:17:02,801 - INFO -    Train Class Distribution: Counter({np.int64(1): 3590, np.int64(0): 1754, np.int64(2): 1708, np.int64(3): 1543})\n",
            "2025-03-25 19:17:02,802 - INFO -    Validation Class Distribution: Counter({np.int64(1): 898, np.int64(0): 438, np.int64(2): 427, np.int64(3): 385})\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    logging.info(f\"[INFO] Fold {fold + 1}/{K_FOLDS}\")\n",
        "\n",
        "    # Extract labels for current fold\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "\n",
        "    # Log class distributions\n",
        "    logging.info(\"[INFO] Class Distributions:\")\n",
        "    logging.info(f\"   Train Class Distribution: {Counter(train_labels_fold)}\")\n",
        "    logging.info(f\"   Validation Class Distribution: {Counter(val_labels_fold)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify k-fold was applied correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:17:02,812 - INFO - [INFO] No data leakage detected in Fold 1\n",
            "2025-03-25 19:17:02,814 - INFO - [INFO] No data leakage detected in Fold 2\n",
            "2025-03-25 19:17:02,815 - INFO - [INFO] No data leakage detected in Fold 3\n",
            "2025-03-25 19:17:02,817 - INFO - [INFO] No data leakage detected in Fold 4\n",
            "2025-03-25 19:17:02,818 - INFO - [INFO] No data leakage detected in Fold 5\n"
          ]
        }
      ],
      "source": [
        "# Ensure no data leakage in folds\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "    train_set = set(train_idx)\n",
        "    val_set = set(val_idx)\n",
        "\n",
        "    # Check for intersection (should be empty)\n",
        "    intersection = train_set.intersection(val_set)\n",
        "    assert len(intersection) == 0, f\"Data leakage detected in Fold {fold + 1}\"\n",
        "\n",
        "    logging.info(f\"[INFO] No data leakage detected in Fold {fold + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "#     train_labels_fold = train_labels[train_idx]\n",
        "#     val_labels_fold = train_labels[val_idx]\n",
        "\n",
        "#     plt.figure(figsize=(10, 4))\n",
        "#     plt.hist(train_labels_fold, bins=len(set(train_labels)), alpha=0.6, label=\"Train\")\n",
        "#     plt.hist(val_labels_fold, bins=len(set(train_labels)), alpha=0.6, label=\"Validation\")\n",
        "#     plt.title(f\"Class Distribution in Fold {fold + 1}\")\n",
        "#     plt.legend()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Move data to the appropriate device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)  # Compute batch loss\n",
        "\n",
        "            # Aggregate loss for averaging\n",
        "            total_loss += loss.item() * labels.size(0)  # Multiply by batch size for proper averaging\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total  # Normalize loss over total samples\n",
        "    accuracy = correct / total  # Compute accuracy\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Weight Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adaptive_weight_decay(epoch, warmup_epochs=5, decay_factors=(0.1, 1.0)):\n",
        "    \"\"\"\n",
        "    Returns a scaled weight decay based on epoch number.\n",
        "    During warm-up, it applies a lower decay (decay_factors[0]).\n",
        "    After warm-up, it applies full weight decay (decay_factors[1]).\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return decay_factors[0]  # Use lower decay during warm-up\n",
        "    return decay_factors[1]  # Use normal decay afterward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_warmup_lr(epoch, warmup_epochs, base_lr):\n",
        "    \"\"\"\n",
        "    Linear warmup schedule for the learning rate.\n",
        "    \"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return base_lr * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        return base_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True):\n",
        "    initialize_wandb(fold)\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    best_val_loss = float(\"inf\")  # Track best validation loss\n",
        "    epochs_without_improvement = 0  # Track epochs without improvement until equals patience\n",
        "\n",
        "    # ================ ReduceLROnPlateau Scheduler ================\n",
        "    plateau_scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=LR_SCHEDULING_FACTOR, patience=3, verbose=True\n",
        "    )\n",
        "    \n",
        "    # AMP GradScaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    epoch_start_time = time.time()  # Start total training timer\n",
        "    logging.info(\"[TRAIN INFO] Starting Training...\")\n",
        "\n",
        "    # Warmup settings\n",
        "    WARMUP_EPOCHS = 8  # Number of epochs for warmup\n",
        "    base_lr_image = LEARNING_RATE_IMAGE  # Base learning rate for EfficientNet\n",
        "    base_lr_text = LEARNING_RATE_TEXT  # Base learning rate for DistilBERT\n",
        "    base_lr_fusion = LEARNING_RATE_FUSION  # Base learning rate for fusion layer\n",
        "    base_lr_classifier = LEARNING_RATE_CLASSIFIER  # Base learning rate for classifier\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        logging.info(f\"[TRAIN INFO] ============================== Epoch {epoch + 1}/{EPOCHS} ==============================\")\n",
        "        \n",
        "        # Apply learning rate warmup\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            warmup_lr_image = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_image)\n",
        "            warmup_lr_text = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_text)\n",
        "            warmup_lr_fusion = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_fusion)\n",
        "            warmup_lr_classifier = get_warmup_lr(epoch, WARMUP_EPOCHS, base_lr_classifier)\n",
        "\n",
        "            # Update learning rates for each parameter group\n",
        "            optimizer.param_groups[0][\"lr\"] = warmup_lr_image  # Unfrozen EfficientNet layer\n",
        "            optimizer.param_groups[1][\"lr\"] = warmup_lr_text  # Unfrozen DistilBERT layer\n",
        "            optimizer.param_groups[2][\"lr\"] = warmup_lr_image  # Image FC layer\n",
        "            optimizer.param_groups[3][\"lr\"] = warmup_lr_text  # Text FC layer\n",
        "            optimizer.param_groups[4][\"lr\"] = warmup_lr_fusion  # Fusion layer\n",
        "            optimizer.param_groups[5][\"lr\"] = warmup_lr_classifier  # Classifier layer\n",
        "\n",
        "        model.train()  # Set model to training modes\n",
        "        total_train_loss = 0  # Track total training loss for the epoch\n",
        "        batch_train_loss = 0  # Track batch loss for gradient accumulation\n",
        "        step = 0  # Track the number of batches processed\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Training phase\n",
        "        for step, batch in enumerate(dataloaders[\"train_loader\"], 1):\n",
        "            # Move data to device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask, images)  # Send inputs to network and receive outputs\n",
        "                loss = criterion(outputs, labels) / GRAD_ACCUM_STEPS  # Compute loss (no normalization for gradient accumulation)\n",
        "\n",
        "            # Backward pass and optimizer step\n",
        "            scaler.scale(loss).backward()  # Scale loss and backpropagate\n",
        "\n",
        "            batch_train_loss += loss.item()\n",
        "            total_train_loss += loss.item() * GRAD_ACCUM_STEPS  # Undo normalization for total loss\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            # Perform optimizer step before learning rate scheduler step\n",
        "            if step % GRAD_ACCUM_STEPS == 0 or step == len(dataloaders[\"train_loader\"]):\n",
        "                # Gradient Clipping\n",
        "                scaler.unscale_(optimizer)  # Unscale gradients before clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to a max norm of 1.0\n",
        "\n",
        "                # Optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Log batch loss\n",
        "                logging.info(f\"[TRAIN INFO] Batch {step}/{len(dataloaders['train_loader'])}, Accumulated loss over {GRAD_ACCUM_STEPS} batches: {batch_train_loss:.4f}\")\n",
        "                batch_train_loss = 0  # Reset batch loss for the next accumulation\n",
        "\n",
        "        # Validation step to see how well model performs this epoch\n",
        "        logging.info(f\"[TRAIN INFO] Evaluating model...\")\n",
        "        val_loss, val_acc = evaluate_model(model, dataloaders[\"val_loader\"], device)\n",
        "        avg_train_loss = total_train_loss / len(dataloaders[\"train_loader\"])\n",
        "\n",
        "        # **Learning Rate Scheduler Handling**\n",
        "        plateau_scheduler.step(val_loss)  \n",
        "\n",
        "        # Log weight decay and learning rate updates\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_val_loss_diff\": avg_train_loss - val_loss,  # Track overfitting tendency\n",
        "            \"early_stopping_epochs\": epochs_without_improvement,  # Track early stopping\n",
        "            \"learning_rate_image\": optimizer.param_groups[0][\"lr\"],  # Log learning rates\n",
        "            \"learning_rate_text\": optimizer.param_groups[1][\"lr\"],\n",
        "            \"learning_rate_fusion\": optimizer.param_groups[4][\"lr\"],\n",
        "            \"learning_rate_classifier\": optimizer.param_groups[5][\"lr\"],\n",
        "        })\n",
        "\n",
        "        logging.info(f\"[TRAIN INFO] Epoch {epoch + 1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss - CONVERGENCE_THRESHOLD:  # If loss improves, save the model\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0  # Reset epochs without improvement counter for patience\n",
        "            torch.save(model.state_dict(), f\"{MODEL_NAME}_fold_{fold+1}.pth\")\n",
        "            logging.info(f\"[TRAIN INFO] Best Model Saved for Fold {fold + 1}\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1  # Increment until patience reached\n",
        "\n",
        "        # Early stopping if no improvement for epochs\n",
        "        if epochs_without_improvement >= PATIENCE:\n",
        "            total_training_time = time.time() - epoch_start_time\n",
        "            logging.info(f\"[TRAIN INFO] Early stopping at epoch {epoch + 1} as validation loss did not improve for {PATIENCE} epochs.\")\n",
        "            logging.info(f\"[TRAIN INFO] Total Time: {total_training_time:.2f}s\")\n",
        "            wandb.finish()\n",
        "            break\n",
        "\n",
        "    total_training_time = time.time() - epoch_start_time\n",
        "    logging.info(f\"[TRAIN INFO] Fold {fold + 1} Training Complete at epoch {epoch + 1}. Total Time: {total_training_time:.2f}s\")\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:17:02,879 - INFO - [K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\n",
            "2025-03-25 19:17:02,883 - INFO - [K-FOLD INFO] ============================== Fold 1/5 ==============================\n",
            "2025-03-25 19:17:02,886 - INFO - [K-FOLD INFO] Fold 1:\n",
            "2025-03-25 19:17:02,886 - INFO -    Train Samples: 8594\n",
            "2025-03-25 19:17:02,887 - INFO -    Validation Samples: 2149\n",
            "2025-03-25 19:17:02,888 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 1\n",
            "2025-03-25 19:17:02,889 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 1:\n",
            "2025-03-25 19:17:02,890 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 19:17:03,660 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 1\n",
            "2025-03-25 19:17:03,661 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 1:\n",
            "2025-03-25 19:17:03,662 - INFO - [K-FOLD INFO] Loss function initialized for Fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: shcau (shcau-university-of-calgary-in-alberta) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_gated_only\\wandb\\run-20250325_191704-dmnq6asf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/dmnq6asf' target=\"_blank\">experiment_multimodal_gated_only_fold_1</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/dmnq6asf' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/dmnq6asf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arkzs\\miniforge3\\envs\\enel645_torch_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_20168\\836902376.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:17:05,791 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 19:17:05,792 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_20168\\836902376.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:17:13,844 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.1097\n",
            "2025-03-25 19:17:21,919 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.4189\n",
            "2025-03-25 19:17:29,936 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3560\n",
            "2025-03-25 19:17:37,761 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3363\n",
            "2025-03-25 19:17:45,558 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3328\n",
            "2025-03-25 19:17:53,138 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2556\n",
            "2025-03-25 19:18:00,680 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2571\n",
            "2025-03-25 19:18:08,211 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2654\n",
            "2025-03-25 19:18:16,340 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2163\n",
            "2025-03-25 19:18:24,097 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2204\n",
            "2025-03-25 19:18:31,927 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.2128\n",
            "2025-03-25 19:18:39,552 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1321\n",
            "2025-03-25 19:18:47,260 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1665\n",
            "2025-03-25 19:18:54,694 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1251\n",
            "2025-03-25 19:19:02,291 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1564\n",
            "2025-03-25 19:19:10,320 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.0694\n",
            "2025-03-25 19:19:18,724 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1093\n",
            "2025-03-25 19:19:26,922 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0682\n",
            "2025-03-25 19:19:34,946 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.0964\n",
            "2025-03-25 19:19:43,267 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0499\n",
            "2025-03-25 19:19:51,625 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0510\n",
            "2025-03-25 19:19:59,535 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0697\n",
            "2025-03-25 19:20:07,912 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 1.0080\n",
            "2025-03-25 19:20:16,497 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0225\n",
            "2025-03-25 19:20:24,497 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.9458\n",
            "2025-03-25 19:20:32,602 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 1.0101\n",
            "2025-03-25 19:20:40,918 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9892\n",
            "2025-03-25 19:20:49,083 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9955\n",
            "2025-03-25 19:20:57,090 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9215\n",
            "2025-03-25 19:21:04,889 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9456\n",
            "2025-03-25 19:21:12,999 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9058\n",
            "2025-03-25 19:21:21,091 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9583\n",
            "2025-03-25 19:21:29,168 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9587\n",
            "2025-03-25 19:21:35,047 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.6756\n",
            "2025-03-25 19:21:35,853 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2160\n",
            "2025-03-25 19:21:35,854 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 19:22:36,087 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1149, Val Loss: 0.7670, Val Acc: 0.7101\n",
            "2025-03-25 19:22:36,426 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 19:22:36,426 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 19:22:42,862 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6533\n",
            "2025-03-25 19:22:51,029 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.9573\n",
            "2025-03-25 19:22:59,461 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.9404\n",
            "2025-03-25 19:23:07,856 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.8648\n",
            "2025-03-25 19:23:15,840 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8654\n",
            "2025-03-25 19:23:23,851 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8008\n",
            "2025-03-25 19:23:31,808 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8249\n",
            "2025-03-25 19:23:40,312 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8555\n",
            "2025-03-25 19:23:48,801 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.8312\n",
            "2025-03-25 19:23:57,065 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.7792\n",
            "2025-03-25 19:24:05,238 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.7620\n",
            "2025-03-25 19:24:13,223 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.8408\n",
            "2025-03-25 19:24:21,418 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7776\n",
            "2025-03-25 19:24:29,638 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.8954\n",
            "2025-03-25 19:24:38,077 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.7834\n",
            "2025-03-25 19:24:46,468 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8887\n",
            "2025-03-25 19:24:55,223 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7250\n",
            "2025-03-25 19:25:03,916 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7674\n",
            "2025-03-25 19:25:12,021 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7633\n",
            "2025-03-25 19:25:20,216 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.8064\n",
            "2025-03-25 19:25:28,168 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7790\n",
            "2025-03-25 19:25:36,212 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.8291\n",
            "2025-03-25 19:25:44,163 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7970\n",
            "2025-03-25 19:25:52,025 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.7750\n",
            "2025-03-25 19:25:59,604 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7109\n",
            "2025-03-25 19:26:07,395 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6751\n",
            "2025-03-25 19:26:15,200 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7011\n",
            "2025-03-25 19:26:22,763 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7443\n",
            "2025-03-25 19:26:30,585 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6934\n",
            "2025-03-25 19:26:38,392 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7440\n",
            "2025-03-25 19:26:46,145 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7817\n",
            "2025-03-25 19:26:53,950 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.7434\n",
            "2025-03-25 19:27:01,881 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.8496\n",
            "2025-03-25 19:27:07,505 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5503\n",
            "2025-03-25 19:27:08,107 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1969\n",
            "2025-03-25 19:27:08,108 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 19:28:04,717 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.7986, Val Loss: 0.5281, Val Acc: 0.8083\n",
            "2025-03-25 19:28:05,021 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 19:28:05,021 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 19:28:11,163 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4711\n",
            "2025-03-25 19:28:18,991 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6698\n",
            "2025-03-25 19:28:26,770 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6870\n",
            "2025-03-25 19:28:35,034 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6647\n",
            "2025-03-25 19:28:43,260 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6093\n",
            "2025-03-25 19:28:51,024 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.7365\n",
            "2025-03-25 19:28:59,283 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6905\n",
            "2025-03-25 19:29:07,094 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.7010\n",
            "2025-03-25 19:29:15,126 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6673\n",
            "2025-03-25 19:29:22,737 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6934\n",
            "2025-03-25 19:29:30,489 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6804\n",
            "2025-03-25 19:29:38,512 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7435\n",
            "2025-03-25 19:29:46,561 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.7154\n",
            "2025-03-25 19:29:54,220 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7234\n",
            "2025-03-25 19:30:01,971 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6122\n",
            "2025-03-25 19:30:09,933 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6952\n",
            "2025-03-25 19:30:17,717 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6039\n",
            "2025-03-25 19:30:25,407 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6319\n",
            "2025-03-25 19:30:33,290 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6536\n",
            "2025-03-25 19:30:41,070 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6098\n",
            "2025-03-25 19:30:49,032 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7348\n",
            "2025-03-25 19:30:56,776 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6386\n",
            "2025-03-25 19:31:04,722 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6089\n",
            "2025-03-25 19:31:12,903 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6322\n",
            "2025-03-25 19:31:20,508 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6581\n",
            "2025-03-25 19:31:28,489 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.6310\n",
            "2025-03-25 19:31:36,199 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7043\n",
            "2025-03-25 19:31:44,061 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6226\n",
            "2025-03-25 19:31:51,901 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6792\n",
            "2025-03-25 19:31:59,804 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6681\n",
            "2025-03-25 19:32:07,355 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5604\n",
            "2025-03-25 19:32:15,261 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6161\n",
            "2025-03-25 19:32:22,944 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6670\n",
            "2025-03-25 19:32:28,690 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4739\n",
            "2025-03-25 19:32:29,259 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1792\n",
            "2025-03-25 19:32:29,260 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 19:33:26,231 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6618, Val Loss: 0.4384, Val Acc: 0.8432\n",
            "2025-03-25 19:33:26,715 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 19:33:26,715 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 19:33:32,959 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4123\n",
            "2025-03-25 19:33:40,797 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5656\n",
            "2025-03-25 19:33:48,861 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6428\n",
            "2025-03-25 19:33:56,648 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6179\n",
            "2025-03-25 19:34:04,462 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6162\n",
            "2025-03-25 19:34:12,399 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5458\n",
            "2025-03-25 19:34:20,325 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6042\n",
            "2025-03-25 19:34:28,251 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6735\n",
            "2025-03-25 19:34:36,048 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5474\n",
            "2025-03-25 19:34:43,847 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6144\n",
            "2025-03-25 19:34:51,626 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5511\n",
            "2025-03-25 19:34:59,552 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5945\n",
            "2025-03-25 19:35:07,441 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5827\n",
            "2025-03-25 19:35:15,356 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5280\n",
            "2025-03-25 19:35:23,039 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5056\n",
            "2025-03-25 19:35:30,716 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5550\n",
            "2025-03-25 19:35:38,742 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5979\n",
            "2025-03-25 19:35:46,392 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6140\n",
            "2025-03-25 19:35:54,281 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5614\n",
            "2025-03-25 19:36:02,036 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5829\n",
            "2025-03-25 19:36:10,014 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5550\n",
            "2025-03-25 19:36:17,816 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5524\n",
            "2025-03-25 19:36:25,609 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5667\n",
            "2025-03-25 19:36:33,483 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5476\n",
            "2025-03-25 19:36:41,313 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6415\n",
            "2025-03-25 19:36:48,904 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5558\n",
            "2025-03-25 19:36:56,590 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4823\n",
            "2025-03-25 19:37:04,306 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5771\n",
            "2025-03-25 19:37:12,208 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6186\n",
            "2025-03-25 19:37:20,001 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5617\n",
            "2025-03-25 19:37:27,746 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5160\n",
            "2025-03-25 19:37:35,656 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6407\n",
            "2025-03-25 19:37:43,336 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6183\n",
            "2025-03-25 19:37:49,259 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4152\n",
            "2025-03-25 19:37:49,865 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0915\n",
            "2025-03-25 19:37:49,866 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 19:38:46,365 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5764, Val Loss: 0.4141, Val Acc: 0.8506\n",
            "2025-03-25 19:38:46,663 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 19:38:46,663 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 19:38:52,569 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4218\n",
            "2025-03-25 19:39:00,562 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5358\n",
            "2025-03-25 19:39:08,197 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5212\n",
            "2025-03-25 19:39:16,118 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5072\n",
            "2025-03-25 19:39:23,857 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4928\n",
            "2025-03-25 19:39:31,289 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5329\n",
            "2025-03-25 19:39:39,183 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5113\n",
            "2025-03-25 19:39:47,119 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4861\n",
            "2025-03-25 19:39:54,710 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5644\n",
            "2025-03-25 19:40:02,535 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4953\n",
            "2025-03-25 19:40:10,476 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6092\n",
            "2025-03-25 19:40:18,345 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5439\n",
            "2025-03-25 19:40:26,279 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5188\n",
            "2025-03-25 19:40:34,053 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5481\n",
            "2025-03-25 19:40:41,953 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5023\n",
            "2025-03-25 19:40:50,093 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5843\n",
            "2025-03-25 19:40:58,330 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4664\n",
            "2025-03-25 19:41:06,062 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4525\n",
            "2025-03-25 19:41:14,049 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4394\n",
            "2025-03-25 19:41:21,920 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4804\n",
            "2025-03-25 19:41:29,718 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4673\n",
            "2025-03-25 19:41:37,497 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4991\n",
            "2025-03-25 19:41:45,280 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5992\n",
            "2025-03-25 19:41:53,108 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5518\n",
            "2025-03-25 19:42:00,895 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5169\n",
            "2025-03-25 19:42:08,500 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4734\n",
            "2025-03-25 19:42:16,283 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4697\n",
            "2025-03-25 19:42:23,914 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5713\n",
            "2025-03-25 19:42:31,907 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4967\n",
            "2025-03-25 19:42:39,698 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5334\n",
            "2025-03-25 19:42:47,427 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5626\n",
            "2025-03-25 19:42:55,299 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4942\n",
            "2025-03-25 19:43:03,285 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4941\n",
            "2025-03-25 19:43:09,017 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4774\n",
            "2025-03-25 19:43:09,627 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1089\n",
            "2025-03-25 19:43:09,628 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 19:44:06,157 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5194, Val Loss: 0.3675, Val Acc: 0.8725\n",
            "2025-03-25 19:44:06,464 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 19:44:06,464 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 19:44:12,671 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-25 19:44:20,407 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4030\n",
            "2025-03-25 19:44:28,186 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4764\n",
            "2025-03-25 19:44:36,063 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4625\n",
            "2025-03-25 19:44:43,820 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5176\n",
            "2025-03-25 19:44:51,228 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4369\n",
            "2025-03-25 19:44:58,945 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4897\n",
            "2025-03-25 19:45:06,840 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5193\n",
            "2025-03-25 19:45:14,652 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4953\n",
            "2025-03-25 19:45:22,373 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4668\n",
            "2025-03-25 19:45:29,958 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4925\n",
            "2025-03-25 19:45:37,909 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4899\n",
            "2025-03-25 19:45:45,694 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5208\n",
            "2025-03-25 19:45:53,396 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4705\n",
            "2025-03-25 19:46:01,390 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5151\n",
            "2025-03-25 19:46:09,198 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5112\n",
            "2025-03-25 19:46:17,050 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4911\n",
            "2025-03-25 19:46:24,731 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5356\n",
            "2025-03-25 19:46:32,567 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5769\n",
            "2025-03-25 19:46:40,352 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4787\n",
            "2025-03-25 19:46:48,124 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4323\n",
            "2025-03-25 19:46:55,928 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5201\n",
            "2025-03-25 19:47:03,807 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4180\n",
            "2025-03-25 19:47:11,559 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4895\n",
            "2025-03-25 19:47:19,111 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5290\n",
            "2025-03-25 19:47:26,843 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5135\n",
            "2025-03-25 19:47:34,521 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4975\n",
            "2025-03-25 19:47:42,409 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4583\n",
            "2025-03-25 19:47:50,198 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5034\n",
            "2025-03-25 19:47:57,990 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5654\n",
            "2025-03-25 19:48:06,403 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5312\n",
            "2025-03-25 19:48:14,199 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4815\n",
            "2025-03-25 19:48:21,984 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3964\n",
            "2025-03-25 19:48:27,723 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3512\n",
            "2025-03-25 19:48:28,301 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1447\n",
            "2025-03-25 19:48:28,302 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 19:49:25,685 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4885, Val Loss: 0.3593, Val Acc: 0.8716\n",
            "2025-03-25 19:49:25,986 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 19:49:25,986 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 19:49:32,124 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3389\n",
            "2025-03-25 19:49:39,895 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4408\n",
            "2025-03-25 19:49:47,796 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4346\n",
            "2025-03-25 19:49:55,570 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4529\n",
            "2025-03-25 19:50:03,291 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3621\n",
            "2025-03-25 19:50:11,040 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4477\n",
            "2025-03-25 19:50:18,966 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4521\n",
            "2025-03-25 19:50:26,759 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4191\n",
            "2025-03-25 19:50:34,544 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5306\n",
            "2025-03-25 19:50:42,349 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4350\n",
            "2025-03-25 19:50:50,149 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4254\n",
            "2025-03-25 19:50:57,945 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4445\n",
            "2025-03-25 19:51:05,746 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4611\n",
            "2025-03-25 19:51:13,363 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5240\n",
            "2025-03-25 19:51:20,972 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5067\n",
            "2025-03-25 19:51:28,966 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4395\n",
            "2025-03-25 19:51:36,729 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4542\n",
            "2025-03-25 19:51:44,517 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4592\n",
            "2025-03-25 19:51:52,290 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4347\n",
            "2025-03-25 19:52:00,108 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4800\n",
            "2025-03-25 19:52:07,904 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4675\n",
            "2025-03-25 19:52:15,927 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5025\n",
            "2025-03-25 19:52:23,750 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4429\n",
            "2025-03-25 19:52:31,723 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5070\n",
            "2025-03-25 19:52:39,479 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4600\n",
            "2025-03-25 19:52:47,455 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5117\n",
            "2025-03-25 19:52:55,272 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4652\n",
            "2025-03-25 19:53:03,096 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4170\n",
            "2025-03-25 19:53:11,181 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5039\n",
            "2025-03-25 19:53:19,114 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4427\n",
            "2025-03-25 19:53:27,040 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4927\n",
            "2025-03-25 19:53:35,029 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4988\n",
            "2025-03-25 19:53:42,820 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4251\n",
            "2025-03-25 19:53:48,726 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2803\n",
            "2025-03-25 19:53:49,324 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1357\n",
            "2025-03-25 19:53:49,325 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 19:54:46,507 - INFO - [TRAIN INFO] Epoch 7/50, Train Loss: 0.4591, Val Loss: 0.3713, Val Acc: 0.8651\n",
            "2025-03-25 19:54:46,508 - INFO - [TRAIN INFO] ============================== Epoch 8/50 ==============================\n",
            "2025-03-25 19:54:52,330 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3084\n",
            "2025-03-25 19:55:00,477 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4088\n",
            "2025-03-25 19:55:08,433 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3989\n",
            "2025-03-25 19:55:16,446 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4116\n",
            "2025-03-25 19:55:24,263 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3750\n",
            "2025-03-25 19:55:32,066 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4267\n",
            "2025-03-25 19:55:39,666 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4253\n",
            "2025-03-25 19:55:47,436 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5047\n",
            "2025-03-25 19:55:55,169 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4647\n",
            "2025-03-25 19:56:02,863 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3860\n",
            "2025-03-25 19:56:10,587 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4209\n",
            "2025-03-25 19:56:18,391 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4081\n",
            "2025-03-25 19:56:26,069 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4469\n",
            "2025-03-25 19:56:33,894 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4074\n",
            "2025-03-25 19:56:41,932 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5154\n",
            "2025-03-25 19:56:49,535 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4336\n",
            "2025-03-25 19:56:57,233 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4037\n",
            "2025-03-25 19:57:05,179 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4786\n",
            "2025-03-25 19:57:12,955 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4024\n",
            "2025-03-25 19:57:20,739 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.4366\n",
            "2025-03-25 19:57:28,480 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4701\n",
            "2025-03-25 19:57:36,125 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4602\n",
            "2025-03-25 19:57:44,021 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4163\n",
            "2025-03-25 19:57:51,637 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4306\n",
            "2025-03-25 19:57:59,624 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4637\n",
            "2025-03-25 19:58:07,351 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4253\n",
            "2025-03-25 19:58:14,982 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4449\n",
            "2025-03-25 19:58:22,809 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4550\n",
            "2025-03-25 19:58:30,576 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4733\n",
            "2025-03-25 19:58:38,370 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5006\n",
            "2025-03-25 19:58:46,087 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4343\n",
            "2025-03-25 19:58:53,804 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.4236\n",
            "2025-03-25 19:59:01,441 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4399\n",
            "2025-03-25 19:59:07,386 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3848\n",
            "2025-03-25 19:59:07,939 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0628\n",
            "2025-03-25 19:59:07,940 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:00:04,518 - INFO - [TRAIN INFO] Epoch 8/50, Train Loss: 0.4370, Val Loss: 0.3987, Val Acc: 0.8571\n",
            "2025-03-25 20:00:04,519 - INFO - [TRAIN INFO] ============================== Epoch 9/50 ==============================\n",
            "2025-03-25 20:00:10,571 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3174\n",
            "2025-03-25 20:00:18,309 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4278\n",
            "2025-03-25 20:00:26,101 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4093\n",
            "2025-03-25 20:00:33,579 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3873\n",
            "2025-03-25 20:00:41,124 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.4418\n",
            "2025-03-25 20:00:48,891 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4421\n",
            "2025-03-25 20:00:56,764 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4065\n",
            "2025-03-25 20:01:04,706 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4118\n",
            "2025-03-25 20:01:12,753 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3644\n",
            "2025-03-25 20:01:20,561 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3811\n",
            "2025-03-25 20:01:28,439 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3713\n",
            "2025-03-25 20:01:36,361 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4165\n",
            "2025-03-25 20:01:44,118 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4513\n",
            "2025-03-25 20:01:52,074 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3626\n",
            "2025-03-25 20:01:59,906 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3697\n",
            "2025-03-25 20:02:08,145 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4347\n",
            "2025-03-25 20:02:15,843 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3968\n",
            "2025-03-25 20:02:23,567 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4217\n",
            "2025-03-25 20:02:31,534 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4312\n",
            "2025-03-25 20:02:39,323 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3808\n",
            "2025-03-25 20:02:47,088 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4354\n",
            "2025-03-25 20:02:54,731 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4383\n",
            "2025-03-25 20:03:02,349 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4568\n",
            "2025-03-25 20:03:09,923 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3549\n",
            "2025-03-25 20:03:17,613 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4231\n",
            "2025-03-25 20:03:24,990 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4345\n",
            "2025-03-25 20:03:32,605 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4115\n",
            "2025-03-25 20:03:40,300 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4404\n",
            "2025-03-25 20:03:48,111 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3963\n",
            "2025-03-25 20:03:55,687 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3746\n",
            "2025-03-25 20:04:03,510 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.4004\n",
            "2025-03-25 20:04:11,305 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3904\n",
            "2025-03-25 20:04:18,978 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4078\n",
            "2025-03-25 20:04:25,082 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2864\n",
            "2025-03-25 20:04:25,677 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1098\n",
            "2025-03-25 20:04:25,678 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:05:22,585 - INFO - [TRAIN INFO] Epoch 9/50, Train Loss: 0.4085, Val Loss: 0.3800, Val Acc: 0.8688\n",
            "2025-03-25 20:05:22,586 - INFO - [TRAIN INFO] ============================== Epoch 10/50 ==============================\n",
            "2025-03-25 20:05:28,305 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3119\n",
            "2025-03-25 20:05:35,833 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3479\n",
            "2025-03-25 20:05:43,443 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3410\n",
            "2025-03-25 20:05:51,170 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3522\n",
            "2025-03-25 20:05:58,874 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3632\n",
            "2025-03-25 20:06:06,857 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4031\n",
            "2025-03-25 20:06:14,744 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3733\n",
            "2025-03-25 20:06:22,428 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3926\n",
            "2025-03-25 20:06:30,732 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3907\n",
            "2025-03-25 20:06:38,643 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3944\n",
            "2025-03-25 20:06:46,337 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3744\n",
            "2025-03-25 20:06:54,057 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3551\n",
            "2025-03-25 20:07:01,854 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3622\n",
            "2025-03-25 20:07:09,445 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3633\n",
            "2025-03-25 20:07:17,161 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4301\n",
            "2025-03-25 20:07:24,755 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3943\n",
            "2025-03-25 20:07:32,216 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3553\n",
            "2025-03-25 20:07:39,771 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3736\n",
            "2025-03-25 20:07:47,849 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3409\n",
            "2025-03-25 20:07:55,844 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3718\n",
            "2025-03-25 20:08:03,433 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.4266\n",
            "2025-03-25 20:08:11,236 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3638\n",
            "2025-03-25 20:08:18,893 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3820\n",
            "2025-03-25 20:08:26,787 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3377\n",
            "2025-03-25 20:08:34,413 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4141\n",
            "2025-03-25 20:08:41,975 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3903\n",
            "2025-03-25 20:08:49,752 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3511\n",
            "2025-03-25 20:08:57,567 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3965\n",
            "2025-03-25 20:09:05,600 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3978\n",
            "2025-03-25 20:09:13,313 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3872\n",
            "2025-03-25 20:09:20,877 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3315\n",
            "2025-03-25 20:09:28,805 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3478\n",
            "2025-03-25 20:09:36,404 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4527\n",
            "2025-03-25 20:09:42,213 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3008\n",
            "2025-03-25 20:09:42,782 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0734\n",
            "2025-03-25 20:09:42,783 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:10:39,669 - INFO - [TRAIN INFO] Epoch 10/50, Train Loss: 0.3776, Val Loss: 0.3566, Val Acc: 0.8734\n",
            "2025-03-25 20:10:39,978 - INFO - [TRAIN INFO] Best Model Saved for Fold 1\n",
            "2025-03-25 20:10:39,979 - INFO - [TRAIN INFO] ============================== Epoch 11/50 ==============================\n",
            "2025-03-25 20:10:46,015 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2699\n",
            "2025-03-25 20:10:53,990 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3507\n",
            "2025-03-25 20:11:01,965 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3510\n",
            "2025-03-25 20:11:09,956 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3317\n",
            "2025-03-25 20:11:17,991 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3920\n",
            "2025-03-25 20:11:25,789 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3549\n",
            "2025-03-25 20:11:33,761 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3837\n",
            "2025-03-25 20:11:41,326 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3129\n",
            "2025-03-25 20:11:48,778 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3735\n",
            "2025-03-25 20:11:56,619 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3592\n",
            "2025-03-25 20:12:04,334 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3775\n",
            "2025-03-25 20:12:12,212 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3337\n",
            "2025-03-25 20:12:19,759 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4032\n",
            "2025-03-25 20:12:27,353 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4103\n",
            "2025-03-25 20:12:34,912 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3249\n",
            "2025-03-25 20:12:42,553 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3643\n",
            "2025-03-25 20:12:50,027 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3469\n",
            "2025-03-25 20:12:57,818 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3690\n",
            "2025-03-25 20:13:05,746 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3678\n",
            "2025-03-25 20:13:13,494 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3498\n",
            "2025-03-25 20:13:21,137 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3315\n",
            "2025-03-25 20:13:28,958 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3941\n",
            "2025-03-25 20:13:36,620 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3570\n",
            "2025-03-25 20:13:44,076 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3717\n",
            "2025-03-25 20:13:52,120 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3198\n",
            "2025-03-25 20:13:59,928 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3466\n",
            "2025-03-25 20:14:07,529 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3623\n",
            "2025-03-25 20:14:15,272 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3248\n",
            "2025-03-25 20:14:22,881 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3450\n",
            "2025-03-25 20:14:30,546 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3820\n",
            "2025-03-25 20:14:38,136 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3699\n",
            "2025-03-25 20:14:46,116 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3510\n",
            "2025-03-25 20:14:53,707 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3974\n",
            "2025-03-25 20:14:59,437 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2662\n",
            "2025-03-25 20:15:00,017 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1759\n",
            "2025-03-25 20:15:00,018 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:15:56,825 - INFO - [TRAIN INFO] Epoch 11/50, Train Loss: 0.3621, Val Loss: 0.3765, Val Acc: 0.8716\n",
            "2025-03-25 20:15:56,826 - INFO - [TRAIN INFO] ============================== Epoch 12/50 ==============================\n",
            "2025-03-25 20:16:02,634 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2262\n",
            "2025-03-25 20:16:10,296 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3580\n",
            "2025-03-25 20:16:17,972 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3341\n",
            "2025-03-25 20:16:25,788 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3377\n",
            "2025-03-25 20:16:33,230 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3373\n",
            "2025-03-25 20:16:41,050 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3262\n",
            "2025-03-25 20:16:48,647 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3351\n",
            "2025-03-25 20:16:56,422 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2984\n",
            "2025-03-25 20:17:04,093 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3433\n",
            "2025-03-25 20:17:11,847 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3775\n",
            "2025-03-25 20:17:19,453 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3082\n",
            "2025-03-25 20:17:27,190 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3439\n",
            "2025-03-25 20:17:34,799 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3561\n",
            "2025-03-25 20:17:42,455 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3314\n",
            "2025-03-25 20:17:49,857 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3466\n",
            "2025-03-25 20:17:57,376 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3275\n",
            "2025-03-25 20:18:05,074 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3045\n",
            "2025-03-25 20:18:13,254 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3472\n",
            "2025-03-25 20:18:21,242 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3102\n",
            "2025-03-25 20:18:28,864 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3063\n",
            "2025-03-25 20:18:36,682 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3272\n",
            "2025-03-25 20:18:44,286 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3255\n",
            "2025-03-25 20:18:52,067 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3300\n",
            "2025-03-25 20:18:59,844 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3609\n",
            "2025-03-25 20:19:07,421 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3546\n",
            "2025-03-25 20:19:15,209 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3282\n",
            "2025-03-25 20:19:23,031 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3451\n",
            "2025-03-25 20:19:30,775 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3924\n",
            "2025-03-25 20:19:38,584 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3117\n",
            "2025-03-25 20:19:46,219 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3781\n",
            "2025-03-25 20:19:53,850 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3412\n",
            "2025-03-25 20:20:01,532 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3440\n",
            "2025-03-25 20:20:09,224 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.3963\n",
            "2025-03-25 20:20:15,172 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2280\n",
            "2025-03-25 20:20:15,720 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0771\n",
            "2025-03-25 20:20:15,721 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:21:13,052 - INFO - [TRAIN INFO] Epoch 12/50, Train Loss: 0.3377, Val Loss: 0.3999, Val Acc: 0.8660\n",
            "2025-03-25 20:21:13,053 - INFO - [TRAIN INFO] ============================== Epoch 13/50 ==============================\n",
            "2025-03-25 20:21:18,927 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2521\n",
            "2025-03-25 20:21:26,589 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3368\n",
            "2025-03-25 20:21:34,007 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.3059\n",
            "2025-03-25 20:21:41,978 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.3009\n",
            "2025-03-25 20:21:49,526 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3063\n",
            "2025-03-25 20:21:57,185 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2972\n",
            "2025-03-25 20:22:04,783 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3354\n",
            "2025-03-25 20:22:12,690 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3570\n",
            "2025-03-25 20:22:20,174 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3470\n",
            "2025-03-25 20:22:27,790 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3004\n",
            "2025-03-25 20:22:35,373 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3604\n",
            "2025-03-25 20:22:42,918 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3609\n",
            "2025-03-25 20:22:50,779 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3277\n",
            "2025-03-25 20:22:58,466 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2964\n",
            "2025-03-25 20:23:06,155 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3356\n",
            "2025-03-25 20:23:13,711 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3235\n",
            "2025-03-25 20:23:21,447 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3537\n",
            "2025-03-25 20:23:28,926 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3070\n",
            "2025-03-25 20:23:36,615 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2911\n",
            "2025-03-25 20:23:44,321 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3174\n",
            "2025-03-25 20:23:52,147 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3187\n",
            "2025-03-25 20:23:59,927 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3338\n",
            "2025-03-25 20:24:07,564 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3549\n",
            "2025-03-25 20:24:15,433 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3418\n",
            "2025-03-25 20:24:23,051 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3215\n",
            "2025-03-25 20:24:30,756 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3711\n",
            "2025-03-25 20:24:38,353 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3763\n",
            "2025-03-25 20:24:46,253 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3251\n",
            "2025-03-25 20:24:53,752 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.3651\n",
            "2025-03-25 20:25:01,522 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3326\n",
            "2025-03-25 20:25:08,801 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3192\n",
            "2025-03-25 20:25:16,597 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3323\n",
            "2025-03-25 20:25:24,437 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2611\n",
            "2025-03-25 20:25:30,116 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2518\n",
            "2025-03-25 20:25:30,684 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0795\n",
            "2025-03-25 20:25:30,685 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:26:27,235 - INFO - [TRAIN INFO] Epoch 13/50, Train Loss: 0.3288, Val Loss: 0.3803, Val Acc: 0.8702\n",
            "2025-03-25 20:26:27,236 - INFO - [TRAIN INFO] ============================== Epoch 14/50 ==============================\n",
            "2025-03-25 20:26:33,024 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2416\n",
            "2025-03-25 20:26:40,639 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-25 20:26:48,373 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2927\n",
            "2025-03-25 20:26:55,975 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2864\n",
            "2025-03-25 20:27:03,859 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3489\n",
            "2025-03-25 20:27:11,485 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2739\n",
            "2025-03-25 20:27:18,979 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.3118\n",
            "2025-03-25 20:27:26,679 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.3254\n",
            "2025-03-25 20:27:33,982 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3205\n",
            "2025-03-25 20:27:41,624 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3382\n",
            "2025-03-25 20:27:49,088 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.3067\n",
            "2025-03-25 20:27:56,820 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.3415\n",
            "2025-03-25 20:28:04,591 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.3292\n",
            "2025-03-25 20:28:12,387 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.3141\n",
            "2025-03-25 20:28:20,067 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.3070\n",
            "2025-03-25 20:28:27,659 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.3179\n",
            "2025-03-25 20:28:35,271 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.3035\n",
            "2025-03-25 20:28:42,961 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.3018\n",
            "2025-03-25 20:28:50,654 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3042\n",
            "2025-03-25 20:28:58,262 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.3093\n",
            "2025-03-25 20:29:05,825 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2986\n",
            "2025-03-25 20:29:13,553 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3226\n",
            "2025-03-25 20:29:21,659 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.3317\n",
            "2025-03-25 20:29:29,869 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.3130\n",
            "2025-03-25 20:29:38,251 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3387\n",
            "2025-03-25 20:29:46,277 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.3302\n",
            "2025-03-25 20:29:54,451 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.3096\n",
            "2025-03-25 20:30:02,766 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.3290\n",
            "2025-03-25 20:30:10,867 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2986\n",
            "2025-03-25 20:30:19,110 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.3453\n",
            "2025-03-25 20:30:27,267 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3299\n",
            "2025-03-25 20:30:35,635 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.3260\n",
            "2025-03-25 20:30:43,805 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2976\n",
            "2025-03-25 20:30:50,124 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2396\n",
            "2025-03-25 20:30:50,723 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0989\n",
            "2025-03-25 20:30:50,724 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:31:49,152 - INFO - [TRAIN INFO] Epoch 14/50, Train Loss: 0.3165, Val Loss: 0.4005, Val Acc: 0.8646\n",
            "2025-03-25 20:31:49,153 - INFO - [TRAIN INFO] ============================== Epoch 15/50 ==============================\n",
            "2025-03-25 20:31:55,164 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.2366\n",
            "2025-03-25 20:32:03,212 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.3030\n",
            "2025-03-25 20:32:11,455 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2861\n",
            "2025-03-25 20:32:19,578 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2979\n",
            "2025-03-25 20:32:27,939 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.3115\n",
            "2025-03-25 20:32:36,193 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3285\n",
            "2025-03-25 20:32:44,300 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2821\n",
            "2025-03-25 20:32:52,596 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2750\n",
            "2025-03-25 20:33:00,693 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.3234\n",
            "2025-03-25 20:33:09,031 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.3118\n",
            "2025-03-25 20:33:17,391 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2836\n",
            "2025-03-25 20:33:25,572 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2817\n",
            "2025-03-25 20:33:33,779 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-25 20:33:41,932 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2766\n",
            "2025-03-25 20:33:50,378 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2689\n",
            "2025-03-25 20:33:58,408 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2815\n",
            "2025-03-25 20:34:06,578 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2681\n",
            "2025-03-25 20:34:14,705 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-25 20:34:22,676 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2826\n",
            "2025-03-25 20:34:30,795 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2882\n",
            "2025-03-25 20:34:38,988 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.3011\n",
            "2025-03-25 20:34:47,100 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.3075\n",
            "2025-03-25 20:34:55,309 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2894\n",
            "2025-03-25 20:35:03,560 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2502\n",
            "2025-03-25 20:35:11,747 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.3128\n",
            "2025-03-25 20:35:19,941 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2734\n",
            "2025-03-25 20:35:28,143 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2716\n",
            "2025-03-25 20:35:36,654 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2525\n",
            "2025-03-25 20:35:44,830 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2741\n",
            "2025-03-25 20:35:53,136 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2715\n",
            "2025-03-25 20:36:01,534 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.3019\n",
            "2025-03-25 20:36:09,789 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2923\n",
            "2025-03-25 20:36:17,924 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2709\n",
            "2025-03-25 20:36:24,103 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2043\n",
            "2025-03-25 20:36:24,758 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0793\n",
            "2025-03-25 20:36:24,759 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:37:23,390 - INFO - [TRAIN INFO] Epoch 15/50, Train Loss: 0.2869, Val Loss: 0.3748, Val Acc: 0.8781\n",
            "2025-03-25 20:37:23,390 - INFO - [TRAIN INFO] ============================== Epoch 16/50 ==============================\n",
            "2025-03-25 20:37:29,485 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1832\n",
            "2025-03-25 20:37:37,813 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2445\n",
            "2025-03-25 20:37:45,834 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2772\n",
            "2025-03-25 20:37:54,064 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2792\n",
            "2025-03-25 20:38:02,102 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2673\n",
            "2025-03-25 20:38:10,411 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.3026\n",
            "2025-03-25 20:38:18,696 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-25 20:38:26,680 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2540\n",
            "2025-03-25 20:38:35,087 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2712\n",
            "2025-03-25 20:38:43,481 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2587\n",
            "2025-03-25 20:38:51,653 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2684\n",
            "2025-03-25 20:38:59,744 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2809\n",
            "2025-03-25 20:39:07,907 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2620\n",
            "2025-03-25 20:39:16,067 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2798\n",
            "2025-03-25 20:39:24,185 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2872\n",
            "2025-03-25 20:39:32,009 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2776\n",
            "2025-03-25 20:39:40,025 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2629\n",
            "2025-03-25 20:39:48,198 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-25 20:39:56,314 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.3141\n",
            "2025-03-25 20:40:04,432 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2835\n",
            "2025-03-25 20:40:12,491 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2713\n",
            "2025-03-25 20:40:20,462 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2913\n",
            "2025-03-25 20:40:28,399 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2536\n",
            "2025-03-25 20:40:36,358 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2679\n",
            "2025-03-25 20:40:44,477 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2386\n",
            "2025-03-25 20:40:52,628 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2680\n",
            "2025-03-25 20:41:00,679 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2727\n",
            "2025-03-25 20:41:08,587 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2644\n",
            "2025-03-25 20:41:16,647 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2538\n",
            "2025-03-25 20:41:24,638 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 20:41:32,618 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2688\n",
            "2025-03-25 20:41:40,836 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2613\n",
            "2025-03-25 20:41:49,005 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2759\n",
            "2025-03-25 20:41:55,063 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2004\n",
            "2025-03-25 20:41:55,650 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0618\n",
            "2025-03-25 20:41:55,651 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:42:54,267 - INFO - [TRAIN INFO] Epoch 16/50, Train Loss: 0.2701, Val Loss: 0.3773, Val Acc: 0.8790\n",
            "2025-03-25 20:42:54,268 - INFO - [TRAIN INFO] ============================== Epoch 17/50 ==============================\n",
            "2025-03-25 20:43:00,329 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1848\n",
            "2025-03-25 20:43:08,604 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2686\n",
            "2025-03-25 20:43:16,788 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2456\n",
            "2025-03-25 20:43:24,582 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2742\n",
            "2025-03-25 20:43:32,190 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2636\n",
            "2025-03-25 20:43:39,807 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2457\n",
            "2025-03-25 20:43:47,449 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2529\n",
            "2025-03-25 20:43:54,938 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2694\n",
            "2025-03-25 20:44:02,540 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2886\n",
            "2025-03-25 20:44:10,335 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2766\n",
            "2025-03-25 20:44:17,973 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2646\n",
            "2025-03-25 20:44:25,390 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2485\n",
            "2025-03-25 20:44:33,197 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2793\n",
            "2025-03-25 20:44:40,833 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 20:44:48,387 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2542\n",
            "2025-03-25 20:44:55,975 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2806\n",
            "2025-03-25 20:45:03,593 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2614\n",
            "2025-03-25 20:45:11,555 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2738\n",
            "2025-03-25 20:45:19,147 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2881\n",
            "2025-03-25 20:45:26,843 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2467\n",
            "2025-03-25 20:45:34,668 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2548\n",
            "2025-03-25 20:45:42,367 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2844\n",
            "2025-03-25 20:45:50,345 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2971\n",
            "2025-03-25 20:45:57,948 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2604\n",
            "2025-03-25 20:46:05,731 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2474\n",
            "2025-03-25 20:46:13,456 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2800\n",
            "2025-03-25 20:46:21,124 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2584\n",
            "2025-03-25 20:46:28,693 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2511\n",
            "2025-03-25 20:46:36,724 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2383\n",
            "2025-03-25 20:46:44,341 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2431\n",
            "2025-03-25 20:46:51,904 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2678\n",
            "2025-03-25 20:46:59,613 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2414\n",
            "2025-03-25 20:47:07,106 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2502\n",
            "2025-03-25 20:47:13,114 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.2004\n",
            "2025-03-25 20:47:13,675 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0604\n",
            "2025-03-25 20:47:13,676 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:48:11,164 - INFO - [TRAIN INFO] Epoch 17/50, Train Loss: 0.2628, Val Loss: 0.3740, Val Acc: 0.8785\n",
            "2025-03-25 20:48:11,165 - INFO - [TRAIN INFO] ============================== Epoch 18/50 ==============================\n",
            "2025-03-25 20:48:17,470 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1904\n",
            "2025-03-25 20:48:25,257 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2465\n",
            "2025-03-25 20:48:33,001 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2491\n",
            "2025-03-25 20:48:40,978 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2477\n",
            "2025-03-25 20:48:49,030 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2939\n",
            "2025-03-25 20:48:56,723 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2808\n",
            "2025-03-25 20:49:04,580 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2682\n",
            "2025-03-25 20:49:12,486 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2752\n",
            "2025-03-25 20:49:20,162 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2662\n",
            "2025-03-25 20:49:28,076 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2608\n",
            "2025-03-25 20:49:35,677 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2665\n",
            "2025-03-25 20:49:43,377 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2444\n",
            "2025-03-25 20:49:51,212 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2764\n",
            "2025-03-25 20:49:58,993 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2721\n",
            "2025-03-25 20:50:06,783 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2468\n",
            "2025-03-25 20:50:14,177 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2472\n",
            "2025-03-25 20:50:21,940 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2467\n",
            "2025-03-25 20:50:29,463 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2793\n",
            "2025-03-25 20:50:37,146 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2396\n",
            "2025-03-25 20:50:44,664 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2523\n",
            "2025-03-25 20:50:52,359 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2627\n",
            "2025-03-25 20:51:00,212 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2656\n",
            "2025-03-25 20:51:07,841 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2534\n",
            "2025-03-25 20:51:15,579 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2522\n",
            "2025-03-25 20:51:23,196 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2456\n",
            "2025-03-25 20:51:30,830 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2427\n",
            "2025-03-25 20:51:38,437 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2527\n",
            "2025-03-25 20:51:46,312 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2380\n",
            "2025-03-25 20:51:53,904 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2543\n",
            "2025-03-25 20:52:01,588 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2604\n",
            "2025-03-25 20:52:09,125 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2548\n",
            "2025-03-25 20:52:17,160 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2595\n",
            "2025-03-25 20:52:24,737 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2491\n",
            "2025-03-25 20:52:30,314 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1862\n",
            "2025-03-25 20:52:30,893 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0605\n",
            "2025-03-25 20:52:30,894 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:53:26,483 - INFO - [TRAIN INFO] Epoch 18/50, Train Loss: 0.2574, Val Loss: 0.3910, Val Acc: 0.8781\n",
            "2025-03-25 20:53:26,483 - INFO - [TRAIN INFO] ============================== Epoch 19/50 ==============================\n",
            "2025-03-25 20:53:32,009 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1811\n",
            "2025-03-25 20:53:39,473 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2645\n",
            "2025-03-25 20:53:47,144 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2507\n",
            "2025-03-25 20:53:54,532 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2785\n",
            "2025-03-25 20:54:02,131 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2491\n",
            "2025-03-25 20:54:09,561 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2722\n",
            "2025-03-25 20:54:17,178 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2594\n",
            "2025-03-25 20:54:24,787 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2510\n",
            "2025-03-25 20:54:32,354 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2445\n",
            "2025-03-25 20:54:39,936 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2460\n",
            "2025-03-25 20:54:47,377 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2528\n",
            "2025-03-25 20:54:54,906 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2404\n",
            "2025-03-25 20:55:02,438 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2499\n",
            "2025-03-25 20:55:09,980 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2477\n",
            "2025-03-25 20:55:17,574 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2476\n",
            "2025-03-25 20:55:24,916 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2565\n",
            "2025-03-25 20:55:32,566 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2601\n",
            "2025-03-25 20:55:40,336 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2399\n",
            "2025-03-25 20:55:47,823 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2384\n",
            "2025-03-25 20:55:55,366 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2514\n",
            "2025-03-25 20:56:02,704 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2414\n",
            "2025-03-25 20:56:10,358 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 20:56:17,674 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2604\n",
            "2025-03-25 20:56:25,350 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2521\n",
            "2025-03-25 20:56:32,880 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2494\n",
            "2025-03-25 20:56:40,520 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2581\n",
            "2025-03-25 20:56:47,662 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2375\n",
            "2025-03-25 20:56:55,332 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2635\n",
            "2025-03-25 20:57:02,860 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2434\n",
            "2025-03-25 20:57:10,840 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2485\n",
            "2025-03-25 20:57:18,442 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2444\n",
            "2025-03-25 20:57:26,203 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2535\n",
            "2025-03-25 20:57:33,936 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2340\n",
            "2025-03-25 20:57:40,094 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1900\n",
            "2025-03-25 20:57:40,663 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1312\n",
            "2025-03-25 20:57:40,664 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 20:58:36,688 - INFO - [TRAIN INFO] Epoch 19/50, Train Loss: 0.2526, Val Loss: 0.3790, Val Acc: 0.8837\n",
            "2025-03-25 20:58:36,689 - INFO - [TRAIN INFO] ============================== Epoch 20/50 ==============================\n",
            "2025-03-25 20:58:42,874 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.1760\n",
            "2025-03-25 20:58:50,812 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.2364\n",
            "2025-03-25 20:58:58,466 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.2492\n",
            "2025-03-25 20:59:06,048 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-25 20:59:13,855 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.2581\n",
            "2025-03-25 20:59:21,677 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.2263\n",
            "2025-03-25 20:59:29,484 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.2496\n",
            "2025-03-25 20:59:37,083 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.2407\n",
            "2025-03-25 20:59:44,898 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.2406\n",
            "2025-03-25 20:59:52,709 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.2385\n",
            "2025-03-25 21:00:00,410 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.2507\n",
            "2025-03-25 21:00:08,081 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.2447\n",
            "2025-03-25 21:00:15,497 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.2392\n",
            "2025-03-25 21:00:23,052 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 21:00:30,228 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.2615\n",
            "2025-03-25 21:00:38,042 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.2312\n",
            "2025-03-25 21:00:45,476 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.2324\n",
            "2025-03-25 21:00:52,873 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.2339\n",
            "2025-03-25 21:01:00,301 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.2516\n",
            "2025-03-25 21:01:08,066 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.2424\n",
            "2025-03-25 21:01:15,498 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.2420\n",
            "2025-03-25 21:01:22,861 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.2494\n",
            "2025-03-25 21:01:30,199 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.2480\n",
            "2025-03-25 21:01:37,588 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.2495\n",
            "2025-03-25 21:01:45,249 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.2444\n",
            "2025-03-25 21:01:52,593 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.2397\n",
            "2025-03-25 21:02:00,238 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.2381\n",
            "2025-03-25 21:02:07,589 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.2355\n",
            "2025-03-25 21:02:15,160 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.2408\n",
            "2025-03-25 21:02:22,810 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.2603\n",
            "2025-03-25 21:02:30,162 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.2409\n",
            "2025-03-25 21:02:37,804 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.2505\n",
            "2025-03-25 21:02:45,638 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.2438\n",
            "2025-03-25 21:02:51,193 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.1892\n",
            "2025-03-25 21:02:51,715 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.0770\n",
            "2025-03-25 21:02:51,716 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 21:03:47,334 - INFO - [TRAIN INFO] Epoch 20/50, Train Loss: 0.2445, Val Loss: 0.3855, Val Acc: 0.8827\n",
            "2025-03-25 21:03:47,335 - INFO - [TRAIN INFO] Early stopping at epoch 20 as validation loss did not improve for 10 epochs.\n",
            "2025-03-25 21:03:47,335 - INFO - [TRAIN INFO] Total Time: 6401.54s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>learning_rate_classifier</td><td></td></tr><tr><td>learning_rate_fusion</td><td></td></tr><tr><td>learning_rate_image</td><td></td></tr><tr><td>learning_rate_text</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>train_val_loss_diff</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopping_epochs</td><td>9</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate_classifier</td><td>0.00045</td></tr><tr><td>learning_rate_fusion</td><td>9e-05</td></tr><tr><td>learning_rate_image</td><td>9e-05</td></tr><tr><td>learning_rate_text</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.2445</td></tr><tr><td>train_val_loss_diff</td><td>-0.14096</td></tr><tr><td>val_accuracy</td><td>0.88274</td></tr><tr><td>val_loss</td><td>0.38546</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_multimodal_gated_only_fold_1</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/dmnq6asf' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/dmnq6asf</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250325_191704-dmnq6asf\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 21:03:49,705 - INFO - [TRAIN INFO] Fold 1 Training Complete at epoch 20. Total Time: 6403.91s\n",
            "2025-03-25 21:03:49,723 - INFO - [K-FOLD INFO] Fold 1 completed in 6406.84 seconds\n",
            "2025-03-25 21:03:49,724 - INFO - [K-FOLD INFO] ============================== Fold 2/5 ==============================\n",
            "2025-03-25 21:03:49,726 - INFO - [K-FOLD INFO] Fold 2:\n",
            "2025-03-25 21:03:49,726 - INFO -    Train Samples: 8594\n",
            "2025-03-25 21:03:49,727 - INFO -    Validation Samples: 2149\n",
            "2025-03-25 21:03:49,727 - INFO - [K-FOLD INFO] Created multimodal datasets for Fold 2\n",
            "2025-03-25 21:03:49,728 - INFO - [K-FOLD INFO] DataLoaders initialized for Fold 2:\n",
            "2025-03-25 21:03:49,729 - INFO -    Train batches: 135, Validation batches: 34\n",
            "2025-03-25 21:03:50,464 - INFO - [K-FOLD INFO] Model initialized on cuda for Fold 2\n",
            "2025-03-25 21:03:50,466 - INFO - [K-FOLD INFO] Optimizer initialized for Fold 2:\n",
            "2025-03-25 21:03:50,466 - INFO - [K-FOLD INFO] Loss function initialized for Fold 2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\iCloudDrive\\iCloud Documents\\2. WINTER\\ENEL 645 - Data Mining and Machine Learning\\Project\\multimodal_gated_only\\wandb\\run-20250325_210350-esns79rw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/esns79rw' target=\"_blank\">experiment_multimodal_gated_only_fold_2</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/esns79rw' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/esns79rw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 21:03:51,187 - INFO - [TRAIN INFO] Starting Training...\n",
            "2025-03-25 21:03:51,188 - INFO - [TRAIN INFO] ============================== Epoch 1/50 ==============================\n",
            "2025-03-25 21:03:57,623 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 1.0719\n",
            "2025-03-25 21:04:06,017 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 1.3964\n",
            "2025-03-25 21:04:14,196 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 1.3534\n",
            "2025-03-25 21:04:21,867 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 1.3247\n",
            "2025-03-25 21:04:29,611 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 1.3092\n",
            "2025-03-25 21:04:37,404 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 1.2952\n",
            "2025-03-25 21:04:45,209 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 1.2671\n",
            "2025-03-25 21:04:52,821 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 1.2432\n",
            "2025-03-25 21:05:00,619 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 1.2396\n",
            "2025-03-25 21:05:08,198 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 1.2210\n",
            "2025-03-25 21:05:15,780 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 1.1760\n",
            "2025-03-25 21:05:23,978 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 1.1969\n",
            "2025-03-25 21:05:31,924 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 1.1399\n",
            "2025-03-25 21:05:39,942 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 1.1126\n",
            "2025-03-25 21:05:48,149 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 1.1519\n",
            "2025-03-25 21:05:56,368 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 1.1457\n",
            "2025-03-25 21:06:04,345 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 1.1407\n",
            "2025-03-25 21:06:12,541 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 1.0616\n",
            "2025-03-25 21:06:20,783 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 1.1089\n",
            "2025-03-25 21:06:28,574 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 1.0909\n",
            "2025-03-25 21:06:36,782 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 1.0302\n",
            "2025-03-25 21:06:45,176 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 1.0088\n",
            "2025-03-25 21:06:53,312 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.9802\n",
            "2025-03-25 21:07:01,366 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 1.0129\n",
            "2025-03-25 21:07:09,356 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 1.0179\n",
            "2025-03-25 21:07:17,497 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.9633\n",
            "2025-03-25 21:07:25,676 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.9935\n",
            "2025-03-25 21:07:33,678 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.9861\n",
            "2025-03-25 21:07:41,564 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.9710\n",
            "2025-03-25 21:07:49,711 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.9293\n",
            "2025-03-25 21:07:57,689 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.9587\n",
            "2025-03-25 21:08:05,537 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.9192\n",
            "2025-03-25 21:08:13,137 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.9186\n",
            "2025-03-25 21:08:18,886 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.7056\n",
            "2025-03-25 21:08:19,476 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.2423\n",
            "2025-03-25 21:08:19,477 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 21:09:16,002 - INFO - [TRAIN INFO] Epoch 1/50, Train Loss: 1.1166, Val Loss: 0.7831, Val Acc: 0.6985\n",
            "2025-03-25 21:09:16,288 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 21:09:16,289 - INFO - [TRAIN INFO] ============================== Epoch 2/50 ==============================\n",
            "2025-03-25 21:09:22,353 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.6980\n",
            "2025-03-25 21:09:30,251 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.8995\n",
            "2025-03-25 21:09:38,279 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.8510\n",
            "2025-03-25 21:09:46,069 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.9021\n",
            "2025-03-25 21:09:54,095 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.8236\n",
            "2025-03-25 21:10:01,900 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.8747\n",
            "2025-03-25 21:10:09,855 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.8468\n",
            "2025-03-25 21:10:17,851 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.8553\n",
            "2025-03-25 21:10:25,724 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.7942\n",
            "2025-03-25 21:10:33,687 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.8260\n",
            "2025-03-25 21:10:42,169 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.8017\n",
            "2025-03-25 21:10:50,458 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7951\n",
            "2025-03-25 21:10:58,333 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.8253\n",
            "2025-03-25 21:11:06,277 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.7884\n",
            "2025-03-25 21:11:13,820 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.8136\n",
            "2025-03-25 21:11:21,450 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.8160\n",
            "2025-03-25 21:11:29,252 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.7944\n",
            "2025-03-25 21:11:36,942 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.7690\n",
            "2025-03-25 21:11:44,678 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.7718\n",
            "2025-03-25 21:11:52,149 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.7633\n",
            "2025-03-25 21:12:00,056 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.8260\n",
            "2025-03-25 21:12:07,465 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.8833\n",
            "2025-03-25 21:12:14,989 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.7972\n",
            "2025-03-25 21:12:22,573 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.8141\n",
            "2025-03-25 21:12:30,147 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.7476\n",
            "2025-03-25 21:12:37,853 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7765\n",
            "2025-03-25 21:12:45,411 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.7306\n",
            "2025-03-25 21:12:52,850 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.7796\n",
            "2025-03-25 21:13:00,449 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.7987\n",
            "2025-03-25 21:13:07,977 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.7970\n",
            "2025-03-25 21:13:15,645 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.7325\n",
            "2025-03-25 21:13:23,243 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6687\n",
            "2025-03-25 21:13:30,624 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.7817\n",
            "2025-03-25 21:13:36,403 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.5547\n",
            "2025-03-25 21:13:36,945 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1846\n",
            "2025-03-25 21:13:36,946 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 21:14:32,547 - INFO - [TRAIN INFO] Epoch 2/50, Train Loss: 0.8054, Val Loss: 0.5766, Val Acc: 0.7711\n",
            "2025-03-25 21:14:32,841 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 21:14:32,841 - INFO - [TRAIN INFO] ============================== Epoch 3/50 ==============================\n",
            "2025-03-25 21:14:38,732 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.5022\n",
            "2025-03-25 21:14:46,398 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.6598\n",
            "2025-03-25 21:14:54,001 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.6889\n",
            "2025-03-25 21:15:02,104 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.6965\n",
            "2025-03-25 21:15:09,609 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.6582\n",
            "2025-03-25 21:15:17,209 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.6384\n",
            "2025-03-25 21:15:24,717 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6923\n",
            "2025-03-25 21:15:32,360 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.6944\n",
            "2025-03-25 21:15:40,202 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6486\n",
            "2025-03-25 21:15:47,726 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.6734\n",
            "2025-03-25 21:15:55,282 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.6608\n",
            "2025-03-25 21:16:02,992 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.7282\n",
            "2025-03-25 21:16:10,442 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.6826\n",
            "2025-03-25 21:16:18,122 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.6245\n",
            "2025-03-25 21:16:25,779 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.6159\n",
            "2025-03-25 21:16:33,384 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.6142\n",
            "2025-03-25 21:16:40,791 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6548\n",
            "2025-03-25 21:16:48,402 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.6604\n",
            "2025-03-25 21:16:55,965 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.6707\n",
            "2025-03-25 21:17:03,745 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6258\n",
            "2025-03-25 21:17:10,919 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.7125\n",
            "2025-03-25 21:17:18,611 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6178\n",
            "2025-03-25 21:17:26,104 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.6592\n",
            "2025-03-25 21:17:33,685 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.6398\n",
            "2025-03-25 21:17:40,942 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.6360\n",
            "2025-03-25 21:17:48,637 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.7118\n",
            "2025-03-25 21:17:56,256 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.6239\n",
            "2025-03-25 21:18:03,852 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.6260\n",
            "2025-03-25 21:18:11,728 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.6399\n",
            "2025-03-25 21:18:19,172 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.6087\n",
            "2025-03-25 21:18:26,735 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5953\n",
            "2025-03-25 21:18:34,283 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6464\n",
            "2025-03-25 21:18:41,947 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.6508\n",
            "2025-03-25 21:18:47,714 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4602\n",
            "2025-03-25 21:18:48,260 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1621\n",
            "2025-03-25 21:18:48,260 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 21:19:44,983 - INFO - [TRAIN INFO] Epoch 3/50, Train Loss: 0.6542, Val Loss: 0.4581, Val Acc: 0.8246\n",
            "2025-03-25 21:19:45,276 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 21:19:45,276 - INFO - [TRAIN INFO] ============================== Epoch 4/50 ==============================\n",
            "2025-03-25 21:19:51,217 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4225\n",
            "2025-03-25 21:19:58,956 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5259\n",
            "2025-03-25 21:20:06,713 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5088\n",
            "2025-03-25 21:20:14,511 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5240\n",
            "2025-03-25 21:20:22,222 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5547\n",
            "2025-03-25 21:20:29,925 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5632\n",
            "2025-03-25 21:20:37,767 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.6494\n",
            "2025-03-25 21:20:45,305 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.5655\n",
            "2025-03-25 21:20:52,927 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.6076\n",
            "2025-03-25 21:21:00,631 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.5892\n",
            "2025-03-25 21:21:08,332 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5960\n",
            "2025-03-25 21:21:15,856 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.5689\n",
            "2025-03-25 21:21:23,418 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.5429\n",
            "2025-03-25 21:21:30,871 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5653\n",
            "2025-03-25 21:21:38,493 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.4887\n",
            "2025-03-25 21:21:46,263 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5553\n",
            "2025-03-25 21:21:53,697 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.6356\n",
            "2025-03-25 21:22:01,474 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5863\n",
            "2025-03-25 21:22:09,075 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5375\n",
            "2025-03-25 21:22:16,456 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.6224\n",
            "2025-03-25 21:22:24,252 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5962\n",
            "2025-03-25 21:22:31,764 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.6133\n",
            "2025-03-25 21:22:39,172 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5109\n",
            "2025-03-25 21:22:46,774 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.5804\n",
            "2025-03-25 21:22:54,627 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5996\n",
            "2025-03-25 21:23:02,264 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5488\n",
            "2025-03-25 21:23:09,652 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5803\n",
            "2025-03-25 21:23:17,253 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5527\n",
            "2025-03-25 21:23:24,630 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5779\n",
            "2025-03-25 21:23:32,262 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.5875\n",
            "2025-03-25 21:23:39,638 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5765\n",
            "2025-03-25 21:23:47,250 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5797\n",
            "2025-03-25 21:23:55,234 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5762\n",
            "2025-03-25 21:24:00,852 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3958\n",
            "2025-03-25 21:24:01,461 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1816\n",
            "2025-03-25 21:24:01,461 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 21:24:59,281 - INFO - [TRAIN INFO] Epoch 4/50, Train Loss: 0.5709, Val Loss: 0.4024, Val Acc: 0.8450\n",
            "2025-03-25 21:24:59,575 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 21:24:59,575 - INFO - [TRAIN INFO] ============================== Epoch 5/50 ==============================\n",
            "2025-03-25 21:25:05,613 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.4061\n",
            "2025-03-25 21:25:13,364 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.5125\n",
            "2025-03-25 21:25:21,051 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.5210\n",
            "2025-03-25 21:25:28,846 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.4887\n",
            "2025-03-25 21:25:36,420 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5433\n",
            "2025-03-25 21:25:44,035 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.5323\n",
            "2025-03-25 21:25:51,786 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.5006\n",
            "2025-03-25 21:25:59,210 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4949\n",
            "2025-03-25 21:26:06,646 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.4981\n",
            "2025-03-25 21:26:14,371 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4715\n",
            "2025-03-25 21:26:22,006 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.5419\n",
            "2025-03-25 21:26:29,587 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4781\n",
            "2025-03-25 21:26:37,393 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4721\n",
            "2025-03-25 21:26:44,998 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.5058\n",
            "2025-03-25 21:26:52,572 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5442\n",
            "2025-03-25 21:27:00,111 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.5251\n",
            "2025-03-25 21:27:07,790 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.5440\n",
            "2025-03-25 21:27:15,249 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.5605\n",
            "2025-03-25 21:27:22,787 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.5362\n",
            "2025-03-25 21:27:30,233 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5709\n",
            "2025-03-25 21:27:37,783 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5553\n",
            "2025-03-25 21:27:45,157 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.5109\n",
            "2025-03-25 21:27:52,778 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.5240\n",
            "2025-03-25 21:28:00,220 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4608\n",
            "2025-03-25 21:28:07,772 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.5042\n",
            "2025-03-25 21:28:15,260 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.5781\n",
            "2025-03-25 21:28:22,959 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.5139\n",
            "2025-03-25 21:28:30,586 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.5510\n",
            "2025-03-25 21:28:38,218 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.5777\n",
            "2025-03-25 21:28:45,574 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4714\n",
            "2025-03-25 21:28:53,158 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5491\n",
            "2025-03-25 21:29:00,589 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.5556\n",
            "2025-03-25 21:29:08,096 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.4918\n",
            "2025-03-25 21:29:13,636 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.4368\n",
            "2025-03-25 21:29:14,207 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1554\n",
            "2025-03-25 21:29:14,208 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 21:30:09,179 - INFO - [TRAIN INFO] Epoch 5/50, Train Loss: 0.5240, Val Loss: 0.3665, Val Acc: 0.8651\n",
            "2025-03-25 21:30:09,470 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 21:30:09,470 - INFO - [TRAIN INFO] ============================== Epoch 6/50 ==============================\n",
            "2025-03-25 21:30:15,308 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3637\n",
            "2025-03-25 21:30:23,136 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4922\n",
            "2025-03-25 21:30:31,117 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4444\n",
            "2025-03-25 21:30:38,839 - INFO - [TRAIN INFO] Batch 16/135, Accumulated loss over 4 batches: 0.5074\n",
            "2025-03-25 21:30:46,913 - INFO - [TRAIN INFO] Batch 20/135, Accumulated loss over 4 batches: 0.5362\n",
            "2025-03-25 21:30:54,834 - INFO - [TRAIN INFO] Batch 24/135, Accumulated loss over 4 batches: 0.4747\n",
            "2025-03-25 21:31:02,607 - INFO - [TRAIN INFO] Batch 28/135, Accumulated loss over 4 batches: 0.4641\n",
            "2025-03-25 21:31:10,720 - INFO - [TRAIN INFO] Batch 32/135, Accumulated loss over 4 batches: 0.4649\n",
            "2025-03-25 21:31:18,697 - INFO - [TRAIN INFO] Batch 36/135, Accumulated loss over 4 batches: 0.5027\n",
            "2025-03-25 21:31:26,693 - INFO - [TRAIN INFO] Batch 40/135, Accumulated loss over 4 batches: 0.4449\n",
            "2025-03-25 21:31:34,539 - INFO - [TRAIN INFO] Batch 44/135, Accumulated loss over 4 batches: 0.4811\n",
            "2025-03-25 21:31:42,628 - INFO - [TRAIN INFO] Batch 48/135, Accumulated loss over 4 batches: 0.4849\n",
            "2025-03-25 21:31:50,678 - INFO - [TRAIN INFO] Batch 52/135, Accumulated loss over 4 batches: 0.4723\n",
            "2025-03-25 21:31:58,873 - INFO - [TRAIN INFO] Batch 56/135, Accumulated loss over 4 batches: 0.4837\n",
            "2025-03-25 21:32:07,094 - INFO - [TRAIN INFO] Batch 60/135, Accumulated loss over 4 batches: 0.5548\n",
            "2025-03-25 21:32:14,835 - INFO - [TRAIN INFO] Batch 64/135, Accumulated loss over 4 batches: 0.4539\n",
            "2025-03-25 21:32:22,500 - INFO - [TRAIN INFO] Batch 68/135, Accumulated loss over 4 batches: 0.4482\n",
            "2025-03-25 21:32:30,246 - INFO - [TRAIN INFO] Batch 72/135, Accumulated loss over 4 batches: 0.4839\n",
            "2025-03-25 21:32:37,832 - INFO - [TRAIN INFO] Batch 76/135, Accumulated loss over 4 batches: 0.4306\n",
            "2025-03-25 21:32:45,255 - INFO - [TRAIN INFO] Batch 80/135, Accumulated loss over 4 batches: 0.5137\n",
            "2025-03-25 21:32:52,863 - INFO - [TRAIN INFO] Batch 84/135, Accumulated loss over 4 batches: 0.5262\n",
            "2025-03-25 21:33:00,691 - INFO - [TRAIN INFO] Batch 88/135, Accumulated loss over 4 batches: 0.4740\n",
            "2025-03-25 21:33:08,227 - INFO - [TRAIN INFO] Batch 92/135, Accumulated loss over 4 batches: 0.4709\n",
            "2025-03-25 21:33:16,081 - INFO - [TRAIN INFO] Batch 96/135, Accumulated loss over 4 batches: 0.4920\n",
            "2025-03-25 21:33:23,787 - INFO - [TRAIN INFO] Batch 100/135, Accumulated loss over 4 batches: 0.4780\n",
            "2025-03-25 21:33:31,478 - INFO - [TRAIN INFO] Batch 104/135, Accumulated loss over 4 batches: 0.4600\n",
            "2025-03-25 21:33:39,073 - INFO - [TRAIN INFO] Batch 108/135, Accumulated loss over 4 batches: 0.4371\n",
            "2025-03-25 21:33:46,665 - INFO - [TRAIN INFO] Batch 112/135, Accumulated loss over 4 batches: 0.4861\n",
            "2025-03-25 21:33:54,159 - INFO - [TRAIN INFO] Batch 116/135, Accumulated loss over 4 batches: 0.4441\n",
            "2025-03-25 21:34:01,684 - INFO - [TRAIN INFO] Batch 120/135, Accumulated loss over 4 batches: 0.4905\n",
            "2025-03-25 21:34:09,481 - INFO - [TRAIN INFO] Batch 124/135, Accumulated loss over 4 batches: 0.5190\n",
            "2025-03-25 21:34:17,043 - INFO - [TRAIN INFO] Batch 128/135, Accumulated loss over 4 batches: 0.6020\n",
            "2025-03-25 21:34:24,852 - INFO - [TRAIN INFO] Batch 132/135, Accumulated loss over 4 batches: 0.5016\n",
            "2025-03-25 21:34:30,470 - INFO - [TRAIN INFO] Batch 135/135, Accumulated loss over 4 batches: 0.3697\n",
            "2025-03-25 21:34:31,063 - INFO - [TRAIN INFO] Batch 136/135, Accumulated loss over 4 batches: 0.1206\n",
            "2025-03-25 21:34:31,063 - INFO - [TRAIN INFO] Evaluating model...\n",
            "2025-03-25 21:35:27,097 - INFO - [TRAIN INFO] Epoch 6/50, Train Loss: 0.4852, Val Loss: 0.3546, Val Acc: 0.8702\n",
            "2025-03-25 21:35:27,399 - INFO - [TRAIN INFO] Best Model Saved for Fold 2\n",
            "2025-03-25 21:35:27,400 - INFO - [TRAIN INFO] ============================== Epoch 7/50 ==============================\n",
            "2025-03-25 21:35:33,483 - INFO - [TRAIN INFO] Batch 4/135, Accumulated loss over 4 batches: 0.3135\n",
            "2025-03-25 21:35:41,262 - INFO - [TRAIN INFO] Batch 8/135, Accumulated loss over 4 batches: 0.4690\n",
            "2025-03-25 21:35:48,917 - INFO - [TRAIN INFO] Batch 12/135, Accumulated loss over 4 batches: 0.4163\n"
          ]
        }
      ],
      "source": [
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "logging.info(\"[K-FOLD INFO] Starting Stratified K-Fold Cross-Validation...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
        "\n",
        "    \n",
        "    fold_start_time = time.time()  # Start timing for this fold\n",
        "    logging.info(f\"[K-FOLD INFO] ============================== Fold {fold+1}/{K_FOLDS} ==============================\")\n",
        "\n",
        "    # Get train and validation subsets\n",
        "    train_texts_fold = train_texts[train_idx]\n",
        "    val_texts_fold = train_texts[val_idx]\n",
        "    train_labels_fold = train_labels[train_idx]\n",
        "    val_labels_fold = train_labels[val_idx]\n",
        "    train_image_paths_fold = train_image_paths[train_idx]\n",
        "    val_image_paths_fold = train_image_paths[val_idx]\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train Samples: {len(train_texts_fold)}\")\n",
        "    logging.info(f\"   Validation Samples: {len(val_texts_fold)}\")\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_image_dataset = ImageDataset(train_image_paths_fold, train_labels_fold, transform[\"train\"])\n",
        "    val_image_dataset = ImageDataset(val_image_paths_fold, val_labels_fold, transform[\"val\"])\n",
        "    \n",
        "    train_text_dataset = CustomTextDataset(train_texts_fold, train_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "    val_text_dataset = CustomTextDataset(val_texts_fold, val_labels_fold, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # Create multimodal datasets\n",
        "    train_multimodal_dataset = MultimodalDataset(train_image_dataset, train_text_dataset)\n",
        "    val_multimodal_dataset = MultimodalDataset(val_image_dataset, val_text_dataset)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Created multimodal datasets for Fold {fold+1}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    dataloaders = {\n",
        "        \"train_loader\": DataLoader(train_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=True),\n",
        "        \"val_loader\": DataLoader(val_multimodal_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    }\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] DataLoaders initialized for Fold {fold+1}:\")\n",
        "    logging.info(f\"   Train batches: {len(dataloaders['train_loader'])}, Validation batches: {len(dataloaders['val_loader'])}\")\n",
        "\n",
        "    # Initialize model, optimizer, and criterion\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Model initialized on {device} for Fold {fold+1}\")\n",
        "\n",
        "    # Define Optimizer using AdamW\n",
        "    optimizer = optim.AdamW([\n",
        "        {\"params\": model.image_model.features[-3:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_IMAGE, \"weight_decay\": WEIGHT_DECAY_IMAGE},  # Unfrozen EfficientNet layer\n",
        "        {\"params\": model.text_model.transformer.layer[-2:].parameters(), \"lr\": LEARNING_RATE_UNFREEZE_TEXT, \"weight_decay\": WEIGHT_DECAY_TEXT},  # Unfrozen DistilBERT layer\n",
        "        {\"params\": model.image_fc.parameters(), \"lr\": LEARNING_RATE_IMAGE, \"weight_decay\": 0}, \n",
        "        {\"params\": model.text_fc.parameters(), \"lr\": LEARNING_RATE_TEXT, \"weight_decay\": 0},\n",
        "        {\"params\": model.fusion_fc.parameters(), \"lr\": LEARNING_RATE_FUSION, \"weight_decay\": WEIGHT_DECAY_FUSION},  \n",
        "        {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER, \"weight_decay\": WEIGHT_DECAY_CLASSIFIER}  \n",
        "    ], betas=(0.9, 0.999), eps=1e-8)  # Default AdamW betas and eps\n",
        "\n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Optimizer initialized for Fold {fold+1}:\")\n",
        "    # Define Loss Function\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING_PREDICTION) \n",
        "\n",
        "    logging.info(f\"[K-FOLD INFO] Loss function initialized for Fold {fold+1}\")\n",
        "\n",
        "    # Train model for this fold\n",
        "    train_model(model, dataloaders, criterion, optimizer, device, fold, use_mixup=True)\n",
        "\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Measure Fold Time\n",
        "    fold_time = time.time() - fold_start_time\n",
        "    logging.info(f\"[K-FOLD INFO] Fold {fold+1} completed in {fold_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for fold in range(K_FOLDS):\n",
        "#     logging.info(f\"\\n[TEST INFO] Evaluating Fold {fold + 1} on Test Set...\")\n",
        "\n",
        "#     # Load best model for the fold\n",
        "#     model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "#     model_path = f\"best_model_fold_{fold + 1}.pth\"\n",
        "    \n",
        "#     try:\n",
        "#         model.load_state_dict(torch.load(model_path))\n",
        "#         logging.info(f\"[TEST INFO] Loaded best model for Fold {fold + 1} from {model_path}\")\n",
        "#     except FileNotFoundError:\n",
        "#         logging.error(f\"[ERROR] Model file {model_path} not found! Skipping Fold {fold + 1} evaluation.\")\n",
        "#         continue  # Skip to the next fold if model file is missing\n",
        "\n",
        "#     model.eval()  # Set to evaluation mode\n",
        "\n",
        "#     # Evaluate model on test data\n",
        "#     test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "\n",
        "#     # Log test set performance for the fold\n",
        "#     logging.info(f\"[TEST INFO] Fold {fold + 1} Test Performance:\")\n",
        "#     logging.info(f\"   Test Loss: {test_loss:.4f}\")\n",
        "#     logging.info(f\"   Test Accuracy: {test_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "enel645_torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
